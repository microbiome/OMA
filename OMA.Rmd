--- 
title: "Orchestrating Microbiome Analysis"
documentclass: book
site: bookdown::bookdown_site
bibliography: [book.bib]
biblio-style: apalike
link-citations: no
github-repo: microbiome/OMA
always_allow_html: yes
---

```{r, echo=FALSE}
.gh_url <- file.path('https://github.com', rmarkdown::metadata[['github-repo']])
```

---
date: "`r rebook::openingDetails(Copyright='', Source=.gh_url)`"
url: "`r .gh_url`"
---





# Welcome {-}


<a href="https://bioconductor.org"><img src="`r rebook::BiocSticker('animated')`" width="200" alt="Bioconductor Sticker" align="right" style="margin: 0 1em 0 1em" /></a>


You are reading the online book, [**Orchestrating Microbiome Analysis
with R and Bioconductor**](microbiome.github.io/OMA) [@OMA], where we
walk through common strategies and workflows in microbiome data
science.

The book shows through concrete examples how you can take advantage of
the latest developments in R/Bioconductor for the manipulation,
analysis, and reproducible reporting of hierarchical and heterogeneous
microbiome profiling data sets. The book was borne out of necessity,
while updating microbiome analysis tools to work with Bioconductor
classes that provide support for multi-modal data collections. Many of
these techniques are generic and widely applicable in other contexts
as well.

This work has been heavily influenced by other similar resources, in
particular the Orchestrating Single-Cell Analysis with Bioconductor
[@Amezquita2020], [phyloseq
tutorials](http://joey711.github.io/phyloseq/tutorials-index)
[@Callahan2016] and [microbiome
tutorials](https://microbiome.github.io/tutorials/) [@Shetty2019].
This book extends these resources to teach the grammar of Bioconductor
workflows in the context of microbiome data science.  As such, it
supports the adoption of general skills in the analysis of large,
hierarchical, and multi-modal data collections. We focus on microbiome
analysis tools, including entirely new, partially updated as well as
previously established methods.

This online resource and its associated ecosystem of microbiome data
science tools are a result of a community-driven development process,
and welcoming new contributors. Several individuals have
[contributed](https://github.com/microbiome/OMA/graphs/contributors)
methods, workflows and improvements as acknowledged in the
Introduction. You can find more information on how to find us online
and join the developer community through the project homepage at
[microbiome.github.io](https://microbiome.github.io). This online
resource has been written in RMarkdown with the bookdown R
package. The material is **free to use** with the [Creative Commons
Attribution-NonCommercial
3.0](https://creativecommons.org/licenses/by-nc/3.0/us/) License.


--------------

```{r include=FALSE}
library(Cairo)

# global knitr options
knitr::opts_chunk$set(
  fig.width=10,
  dpi=300,
  dev = "png",
  dev.args = list(type = "cairo-png")
)
```

```{js, echo = FALSE}
// This block adds image to the front page
title=document.getElementById('header');
title.innerHTML = title.innerHTML + 

'<img src="https://user-images.githubusercontent.com/60338854/128359392\
-6feef8df-30e9-4ea0-ae3b-4bb619d746ed.png" alt="Microbiome" width="50%"/>' +

'<p style="font-size:12px">Figure source: Moreno-Indias <i>et al</i>. (2021) \
<a href="https://doi.org/10.3389/fmicb.2021.635781">Statistical and \
Machine Learning Techniques in Human Microbiome Studies: Contemporary \
Challenges and Solutions</a>. Frontiers in Microbiology 12:11.</p>'
```

<!--chapter:end:index.Rmd-->

# (PART) Introduction {-}

# Introduction {#intro}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

This work - [**Orchestrating Microbiome Analysis with R and
Bioconductor**](microbiome.github.io/OMA) [@OMA] - contributes novel
methods and educational resources for microbiome data science.  It
aims to teach the grammar of Bioconductor workflows in the context of
microbiome data science. We show through concrete examples how to use
the latest developments and data analytical strategies in
R/Bioconductor for the manipulation, analysis, and reproducible
reporting of hierarchical, heterogeneous, and multi-modal microbiome
profiling data. The data science methodology is tightly integrated
with the broader R/Bioconductor ecosystem that focuses on the
development of high-quality open research software for life
sciences (@Gentleman2004, @Huber2015). The support for modularity and
interoperability is a key to efficient resource sharing and
collaborative development both within and across research fields. The
central data infrastructure, the `SummarizedExperiment` data container
and its derivatives, have already been widely adopted in microbiome
research, single cell sequencing, and in other fields, allowing a
rapid adoption and extensions of emerging data science techniques
across application domains.

We assume that the reader is already familiar with R programming. For
references and tips on introductory material for R and Bioconductor,
see Chapter \@ref(resources). This online resource and its associated
ecosystem of microbiome data science tools are a result of a
community-driven development process, and welcoming new users and
contributors. You can find more information on how to find us online
and join the developer community through the project homepage at
[microbiome.github.io](https://microbiome.github.io).

The book is organized into three parts. We start by introducing the
material and link to further resources for learning R and
Bioconductor. We describe the key data infrastructure, the
`TreeSummarizedExperiment` class that provides a container for
microbiome data, and how to get started by loading microbiome data set
in the context of this new framework. The second section, *Focus
Topics*, covers the common steps in microbiome data analysis,
beginning with the most common steps and progressing to more
specialized methods in subsequent sections. Third, *Workflows*,
provides case studies for the various datasets used throughout the
book. Finally, *Appendix*, links to further resources and
acknowledgments.








<!--chapter:end:01_intro.Rmd-->

# Packages {#packages}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

<img src="general/figures/mia_logo.png" width="100" alt="mia logo" align="right" style="margin: 0 1em 0 1em" />


The Bioconductor microbiome data science framework consists of:

- **data containers**, designed to organize multi-assay microbiome data
- **R packages** that provide dedicated methods for analysing such data
- **community** of users and developers 

<img src="general/figures/ecosystem.png" width="100" alt="mia logo" align="right" style="margin: 0 1em 0 1em" />


This section provides an overview of the package ecosystem. Section
\@ref(example-data) links to various open microbiome data resources
that support this framework.


## Package installation

You can install all packages that are required to run every example in this book via the following command:

```{r eval=FALSE, message=FALSE}
source("https://raw.githubusercontent.com/microbiome/OMA/master/install_packages.R")
```

### Installing specific packages {#packages_specific}

You can install R packages of your choice with the following command
line procedure.

**Bioconductor release version** is the most stable and tested version
but may miss some of the latest methods and updates. It can be
installed with:

```{r eval=FALSE, message=FALSE}
BiocManager::install("microbiome/mia")
```

**Bioconductor development version** requires the installation of the
latest R beta version. This is primarily recommended for those who
already have experience with R/Bioconductor and need access to the
latest updates.

```{r eval=FALSE, message=FALSE}
BiocManager::install("microbiome/mia", version="devel")
```

**Github development version** provides access to the latest but
potentially unstable features. This is useful when you want access to
all available tools.

```{r eval=FALSE, message=FALSE}
devtools::install_github("microbiome/mia")
```


## Package ecosystem 

Methods for the analysis and manipulation of
`(Tree)SummarizedExperiment` and `MultiAssayExperiment` data
containers are available through a number of R packages. Some of these
are listed below. If you know more tips on such packages, data
sources, or other resources, kindly [let us
know](https://microbiome.github.io) through the issues, pull requests,
or online channels.


### mia family of methods

- [mia](https://microbiome.github.io/mia/): Microbiome analysis tools [@R_mia]
- [miaViz](https://microbiome.github.io/miaViz/): Microbiome analysis specific visualization [@Ernst2022]
- [miaSim](https://microbiome.github.io/miaSim/): Microbiome data simulations [@Simsek2021]
- [miaTime](https://microbiome.github.io/miaTime/): Microbiome time series analysis [@Lahti2021]



### Tree-based methods {#sub-tree-methods}

- [philr](http://bioconductor.org/packages/devel/bioc/html/philr.html) (@Silverman2017)


### Differential abundance {#sub-diff-abund}

- [ANCOMBC](https://bioconductor.org/packages/devel/bioc/html/ANCOMBC.html) for differential abundance analysis
- [benchdamic](https://bioconductor.org/packages/release/bioc/vignettes/benchdamic/inst/doc/intro.html) for benchmarking differential abundance methods
- [LinDA](https://cran.r-project.org/web/packages/MicrobiomeStat/) for differential abundance analysis
- [ZicoSeq](https://cran.r-project.org/web/packages/GUniFrac/) for differential abundance analysis
- [ALDEx2](https://www.bioconductor.org/packages/release/bioc/html/ALDEx2.html) for differential abundance analysis
- [phyloseq](https://www.bioconductor.org/packages/release/bioc/html/phyloseq.html) for data preparation into phyloseq format for differential abundance analysis, such as ANCOMBC requires the input data is phyloseq format



### Manipulation {#sub-manipulation}

- [MicrobiotaProcess](https://bioconductor.org/packages/release/bioc/html/MicrobiotaProcess.html) for analyzing microbiome and other ecological data within the tidy framework


### Further options

- [Tools for Microbiome
  Analysis](https://microsud.github.io/Tools-Microbiome-Analysis/)
  site listed over 130 R packages for microbiome data science in
  2023. Many of these are not in Bioconductor, or do not directly
  support the data containers used in this book but can be used with
  minor modifications.



### Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```

<!--chapter:end:06_packages.Rmd-->

# Microbiome Data {#containers}


```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```


## Data science framework

The building blocks of the framework are **data container**
(SummarizedExperiment and its derivatives), **packages** from various
developers using the TreeSE container, open **demonstration data
sets**, in a separate chapter \@ref(example-data), and **online
tutorials** including this online book as well as the various package
vignettes and other material.

```{r echo=FALSE}
knitr::include_graphics("general/figures/FigureOverviewV2_mod.png")
```


## Data containers

`SummarizedExperiment` (`SE`) [@R_SummarizedExperiment] is a generic and highly optimized container for complex data

structures. It has become a common choice for analysing various types
of biomedical profiling data, such as RNAseq, ChIp-Seq, microarrays,
flow cytometry, proteomics, and single-cell
sequencing.

[`TreeSummarizedExperiment`] (`TreeSE`) [@R_TreeSummarizedExperiment] was developed as an extension to incorporate hierarchical

information (such as phylogenetic trees and sample hierarchies) and
reference sequences.

[`MultiAssayExperiment`] (`MAE`) [@Ramos2017] provides an organized way to bind several different data
structures together in a single object. For example, we can bind
microbiome data (in `TreeSE` format) with metabolomic profiling data
(in `SE`) format, with shared sample metadata. This is convenient and
robust for instance in subsetting and other data manipulation
tasks. Microbiome data can be part of multiomics experiments and
analysis strategies and we want to outline the understanding in which
we think the packages explained and used in this book relate to these
experiment layouts using the `TreeSummarizedExperiment` and classes
beyond.

This section provides an introductions to these data containers. In
microbiome data science, these containers link taxonomic abundance
tables with rich side information on the features and
samples. Taxonomic abundance data can be obtained by 16S rRNA amplicon
or metagenomic sequencing, phylogenetic microarrays, or by other
means. Many microbiome experiments include multiple versions and types
of data generated independently or derived from each other through
transformation or agglomeration. We start by providing recommendations
on how to represent different varieties of multi-table data within the
`TreeSummarizedExperiment` class.

The options and recommendations are summarized in Table \@ref(tab:options).


### Assay data 

The original count-based taxonomic abundance tables may have different 
transformations, such as logarithmic, Centered Log-Ratio (CLR), or relative 
abundance. These are typically stored in _**assays**_.

```{r}
library(mia)
data(GlobalPatterns, package="mia")
tse <- GlobalPatterns
assays(tse)
```

The `assays` slot contains the experimental data as count matrices. Multiple 
matrices can be stored the result of `assays` is actually a list of matrices.

```{r}
assays(tse)
```

Individual assays can be accessed via `assay`

```{r}
assay(tse, "counts")[1:5,1:7]
```

To illustrate the use of multiple assays, the relative abundance data can be 
calculated and stored along the original count data using `transformCounts`.

```{r}
tse <- transformCounts(tse, assay.type = "counts", method = "relabundance")
assays(tse)
```

Now there are two assays available in the `tse` object, `counts` and 
`relabundance`.

```{r}
assay(tse, "relabundance")[1:5,1:7]
```


Here the dimension of the count data remains unchanged. This is in
fact a libraryment for any `SummarizedExperiment` object.



### colData

`colData` contains data on the samples.

```{r coldata}
colData(tse)
```



### rowData

`rowData` contains data on the features of the analyzed samples. Of particular
interest for the microbiome field this is used to store taxonomic information.

```{r rowdata}
rowData(tse)
```

### rowTree  

Phylogenetic trees also play an important role for the microbiome field. The 
`TreeSummarizedExperiment` class is able to keep track of feature and node
relations via two functions, `rowTree` and `rowLinks`.

A tree can be accessed via `rowTree` as `phylo` object.       
```{r rowtree}
rowTree(tse)
```

The links to the individual features are available through `rowLinks`.

```{r rowlinks}
rowLinks(tse)
```

Please note that there can be a 1:1 relationship between tree nodes and 
features, but this is not a must have. This means there can be features, which
are not linked to nodes, and nodes, which are not linked to features. To change
the links in an existing object, the `changeTree` function is available.



### Alternative experiments {#alt-exp}

_**Alternative experiments**_ differ from transformations as they can
contain complementary data, which is no longer tied to the same
dimensions as the assay data. However, the number of samples (columns)
must be the same.

This can come into play for instance when one has taxonomic abundance
profiles quantified with different measurement technologies, such as
phylogenetic microarrays, amplicon sequencing, or metagenomic
sequencing. Such alternative experiments that concern the same samples
can be stored as

1. Separate _assays_ assuming that the taxonomic information can be mapped 
between feature directly 1:1; or 
2. data in the _altExp_ slot of the `TreeSummarizedExperiment`, if the feature 
dimensions differ. Each element of the _altExp_ slot is a `SummarizedExperiment`
or an object from a derived class with independent feature data.


As an example, we show how to store taxonomic abundance tables
agglomerated at different taxonomic levels. However, the data could as
well originate from entirely different measurement sources as long as
the samples are matched.

```{r}
# Agglomerate the data to Phylym level
tse_phylum <- agglomerateByRank(tse, "Phylum")
# both have the same number of columns (samples)
dim(tse)
dim(tse_phylum)

# Add the new table as an alternative experiment
altExp(tse, "Phylum") <- tse_phylum
altExpNames(tse)

# Pick a sample subset: this acts on both altExp and assay data
tse[,1:10]
dim(altExp(tse[,1:10],"Phylum"))
```

For more details of altExp have a look at the [Intro vignette](https://bioconductor.org/packages/release/bioc/vignettes/SingleCellExperiment/inst/doc/intro.html) of the 
`SingleCellExperiment` package [@R_SingleCellExperiment].



### MultiAssayExperiments {#mae}

_**Multiple experiments**_ relate to complementary measurement types,
such as transcriptomic or metabolomic profiling of the microbiome or
the host. Multiple experiments can be represented using the same
options as alternative experiments, or by using the
`MultiAssayExperiment` class [@Ramos2017]. Depending on how the 
datasets relate to each other the data can be stored as:

1. Separate _altExp_ if the samples can be matched directly 1:1; or
2. As `MultiAssayExperiment` objects, in which the connections between
samples are defined through a `sampleMap`. Each element on the
`experimentsList` of an `MultiAssayExperiment` is `matrix` or
`matrix`-like object including `SummarizedExperiment` objects, and the
number of samples can differ between the elements.


```{r}
#TODO: Find the right dataset to explain a non 1:1 sample relationship
```


For information have a look at the [intro vignette](https://bioconductor.org/packages/release/bioc/vignettes/MultiAssayExperiment/inst/doc/MultiAssayExperiment.html) of the `MultiAssayExperiment` package.  

 
   Option   Rows (features)    Cols (samples)               Recommended  
---------   --------------    ---------------  ------------------------
   assays  	     match              match       Data transformations  
   altExp             free              match    Alternative experiments  
MultiAssay            free      free (mapping)    Multi-omic experiments    

Table: (\#tab:options) **Recommended options for storing multiple data tables in microbiome studies** The _assays_ are best suited for data transformations (one-to-one match between samples and columns across the assays). The _alternative experiments_ are particularly suitable for alternative versions of the data that are of same type but may have a different number of features (e.g. taxonomic groups); this is for instance the case with taxonomic abundance tables agglomerated at different levels (e.g. genus vs. phyla) or alternative profiling technologies (e.g. amplicon sequencing vs. shallow shotgun metagenomics). For alternative experiments one-to-one match between samples (cols) is libraryd but the alternative experiment tables can have different numbers of features (rows). Finally, elements of the _MultiAssayExperiment_ provide the most flexible way to incorporate multi-omic data tables with flexible numbers of samples and features. We recommend these conventions as the basis for methods development and application in microbiome studies.




## Demonstration data {#example-data}

Open demonstration data for testing and benchmarking purposes is
available from multiple locations. This chapter introduces some
options. The other chapters of this book provide ample examples about
the use of the data.




### Package data {#package-data}

The `mia` R package contains example data sets that are direct
conversions from the alternative `phyloseq` container to the
`TreeSummarizedExperiment` container.

List the [available
datasets](https://microbiome.github.io/mia/reference/index.html) in
the `mia` package:


```{r, message=FALSE, eval=FALSE}
library(mia)
data(package="mia")
```

Load the `GlobalPatterns` data from the `mia` package:

```{r, message=FALSE}
data("GlobalPatterns", package="mia")
GlobalPatterns
```


Check the documentation for this data set:

```{r, message=FALSE, echo=FALSE}
help(GlobalPatterns)
```



### ExperimentHub data

[ExperimentHub](https://bioconductor.org/packages/release/bioc/vignettes/ExperimentHub/inst/doc/ExperimentHub.html)
provides a variety of data resources, including the
[microbiomeDataSets](https://bioconductor.org/packages/release/data/experiment/html/microbiomeDataSets.html)
package [@Morgan2021; @microlahti2021].

A table of the available data sets is available through the
`availableDataSets` function.

```{r, message=FALSE}
library(microbiomeDataSets)
availableDataSets()
```

All data are downloaded from ExperimentHub and cached for local
re-use. Check the [man pages of each
function](https://microbiome.github.io/microbiomeDataSets/reference/index.html)
for a detailed documentation of the data contents and references. Let
us retrieve a `r Biocpkg("MultiAssayExperiment")` data set:

```{r eval=FALSE, message=FALSE}
# mae <- HintikkaXOData()
# Since HintikkaXOData is now added to mia, we can load it directly from there
# We suggest to check other datasets from microbiomeDataSets
data(HintikkaXOData)
mae <- HintikkaXOData
```

Data is available in `r Biocpkg("SummarizedExperiment")`, `r
Biocpkg("TreeSummarizedExperiment")` and `r
Biocpkg("MultiAssayExperiment")` data containers; see the separate
page on [alternative
containers](https://microbiome.github.io/OMA/multitable.html) for more
details.


### Curated metagenomic data

[curatedMetagenomicData](https://bioconductor.org/packages/release/data/experiment/html/curatedMetagenomicData.html)
is a large collection of curated human microbiome data sets, provided as
`(Tree)SummarizedExperiment` objects [@Pasolli2017]. The resource
provides curated human microbiome data including gene families, marker
abundance, marker presence, pathway abundance, pathway coverage, and
relative abundance for samples from different body sites. See the
package homepage for more details on data availability and access.

As one example, let us retrieve the Vatanen (2016) [@Vatanen2016] data
set. This is a larger collection with a bit longer download time.

```{r, message=FALSE, eval=FALSE}
library(curatedMetagenomicData)
tse <- curatedMetagenomicData("Vatanen*", dryrun = FALSE, counts = TRUE)
```



### Other data sources

The current collections provide access to vast microbiome data
resources. The output has to be converted into TreeSE/MAE separately.

- [MGnifyR](https://github.com/beadyallen/MGnifyR) provides access to [EBI/MGnify](https://www.ebi.ac.uk/metagenomics/) 
- [qiitr](https://github.com/cran/qiitr) provides access to [QIITA](https://qiita.com/about) 


## Loading experimental microbiome data

### 16S workflow

Result of amplicon sequencing is large number of files that include all the sequences
that were read from samples. Those sequences need to be matched with taxa. Additionally,
we need to know how many times each taxa were found from each sample. 

There are several algorithms to do that, and DADA2 is one of the most common. 
You can find DADA2 pipeline tutorial for example from 
[here](https://benjjneb.github.io/dada2/tutorial.html).
After DADA2 portion of the tutorial is the data is stored into _phyloseq_ object 
(Bonus: Handoff to phyloseq). To store the data to _TreeSummarizedExperiment_,
follow the example below. 

You can find full workflow script without further explanations and comments from 
[here](https://github.com/microbiome/OMA/blob/master/dada2_workflow.Rmd)

```{r dada2_1, include=FALSE}
# Load objects
seqtab.nochim <- readRDS("data/dada2_seqtab.nochim")
taxa <- readRDS("data/dada2_taxa")
```

Load required packages.

```{r dada2_2}
library(mia)
library(ggplot2)

library("BiocManager")
library("Biostrings")

library(Biostrings)
```

Create arbitrary example sample metadata like it was done in tutorial. Usually, 
sample metadata is imported as a file.

```{r dada2_3}
samples.out <- rownames(seqtab.nochim)
subject <- sapply(strsplit(samples.out, "D"), `[`, 1)
gender <- substr(subject,1,1)
subject <- substr(subject,2,999)
day <- as.integer(sapply(strsplit(samples.out, "D"), `[`, 2))
samdf <- data.frame(Subject=subject, Gender=gender, Day=day)
samdf$When <- "Early"
samdf$When[samdf$Day>100] <- "Late"
rownames(samdf) <- samples.out
```

Convert data into right format and create _TreeSE_ object.

```{r dada2_4}
# Create a list that contains assays
counts <- t(seqtab.nochim)
counts <- as.matrix(counts)
assays <- SimpleList(counts = counts)

# Convert colData and rowData into DataFrame
samdf <- DataFrame(samdf)
taxa <- DataFrame(taxa)

# Create TreeSE
tse <- TreeSummarizedExperiment(assays = assays,
                                colData = samdf,
                                rowData = taxa
                                )

# Remove mock sample like it is also done in DADA2 pipeline tutorial
tse <- tse[ , colnames(tse) != "mock"]
```

Add sequences into _referenceSeq_ slot and convert rownames into simpler format.

```{r dada2_5}
# Convert sequences into right format
dna <- Biostrings::DNAStringSet( rownames(tse) )
# Add sequences into referenceSeq slot
referenceSeq(tse) <- dna
# Convert rownames into ASV_number format
rownames(tse) <- paste0("ASV", seq( nrow(tse) ))
tse
```



### Import from external files

Microbiome (taxonomic) profiling data is commonly distributed in
various file formats. You can import such external data files as a
(Tree)SummarizedExperiment object but the details depend on the file
format. Here, we provide examples for common formats. 


#### CSV import

**CSV data tables** can be imported with the standard R functions,
  then converted to the desired format. For detailed examples, you can
  check the [Bioconductor course
  material](https://bioconductor.org/help/course-materials/2019/BSS2019/04_Practical_CoreApproachesInBioconductor.html)
  by Martin Morgan. You can also check the [example
  files](https://github.com/microbiome/OMA/tree/master/data) and
  construct your own CSV files accordingly.

Recommendations for the CSV files are the following. File names are
arbitrary; we refer here to the same names than in the examples:

- Abundance table (`assay_taxa.csv`): data matrix (features x
  samples); first column provides feature IDs, the first row provides
  sample IDs; other values should be numeric (abundances).

- Row data (`rowdata_taxa.csv`): data table (features x info); first
  column provides feature IDs, the first row provides column headers;
  this file usually contains the taxonomic mapping between different
  taxonomic levels. Ideally, the feature IDs (row names) match one-to-one with
  the abundance table row names. 

- Column data (`coldata.csv`): data table (samples x info); first
  column provides sample IDs, the first row provides column headers;
  this file usually contains the sample metadata/phenodata (such as
  subject age, health etc). Ideally, the sample IDs match one-to-one with
  the abundance table column names. 

After you have set up the CSV files, you can read them in R:

```{r importingcsv1, message=FALSE}
count_file  <- "data/assay_taxa.csv"
tax_file    <- "data/rowdata_taxa.csv"
sample_file <- "data/coldata.csv"

# Load files
counts  <- read.csv(count_file, row.names=1)   # Abundance table (e.g. ASV data; to assay data)
tax     <- read.csv(tax_file, row.names=1)     # Taxonomy table (to rowData)
samples <- read.csv(sample_file, row.names=1)  # Sample data (to colData)
```

After reading the data in R, ensure the following:

- abundance table (`counts`): numeric `matrix`, with feature IDs as
  rownames and sample IDs as column names

- rowdata (`tax`): `DataFrame`, with feature IDs as rownames. If this
  is a `data.frame` you can use the function `DataFrame()` to change
  the format. Column names are free but in microbiome analysis they
  usually they refer to taxonomic ranks. The rownames in rowdata
  should match with rownames in abundance table.

- coldata (`samples`): `DataFrame`, with sample IDs as rownames. If
  this is a `data.frame` you can use the function `DataFrame()` to
  change the format.  Column names are free. The rownames in coldata
  should match with colnames in abundance table.

**Always ensure that the tables have rownames!** The _TreeSE_ constructor compares 
rownames and ensures that, for example, right samples are linked with right patient.

Also ensure that the row and column names match one-to-one between
abundance table, rowdata, and coldata:

```{r importingcsv2}
# Match rows and columns
counts <- counts[rownames(tax), rownames(samples)]

# Let us ensure that the data is in correct (numeric matrix) format:
counts <- as.matrix(counts)
```

If you hesitate about the format of the data, you can compare to one
of the available demonstration data sets, and make sure that your data
components have the same format.

There are many different source files and many different ways to read
data in R. One can do data manipulation in R as well. Investigate the
entries as follows.


```{r demodata, message=FALSE}
# coldata rownames match assay colnames
all(rownames(samples) == colnames(counts)) # our data set
class(samples) # should be data.frame or DataFrame

# rowdata rownames match assay rownames
all(rownames(tax) == rownames(counts)) # our data set
class(tax) # should be data.frame or DataFrame

# Counts 
class(counts) # should be a numeric matrix
```


### Constructing TreeSummarizedExperiment

Now let us create the TreeSE object from the input data tables. Here
we also convert the data objects in their preferred formats:

   - counts --> numeric matrix
   - rowData --> DataFrame
   - colData --> DataFrame

The `SimpleList` could be used to include multiple alternative assays, if
necessary.


```{r importingcsv3}
# Create a TreeSE
tse_taxa <- TreeSummarizedExperiment(assays =  SimpleList(counts = counts),
                                     colData = DataFrame(samples),
                                     rowData = DataFrame(tax))

tse_taxa
```

Now you should have a ready-made TreeSE data object that can be used in downstream analyses.


### Constructing MultiAssayExperiment

To construct a _MultiAssayExperiment_ object, just combine multiple _TreeSE_ data containers. 
Here we import metabolite data from the same study.

```{r importingcsv4, message=FALSE}
count_file <- "data/assay_metabolites.csv"
sample_file <- "data/coldata.csv"

# Load files
counts  <- read.csv(count_file, row.names=1)  
samples <- read.csv(sample_file, row.names=1)

# Create a TreeSE for the metabolite data
tse_metabolite <- TreeSummarizedExperiment(assays = SimpleList(concs = as.matrix(counts)),
                                           colData = DataFrame(samples))

tse_metabolite
```

Now we can combine these two experiments into _MAE_.

```{r importingcsv5}
# Create an ExperimentList that includes experiments
experiments <- ExperimentList(microbiome = tse_taxa, 
                              metabolite = tse_metabolite)

# Create a MAE
mae <- MultiAssayExperiment(experiments = experiments)

mae
```



### Import functions for standard formats

Specific import functions are provided for:

-   Biom files (see `help(mia::loadFromBiom)`)
-   QIIME2 files (see `help(mia::loadFromQIIME2)`)
-   Mothur files (see `help(mia::loadFromMothur)`)


#### Biom import

This example shows how Biom files are imported into a
`TreeSummarizedExperiment` object.

The data is from following publication: 
Tengeler AC _et al._ (2020) [**Gut microbiota from persons with
attention-deficit/hyperactivity disorder affects the brain in
mice**](https://doi.org/10.1186/s40168-020-00816-x). 

The data set consists of 3 files:

-   biom file: abundance table and taxonomy information
-   csv file: sample metadata
-   tree file: phylogenetic tree


Store the data in your desired local directory (for instance, _data/_ under the
working directory), and define source file paths

```{r}
biom_file_path <- "data/Aggregated_humanization2.biom"
sample_meta_file_path <- "data/Mapping_file_ADHD_aggregated.csv"
tree_file_path <- "data/Data_humanization_phylo_aggregation.tre"
```  

Now we can load the biom data into a SummarizedExperiment (SE) object.

```{r}
library(mia)

# Imports the data
se <- loadFromBiom(biom_file_path)

# Check
se
```  

The `assays` slot includes a list of abundance tables. The imported
abundance table is named as "counts".  Let us inspect only the first
cols and rows.

```{r}
assays(se)$counts[1:3, 1:3]
```

The `rowdata` includes taxonomic information from the biom file. The `head()` command
shows just the beginning of the data table for an overview.

`knitr::kable()` is for printing the information more nicely.

```{r}
head(rowData(se))
```

These taxonomic rank names (column names) are not real rank
names. Letâ€™s replace them with real rank names.

In addition to that, the taxa names include, e.g., '"k__' before the name, so let's
make them cleaner by removing them. 

```{r}
names(rowData(se)) <- c("Kingdom", "Phylum", "Class", "Order", 
                        "Family", "Genus")

# Goes through the whole DataFrame. Removes '.*[kpcofg]__' from strings, where [kpcofg] 
# is any character from listed ones, and .* any character.
rowdata_modified <- BiocParallel::bplapply(rowData(se), 
                                           FUN = stringr::str_remove, 
                                           pattern = '.*[kpcofg]__')

# Genus level has additional '\"', so let's delete that also
rowdata_modified <- BiocParallel::bplapply(rowdata_modified, 
                                           FUN = stringr::str_remove, 
                                           pattern = '\"')

# rowdata_modified is a list, so it is converted back to DataFrame format. 
rowdata_modified <- DataFrame(rowdata_modified)

# And then assigned back to the SE object
rowData(se) <- rowdata_modified

# Now we have a nicer table
head(rowData(se))
```

We notice that the imported biom file did not contain the sample meta data
yet, so it includes an empty data frame.

```{r}
head(colData(se))
```

Let us add a sample metadata file.

```{r}
# We use this to check what type of data it is
# read.table(sample_meta_file_path)

# It seems like a comma separated file and it does not include headers
# Let us read it and then convert from data.frame to DataFrame
# (required for our purposes)
sample_meta <- DataFrame(read.table(sample_meta_file_path, sep = ",", header = FALSE))

# Add sample names to rownames
rownames(sample_meta) <- sample_meta[,1]

# Delete column that included sample names
sample_meta[,1] <- NULL

# We can add headers
colnames(sample_meta) <- c("patient_status", "cohort", "patient_status_vs_cohort", "sample_name")

# Then it can be added to colData
colData(se) <- sample_meta
```

Now `colData` includes the sample metadata.

```{r}
head(colData(se))
```

Now, let's add a phylogenetic tree.

The current data object, se, is a SummarizedExperiment object. This
does not include a slot for adding a phylogenetic tree. In order to do
this, we can convert the SE object to an extended TreeSummarizedExperiment
object which includes also a `rowTree` slot.

TreeSummarizedExperiment contains also other additional slots and features which
is why we recommend to use `TreeSE`.

```{r}
tse <- as(se, "TreeSummarizedExperiment")

# tse includes same data as se
tse
```

Next, let us read the tree data file and add it to the R data object (tse).

```{r}
# Reads the tree file
tree <- ape::read.tree(tree_file_path)

# Add tree to rowTree
rowTree(tse) <- tree

# Check
tse
```

Now `rowTree` includes a phylogenetic tree:

```{r, eval=FALSE}
head(rowTree(tse))
```


### Conversions between data formats in R

If the data has already been imported in R in another format, it
can be readily converted into `TreeSummarizedExperiment`, as shown in our next
example. Note that similar conversion functions to
`TreeSummarizedExperiment` are available for multiple data formats via
the `mia` package (see makeTreeSummarizedExperimentFrom* for phyloseq,
Biom, and DADA2).

```{r, message=FALSE}
library(mia)

# phyloseq example data
data(GlobalPatterns, package="phyloseq") 
GlobalPatterns_phyloseq <- GlobalPatterns
GlobalPatterns_phyloseq
```

```{r, message=FALSE}
# convert phyloseq to TSE
GlobalPatterns_TSE <- makeTreeSummarizedExperimentFromPhyloseq(GlobalPatterns_phyloseq) 
GlobalPatterns_TSE
```

We can also convert `TreeSummarizedExperiment` objects into `phyloseq`
with respect to the shared components that are supported by both
formats (i.e. taxonomic abundance table, sample metadata, taxonomic
table, phylogenetic tree, sequence information). This is useful for
instance when additional methods are available for `phyloseq`.

```{r, message=FALSE}
# convert TSE to phyloseq
GlobalPatterns_phyloseq2 <- makePhyloseqFromTreeSummarizedExperiment(GlobalPatterns_TSE) 
GlobalPatterns_phyloseq2
```


Conversion is possible between other data formats. Interested readers can refer to the following functions:
* [makeTreeSummarizedExperimentFromDADA2](https://microbiome.github.io/mia/reference/makeTreeSummarizedExperimentFromDADA2.html)
* [makeSummarizedExperimentFromBiom](https://microbiome.github.io/mia/reference/makeSummarizedExperimentFromBiom.html)
* [loadFromMetaphlan](https://microbiome.github.io/mia/reference/loadFromMetaphlan.html)
* [readQZA](https://microbiome.github.io/mia/reference/loadFromQIIME2.html)


### Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()

```

<!--chapter:end:04_containers.Rmd-->

# (PART) Focus Topics {-}


# Data Manipulation {#datamanipulation}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```


## Tidying and subsetting

### Tidy data

For several custom analysis and visualization packages, such as those from
`tidyverse`, the `SE` data can be converted to a long data.frame format with 
`meltAssay`.    



```{r}
library(mia)
data(GlobalPatterns, package="mia")
tse <- GlobalPatterns
tse <- transformCounts(tse, MARGIN = "samples", method="relabundance")
molten_tse <- mia::meltAssay(tse,
                        add_row_data = TRUE,
                        add_col_data = TRUE,
                        assay.type = "relabundance")
molten_tse
```

### Subsetting

**Subsetting** data helps to draw the focus of analysis on particular
  sets of samples and / or features. When dealing with large data
  sets, the subset of interest can be extracted and investigated
  separately. This might improve performance and reduce the
  computational load.

Load:

* mia
* dplyr
* knitr
* data `GlobalPatterns`

```{r include = FALSE}
# Load libraries and data
library(mia)
library(dplyr)
library(knitr)
```

Let us store `GlobalPatterns` into `tse` and check its original number of features (rows) and samples (columns). **Note**: when subsetting by sample, expect the number of columns to decrease; when subsetting by feature, expect the number of rows to decrease.

```{r}
# Store data into se and check dimensions
data("GlobalPatterns", package="mia")
tse <- GlobalPatterns
# Show dimensions (features x samples)
dim(tse) 
```

#### Subset by sample (column-wise)

For the sake of demonstration, here we will extract a subset containing only the samples of human origin (feces, skin or tongue), stored as `SampleType` within `colData(tse)` and also in `tse`.

First, we would like to see all the possible values that `SampleType` can take on and how frequent those are: 

```{r}
# Inspect possible values for SampleType
unique(tse$SampleType)
```
```{r eval = FALSE}
# Show the frequency of each value
tse$SampleType %>% table()
```
```{r echo = FALSE}
# Show the frequency of each value
tse$SampleType %>% table() %>% kable() %>%
    kableExtra::kable_styling("striped", latex_options="scale_down") %>% 
    kableExtra::scroll_box(width = "100%")
```

**Note**: after subsetting, expect the number of columns to equal the
  sum of the frequencies of the samples that you are interested
  in. For instance, `ncols = Feces + Skin + Tongue = 4 + 3 + 2 = 9`.

Next, we _logical index_ across the columns of `tse` (make sure to
leave the first index empty to select all rows) and filter for the
samples of human origin. For this, we use the information on the
samples from the meta data `colData(tse)`.

```{r}
# Subset by sample
tse_subset_by_sample <- tse[ , tse$SampleType %in% c("Feces", "Skin", "Tongue")]

# Show dimensions
dim(tse_subset_by_sample)
```

As a sanity check, the new object `tse_subset_by_sample` should have
the original number of features (rows) and a number of samples
(columns) equal to the sum of the samples of interest (in this case
9).

Several characteristics can be used to subset by sample:

* origin
* sampling time
* sequencing method
* DNA / RNA barcode
* cohort

#### Subset by feature (row-wise)

Similarly, here we will extract a subset containing only the features
that belong to the phyla Actinobacteria and Chlamydiae, stored as
`Phylum` within `rowData(tse)`. However, subsetting by feature implies
a few more obstacles, such as the presence of `NA` elements and the
possible need for agglomeration.

As previously, we would first like to see all the possible values that
`Phylum` can take on and how frequent those are:
  
```{r}
# Inspect possible values for phylum
unique(rowData(tse)$Phylum)
```
```{r eval = FALSE}
# Show the frequency of each value
rowData(tse)$Phylum %>% table()
```
```{r echo = FALSE}
# Show te frequency of each value
rowData(tse)$Phylum %>% table() %>% kable() %>%
    kableExtra::kable_styling("striped", latex_options="scale_down") %>% 
    kableExtra::scroll_box(width = "100%")
```

**Note**: after subsetting, expect the number of columns to equal the
  sum of the frequencies of the feature(s) that you are interested
  in. For instance, `nrows = Actinobacteria + Chlamydiae = 1631 + 21 =
  1652`.

Depending on your research question, you might or might not need to
agglomerate the data in the first place: if you want to find the
abundance of each and every feature that belongs to Actinobacteria and
Chlamydiae, agglomeration is not needed; if you want to find the total
abundance of all features that belong to Actinobacteria or
Chlamydiae, agglomeration is recommended.

##### Non-agglomerated data

Next, we _logical index_ across the rows of `tse` (make sure to leave
the second index empty to select all columns) and filter for the
features that fall in either Actinobacteria or Chlamydiae group. For this,
we use the information on the samples from the metadata
`rowData(tse)`.

The first term with the `%in%` operator includes all the features
of interest, whereas the second term after the AND operator `&`
filters out all features that have an `NA` in place of the phylum variable.

```{r}
# Subset by feature
tse_subset_by_feature <- tse[rowData(tse)$Phylum %in% c("Actinobacteria", "Chlamydiae") & !is.na(rowData(tse)$Phylum), ]

# Show dimensions
dim(tse_subset_by_feature)
```

As a sanity check, the new object, `tse_subset_by_feature`, should have the original number of samples (columns) and a number of features (rows) equal to the sum of the features of interest (in this case, 1652).

##### Agglomerated data

When total abundances of certain phyla are of relevance, the data is initially agglomerated by Phylum. Then, similar steps as in the case of non-agglomerated data are followed.

```{r}
# Agglomerate by phylum
tse_phylum <- tse %>% agglomerateByRank(rank = "Phylum")

# Subset by feature and remove NAs
tse_phylum_subset_by_feature <- tse_phylum[rowData(tse_phylum)$Phylum %in% c("Actinobacteria", "Chlamydiae") & !is.na(rowData(tse_phylum)$Phylum), ]

# Show dimensions
dim(tse_phylum_subset_by_feature)
```

**Note**: as data was agglomerated, the number of rows should equal the
  number of phyla used to index (in this case, just 2).

Alternatively:

```{r}
# Store features of interest into phyla
phyla <- c("Phylum:Actinobacteria", "Phylum:Chlamydiae")
# subset by feature
tse_phylum_subset_by_feature <- tse_phylum[phyla, ]
# Show dimensions
dim(tse_subset_by_feature)
```

The code above returns the non-agglomerated version of the data.

Fewer characteristics can be used to subset by feature:

* Taxonomic rank
* Meta-taxonomic group

For subsetting by kingdom, agglomeration does not apply, whereas for
the other ranks it can be applied if necessary.

#### Subset by sample and feature

Finally, we can subset data by sample and feature at once. The
resulting subset contains all the samples of human origin and all the
features of phyla Actinobacteria or Chlamydiae.

```{r}
# Subset by sample and feature and remove NAs
tse_subset_by_sample_feature <- tse[rowData(tse)$Phylum %in% c("Actinobacteria", "Chlamydiae") & !is.na(rowData(tse)$Phylum), tse$SampleType %in% c("Feces", "Skin", "Tongue")]

# Show dimensions
dim(tse_subset_by_sample_feature)
```

**Note**: the dimensions of `tse_subset_by_sample_feature` agree with
  those of the previous subsets (9 columns filtered by sample and 1652
  rows filtered by feature).

If a study was to consider and quantify the presence of Actinobacteria
as well as Chlamydiae in different sites of the human body,
`tse_subset_by_sample_feature` might be a suitable subset to start
with.

#### Remove empty columns and rows

Sometimes data might contain, e.g., features that are not present in any of the  samples.
This can occur, for example, after the data subsetting. In certain analyses, we might want to
remove those instances.

```{r}
# Agglomerate data at Genus level 
tse_genus <- agglomerateByRank(tse, rank = "Genus")
# List bacteria that we want to include
genera <- c("Class:Thermoprotei", "Genus:Sulfolobus", "Genus:Sediminicola")
# Subset data
tse_genus_sub <- tse_genus[genera, ]

tse_genus_sub
```

```{r}
# List total counts of each sample
colSums(assay(tse_genus_sub, "counts"))
```

Now we can see that certain samples do not include any bacteria. We can remove those.

```{r}
# Remove samples that do not contain any bacteria
tse_genus_sub <- tse_genus_sub[ , colSums(assay(tse_genus_sub, "counts")) != 0 ]
tse_genus_sub
```

The same action can also be applied to the features.

```{r}
# Take only those samples that are collected from feces, skin, or tongue
tse_genus_sub <- tse_genus[ , tse_genus$SampleType %in% c("Feces", "Skin", "Tongue")]

tse_genus_sub
```

```{r}
# What is the number of bacteria that are not present?
sum(rowSums(assay(tse_genus_sub, "counts")) == 0)
```

We can see that there are bacteria that are not present in these samples we chose.
We can remove those bacteria from the data. 

```{r}
# Take only those bacteria that are present
tse_genus_sub <- tse_genus_sub[rowSums(assay(tse_genus_sub, "counts")) > 0, ]

tse_genus_sub
```

### Splitting

You can split the data based on variables by using the functions `splitByRanks` 
and `splitOn`.

`splitByRanks` splits the data based on taxonomic ranks. Since the elements of the output list
share columns, they can be stored into `altExp`. 

```{r splitbyRanks}
altExps(tse) <- splitByRanks(tse)
altExps(tse)
```

If you want to split the data based on another variable than taxonomic rank, use 
`splitOn`. It works for row-wise and column-wise splitting.

```{r splitOn}
splitOn(tse, "SampleType")
```

## Add or modify data

The information contained by the `colData` of a `TreeSE` can be modified by
accessing the desired variables.

```{r modify-coldata}
# modify the Description entries
colData(tse)$Description <- paste(colData(tse)$Description, "modified description")

# view modified variable
head(tse$Description)
```
New information can also be added to the experiment by creating a new variable.

```{r add-coldata}
# simulate new data
new_data <- runif(ncol(tse))

# store new data as new variable in colData
colData(tse)$NewVariable <- new_data

# view new variable
head(tse$NewVariable)
```

## Merge data

`mia` package has `mergeSEs` function that merges multiple `SummarizedExperiment`
objects. For example, it is possible to combine multiple `TreeSE` objects which each
includes one sample. 

`mergeSEs` works like `dplyr` joining functions. In fact, there are available
`dplyr-like` aliases of `mergeSEs`, such as `full_join`.

```{r merge1}
# Take subsets for demonstration purposes
tse1 <- tse[, 1]
tse2 <- tse[, 2]
tse3 <- tse[, 3]
tse4 <- tse[1:100, 4]
```

```{r merge2}
# With inner join, we want to include all shared rows. When using mergeSEs function
# all samples are always preserved.
tse <- mergeSEs(list(tse1, tse2, tse3, tse4), join = "inner")
tse
```

```{r merge3}
# Left join preserves all rows of the 1st object
tse <- mia::left_join(tse1, tse4, missing_values = 0)
tse
```

### Additional functions
* [mapTaxonomy](https://microbiome.github.io/mia/reference/taxonomy-methods.html)
* [mergeRows/mergeCols](https://microbiome.github.io/mia/reference/merge-methods.html)

<!--chapter:end:10_manipulation.Rmd-->

# Exploration and quality Control {#quality-control}


```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

This chapter focuses on the quality control and exploration of
microbiome data and establishes commonly used descriptive
summaries. Familiarizing with the peculiarities of a given data set is
the essential basis for any data analysis and model building.

The dataset should not suffer from severe technical biases, and you
should at least be aware of potential challenges, such as outliers,
biases, unexpected patterns and so forth. Standard summaries and
visualizations can help, and the rest comes with experience. The
exploration and quality control can be iterative processes.


```{r, message=FALSE}
library(mia)
```


## Abundance

Abundance visualization is an important data exploration
approach. `miaViz` offers the function `plotAbundanceDensity` to plot
the most abundant taxa with several options.

Next, a few demonstrations are shown, using the [@Lahti2014]
dataset. A Jitter plot based on relative abundance data, similar to
the one presented at [@Salosensaari2021] supplementary figure 1, could
be visualized as follows:

```{r, warning=FALSE, message=FALSE}
# Load example data
library(miaTime)
library(miaViz)
data(hitchip1006)
tse <- hitchip1006

# Add relative abundances
tse <- transformCounts(tse, MARGIN = "samples", method = "relabundance")

# Use argument names
# assay.type / assay.type / assay.type
# depending on the mia package version
plotAbundanceDensity(tse, layout = "jitter", assay.type = "relabundance",
                     n = 40, point_size=1, point_shape=19, point_alpha=0.1) + 
                     scale_x_log10(label=scales::percent)
```

The relative abundance values for the top-5 taxonomic features can be
visualized as a density plot over a log scaled axis, with
"nationality" indicated by colors:

```{r, warning=FALSE, message=FALSE}
plotAbundanceDensity(tse, layout = "density", assay.type = "relabundance",
                     n = 5, colour_by="nationality", point_alpha=1/10) +
    scale_x_log10()
```



## Prevalence

Prevalence quantifies the frequency of samples where certain microbes
were detected (above a given detection threshold). The prevalence can
be given as sample size (N) or percentage (unit interval).

Investigating prevalence allows you either to focus on changes which
pertain to the majority of the samples, or identify rare microbes,
which may be _conditionally abundant_ in a small number of samples.

The population prevalence (frequency) at a 1% relative abundance
threshold (`detection = 1/100` and `as_relative = TRUE`), can look
like this. 

```{r exploration-prevalence}
head(getPrevalence(tse, detection = 1/100, sort = TRUE, as_relative = TRUE))
```

The function arguments `detection` and `as_relative` can also be used
to access, how many samples do pass a threshold for raw counts. Here,
the population prevalence (frequency) at the absolute abundance
threshold (`as_relative = FALSE`) at read count 1 (`detection = 1`) is
accessed.

```{r concepts_prevalence2}
head(getPrevalence(tse, detection = 1, sort = TRUE, assay.type = "counts",
                   as_relative = FALSE))
```

If the output should be used for subsetting or storing the data in the
`rowData`, set `sort = FALSE`.


### Prevalence analysis

To investigate microbiome prevalence at a selected taxonomic level, two 
approaches are available.

First the data can be agglomerated to the taxonomic level and `getPrevalence` 
applied on the resulting object.

```{r}
# Agglomerate taxa abundances to Phylum level, and add the new table
# to the altExp slot
altExp(tse,"Phylum") <- agglomerateByRank(tse, "Phylum")
# Check prevalence for the Phylum abundance table from the altExp slot
head(getPrevalence(altExp(tse,"Phylum"), detection = 1/100, sort = TRUE,
                   assay.type = "counts", as_relative = TRUE))
```


Alternatively, the `rank` argument could be set to perform the
agglomeration on the fly.

```{r}
head(getPrevalence(tse, rank = "Phylum", detection = 1/100, sort = TRUE,
                   assay.type = "counts", as_relative = TRUE))
```

Note that, by default, `na.rm = TRUE` is used for agglomeration in
`getPrevalence`, whereas the default for `agglomerateByRank` is
`FALSE` to prevent accidental data loss.

If you only need the names of the prevalent taxa, `getPrevalentTaxa`
is available. This returns the taxa that exceed the given prevalence
and detection thresholds.

```{r core-members, message=FALSE, warning=FALSE, eval = FALSE}
getPrevalentTaxa(tse, detection = 0, prevalence = 50/100)
prev <- getPrevalentTaxa(tse, detection = 0, prevalence = 50/100,
                         rank = "Phylum", sort = TRUE)
prev
```

Note that the `detection` and `prevalence` thresholds are not the same, since
`detection` can be applied to relative counts or absolute counts depending on 
whether `as_relative` is set `TRUE` or `FALSE`


The function â€˜getPrevalentAbundanceâ€™ can be used to check the total
relative abundance of the prevalent taxa (between 0 and 1).


### Rare taxa

Related functions are available for the analysis of rare taxa
(`rareMembers`; `rareAbundance`; `lowAbundance`, `getRareTaxa`,
`subsetByRareTaxa`).


### Plotting prevalence

To plot the prevalence, add the prevalence of each taxon to
`rowData`. Here, we are analysing the Phylum level abundances, which
are stored in the `altExp` slot.

```{r}
rowData(altExp(tse,"Phylum"))$prevalence <- 
    getPrevalence(altExp(tse,"Phylum"), detection = 1/100, sort = FALSE,
                  assay.type = "counts", as_relative = TRUE)
```

The prevalences can then be plotted using the plotting functions from
the `scater` package.
 
```{r, message=FALSE, warning=FALSE}
library(scater)
plotRowData(altExp(tse,"Phylum"), "prevalence", colour_by = "Phylum")
```

The prevalence can also be visualized on the taxonomic tree with the
`miaViz` package.

```{r}
altExps(tse) <- splitByRanks(tse)
altExps(tse) <-
   lapply(altExps(tse),
          function(y){
              rowData(y)$prevalence <- 
                  getPrevalence(y, detection = 1/100, sort = FALSE,
                                assay.type = "counts", as_relative = TRUE)
              y
          })
top_phyla <- getTopTaxa(altExp(tse,"Phylum"),
                        method="prevalence",
                        top=5L,
                        assay.type="counts")
top_phyla_mean <- getTopTaxa(altExp(tse,"Phylum"),
                             method="mean",
                             top=5L,
                             assay.type="counts")
x <- unsplitByRanks(tse, ranks = taxonomyRanks(tse)[1:6])
x <- addTaxonomyTree(x)
```
 
After some preparation, the data is assembled and can be plotted with
`plotRowTree`.

```{r plot-prev-prev, message=FALSE, fig.cap="Prevalence of top phyla as judged by prevalence"}
library(miaViz)
plotRowTree(x[rowData(x)$Phylum %in% top_phyla,],
            edge_colour_by = "Phylum",
            tip_colour_by = "prevalence",
            node_colour_by = "prevalence")
```


```{r plot-prev-mean, message=FALSE, fig.cap="Prevalence of top phyla as judged by mean abundance"}
plotRowTree(x[rowData(x)$Phylum %in% top_phyla_mean,],
            edge_colour_by = "Phylum",
            tip_colour_by = "prevalence",
            node_colour_by = "prevalence")
```

## Quality control {#qc}

Next, let us load the `GlobalPatterns` data set to illustrate standard
microbiome data summaries.

```{r, message=FALSE}
library(mia)
data("GlobalPatterns", package="mia")
tse <- GlobalPatterns 
```


### Top taxa  

The `getTopTaxa` identifies top taxa in the data.   

```{r top-feature-taxo}
# Pick the top taxa
top_features <- getTopTaxa(tse, method="median", top=10)

# Check the information for these
rowData(tse)[top_features, taxonomyRanks(tse)]
```


### Library size / read count  

The total counts/sample can be calculated using `perCellQCMetrics`/`addPerCellQC` from the `scater` package. The former one
just calculates the values, whereas the latter one directly adds them to
`colData`.

```{r lib-size}
library(scater)
perCellQCMetrics(tse)
tse <- addPerCellQC(tse)
colData(tse)
```

The distribution of calculated library sizes can be visualized as a
histogram (left), or by sorting the samples by library size (right).

```{r plot-viz-lib-size-1, fig.width=8, fig.height=4, fig.cap="Library size distribution."}
library(ggplot2)

p1 <- ggplot(as.data.frame(colData(tse))) +
        geom_histogram(aes(x = sum), color = "black", fill = "gray", bins = 30) +
        labs(x = "Library size", y = "Frequency (n)") + 
        # scale_x_log10(breaks = scales::trans_breaks("log10", function(x) 10^x), 
        # labels = scales::trans_format("log10", scales::math_format(10^.x))) +
        theme_bw() +
        theme(panel.grid.major = element_blank(), # Removes the grid
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank(),
          axis.line = element_line(colour = "black")) # Adds y-axis

library(dplyr)
df <- as.data.frame(colData(tse)) %>%
        arrange(sum) %>%
        mutate(index = 1:n())
p2 <- ggplot(df, aes(y = index, x = sum/1e6)) +
        geom_point() +	
        labs(x = "Library size (million reads)", y = "Sample index") +	
        theme_bw() +
        theme(panel.grid.major = element_blank(), # Removes the grid
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank(),
          axis.line = element_line(colour = "black")) # Adds y-axis

library(patchwork)
p1 + p2
```

Library sizes other variables from `colData` can be
visualized by using specified function called `plotColData`.

```{r plot-viz-lib-size-2, fig.width=8, fig.height=4, fig.cap="Library sizes per sample."}
library(ggplot2)
# Sort samples by read count, order the factor levels, and store back to tse as DataFrame
# TODO: plotColData could include an option for sorting samples based on colData variables
colData(tse) <- as.data.frame(colData(tse)) %>%
                 arrange(X.SampleID) %>%
        	 mutate(X.SampleID = factor(X.SampleID, levels=X.SampleID)) %>%
		 DataFrame
plotColData(tse,"sum","X.SampleID", colour_by = "SampleType") + 
    theme(axis.text.x = element_text(angle = 45, hjust=1)) +
    labs(y = "Library size (N)", x = "Sample ID") 	    
```

```{r plot-viz-lib-size-3, fig.width=8, fig.height=4, fig.cap="Library sizes per sample type."}
plotColData(tse,"sum","SampleType", colour_by = "SampleType") + 
    theme(axis.text.x = element_text(angle = 45, hjust=1))
```

In addition, data can be rarefied with
[subsampleCounts](https://microbiome.github.io/mia/reference/subsampleCounts.html),
which normalises the samples to an equal number of reads. However,
this practice has been discouraged for the analysis of differentially
abundant microorganisms (see [@mcmurdie2014waste]).
  

### Contaminant sequences

Samples might be contaminated with exogenous sequences. The impact of
each contaminant can be estimated based on their frequencies and
concentrations across the samples.

The following [decontam
functions](https://microbiome.github.io/mia/reference/isContaminant.html)
are based on the [@davis2018simple] and support such functionality:

* `isContaminant`, `isNotContaminant`
* `addContaminantQC`, `addNotContaminantQC`


### Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```


<!--chapter:end:12_quality_control.Rmd-->

# Taxonomic Information {#taxonomic-information}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

```{r, message=FALSE}
library(mia)
data("GlobalPatterns", package="mia")
tse <- GlobalPatterns 
```

Taxonomic information is a key part of analyzing microbiome data and without
it, any type of data analysis probably will not make much sense. However,
the degree of detail of taxonomic information differs depending on the dataset
and annotation data used.

Therefore, the mia package expects a loose assembly of taxonomic information
and assumes certain key aspects:

* Taxonomic information is given as character vectors or factors in the 
`rowData` of a `SummarizedExperiment` object.
* The columns containing the taxonomic information must be named `domain`,
`kingdom`, `phylum`, `class`, `order`, `family`, `genus`, `species` or with
a capital first letter.
* the columns must be given in the order shown above
* column can be omited, but the order must remain

## Assigning taxonomic information.

There are a number of methods to assign taxonomic information. We like to give
a short introduction about the methods available without ranking one over the 
other. This has to be your choice based on the result for the individual 
dataset.

### dada2

The dada2 package [@Callahan2016dada2] implements the `assignTaxonomy`
function, which takes as input the ASV sequences associated with each
row of data and a training dataset. For more information visit the
[dada2 homepage](https://benjjneb.github.io/dada2/assign.html).

### DECIPHER

The DECIPHER package [@R_DECIPHER] implements the `IDTAXA` algorithm to assign
either taxonomic information or function information. For `mia`
only the first option is of interest for now and more information can be
found on the [DECIPHER website](http://www2.decipher.codes/Classification.html).

## Functions to access taxonomic information

`checkTaxonomy` checks whether the taxonomic information is usable for `mia`

```{r}
checkTaxonomy(tse)
```

Since the `rowData` can contain other data, `taxonomyRanks` will return the 
columns `mia` assumes to contain the taxonomic information.

```{r}
taxonomyRanks(tse)
```

This can then be used to subset the `rowData` to columns needed.

```{r}
rowData(tse)[,taxonomyRanks(tse)]
```

`taxonomyRankEmpty` checks for empty values in the given `rank` and returns a 
logical vector of `length(x)`.

```{r}
all(!taxonomyRankEmpty(tse, rank = "Kingdom"))
table(taxonomyRankEmpty(tse, rank = "Genus"))
table(taxonomyRankEmpty(tse, rank = "Species"))
```

`getTaxonomyLabels` is a multi-purpose function, which turns taxonomic
information into a character vector of `length(x)`

```{r}
head(getTaxonomyLabels(tse))
```

By default, this will use the lowest non-empty information to construct a
string with the following scheme `level:value`. If all levels are the same,
this part is omitted, but can be added by setting `with_rank = TRUE`.

```{r}
phylum <- !is.na(rowData(tse)$Phylum) & 
    vapply(data.frame(apply(rowData(tse)[,taxonomyRanks(tse)[3:7]],1L,is.na)),all,logical(1))
head(getTaxonomyLabels(tse[phylum,]))
head(getTaxonomyLabels(tse[phylum,], with_rank = TRUE))
```

By default the return value of `getTaxonomyLabels` contains only
unique elements by passing it through `make.unique`. This step can be
omitted by setting `make_unique = FALSE`.

```{r}
head(getTaxonomyLabels(tse[phylum,], with_rank = TRUE, make_unique = FALSE))
```

To apply the loop resolving function `resolveLoop` from the
`TreeSummarizedExperiment` package [@R_TreeSummarizedExperiment] within
`getTaxonomyLabels`, set `resolve_loops = TRUE`.

The function `getUniqueTaxa` gives a list of unique taxa for the
specified taxonomic rank.

```{r}
head(getUniqueTaxa(tse, rank = "Phylum"))
```


### Generate a taxonomic tree on the fly

To create a taxonomic tree, `taxonomyTree` used the information and returns a
`phylo` object. Duplicate information from the `rowData` is removed.

```{r}
taxonomyTree(tse)
```

```{r}
tse <- addTaxonomyTree(tse)
tse
```

The implementation is based on the `toTree` function from the
`TreeSummarizedExperiment` package [@R_TreeSummarizedExperiment].


## Data agglomeration {#data-agglomeration}

One of the main applications of taxonomic information in regards to count data
is to agglomerate count data on taxonomic levels and track the influence of 
changing conditions through these levels. For this `mia` contains the
`agglomerateByRank` function. The ideal location to store the agglomerated data
is as an alternative experiment.

```{r}
tse <- transformCounts(tse, assay.type = "counts", method = "relabundance")
altExp(tse, "Family") <- agglomerateByRank(tse, rank = "Family",
                                           agglomerateTree = TRUE)
altExp(tse, "Family")
```

If multiple assays (counts and relabundance) exist, both will be agglomerated.

```{r}
assayNames(tse)
assayNames(altExp(tse, "Family"))
```

```{r}
assay(altExp(tse, "Family"), "relabundance")[1:5,1:7]
```
  
```{r taxinfo_altexp_example}
assay(altExp(tse, "Family"), "counts")[1:5,1:7]
```

`altExpNames` now consists of `Family` level data. This can be extended to use 
any taxonomic level listed in `mia::taxonomyRanks(tse)`.   


## Data transformation

Data transformations are common in microbiome analysis. Examples
include the logarithmic transformation, calculation of relative
abundances (percentages), and compositionality-aware transformations
such as the centered log-ratio transformation (clr).

In mia package, transformations are applied to abundance data. The transformed 
abundance table is stored back to 'assays'. mia includes transformation 
function ('transformCounts()') which applies sample-wise or column-wise transformation when MARGIN = 'samples', feature-wise or row-wise transformation when MARGIN = 'features'.

For a complete list of available transformations and parameters, see function 
[help](https://microbiome.github.io/mia/reference/transformCounts.html).

```{r}
assay(tse, "pseudo") <- assay(tse, "counts") + 1
tse <- transformCounts(tse, assay.type = "pseudo", method = "relabundance")
tse <- transformCounts(x = tse, assay.type = "relabundance", method = "clr", 
                        pseudocount = 1, name = "clr_transformation")

head(assay(tse, "clr_transformation"))
```

-   In 'pa' transformation, abundance table is converted to present/absent table.

```{r}
tse <- transformCounts(tse, method = "pa")

head(assay(tse, "pa"))
```

```{r}
# list of abundance tables that assays slot contains
assays(tse)
```

## Pick specific  

Retrieving of specific elements that are required for specific analysis. For
instance, extracting abundances for a specific taxa in all samples or all taxa 
in one sample.  

### Abundances of all taxa in specific sample 
```{r}
taxa.abund.cc1 <- getAbundanceSample(tse, 
                                     sample_id = "CC1",
                                     assay.type = "counts")
taxa.abund.cc1[1:10]
```

### Abundances of specific taxa in all samples   

```{r}
taxa.abundances <- getAbundanceFeature(tse, 
                                      feature_id = "Phylum:Bacteroidetes",
                                      assay.type = "counts")
taxa.abundances[1:10]
```


### Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```

<!--chapter:end:11_taxonomic_information.Rmd-->

# Community diversity {#community-diversity}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```


Diversity estimates are a central topic in microbiome data analysis. 

There are three commonly employed levels of diversity measurements,
which are trying to put a number on different aspects of the questions
associated with diversity [@Whittaker1960].

Many different ways for estimating such diversity measurements have been 
described in the literature. Which measurement is best or applicable for your 
samples, is not the aim of the following sections.

```{r load-pkg-data}
library(mia)
data("GlobalPatterns", package="mia")
tse <- GlobalPatterns
```


**_Alpha diversity_**, also sometimes interchangeably used with the
term **_species diversity_**, summarizes the distribution of species
abundances in a given sample into a single number that depends on
species richness and evenness. Diversity indices measure the overall
community heterogeneity. A number of ecological diversity measures are
available. The Hill coefficient combines many standard indices into a
single equation that provides observed richness, inverse Simpson, and
Shannon diversity, and generalized diversity as special cases. In
general, diversity increases together with increasing richness and
evenness. Sometimes richness, phylogenetic diversity, evenness, dominance, 
and rarity are considered to be variants of alpha diversity.

**Richness** refers to the total number of species in a community
(sample). The simplest richness index is the number of observed
species (observed richness). Assuming limited sampling from the
community, however, this may underestimate the true species
richness. Several estimators are available, including for instance ACE
[@Chao1992] and Chao1 [@Chao1984]. Richness estimates are unaffected
by species abundances.
  
**Phylogenetic diversity** was first proposed by [@Faith1992]. Unlike the 
  diversity measures mentioned above, Phylogenetic diversity (PD) 
  measure incorporates information from phylogenetic relationships 
  stored in `phylo` tree between species in a community (sample). The 
  Faith's PD is calculated as the sum of branch length of all species in 
  a community (sample).

**Evenness** focuses on species abundances, and can thus complement
  the number of species. A typical evenness index is the Pielou's
  evenness, which is Shannon diversity normalized by the observed
  richness.

**Dominance** indices are in general negatively correlated with
  diversity, and sometimes used in ecological literature. High
  dominance is obtained when one or few species have a high share of
  the total species abundance in the community.  
  
**Rarity** indices characterize the concentration of taxa at low abundance. 
  Prevalence and detection thresholds determine rare taxa whose total concentration
  is represented as a rarity index.
  
## Estimation 

Alpha diversity can be estimated with wrapper functions that interact
with other packages implementing the calculation, such as _`vegan`_
[@R_vegan].


### Richness {#richness}

Richness gives the number of features present within a community and can be calculated with `estimateRichness`. Each of the estimate diversity/richness/evenness/dominance functions adds the calculated measure(s) to the `colData` of the `SummarizedExperiment` under the given column `name`. Here, we calculate `observed` features as a measure of richness.     

```{r}
tse <- mia::estimateRichness(tse, 
                             assay.type = "counts", 
                             index = "observed", 
                             name="observed")

head(colData(tse)$observed)
```
This allows access to the values to be analyzed directly from the `colData`, for example
by plotting them using `plotColData` from the _`scater`_ package [@R_scater].

```{r plot-div-shannon, message=FALSE, fig.cap="Shannon diversity estimates plotted grouped by sample type with colour-labeled barcode."}
library(scater)
plotColData(tse, 
            "observed", 
            "SampleType", 
            colour_by = "Final_Barcode") +
    theme(axis.text.x = element_text(angle=45,hjust=1)) + 
  ylab(expression(Richness[Observed]))

```

### Diversity {#estimate-diversity}  

The main function, `estimateDiversity`, calculates the selected
diversity index based on the selected assay data.  

```{r estimate-shannon}
tse <- mia::estimateDiversity(tse, 
                              assay.type = "counts",
                              index = "shannon", 
                              name = "shannon")
head(colData(tse)$shannon)
```

Alpha diversities can be visualized with boxplot. Here, Shannon index is compared 
between different sample type groups. Individual data points are visualized by 
plotting them as points with `geom_jitter`.

`geom_signif` is used to test whether these differences are statistically significant.
It adds p-values to plot.

```{r visualize-shannon}
library(ggsignif)
library(ggplot2)
library(patchwork)
library(ggsignif)

# Subsets the data. Takes only those samples that are from feces, skin, or tongue,
# and creates data frame from the collected data
df <- as.data.frame(tse[ , tse$SampleType %in% c("Feces", "Skin", "Tongue")])

# Changes old levels with new levels
df$SampleType <- factor(df$SampleType)

# For significance testing, all different combinations are determined
comb <- split(t(combn(levels(df$SampleType), 2)), 
           seq(nrow(t(combn(levels(df$SampleType), 2)))))

ggplot(df, aes(x = SampleType, y = shannon)) +
  # Outliers are removed, because otherwise each data point would be plotted twice; 
  # as an outlier of boxplot and as a point of dotplot.
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(width = 0.2) + 
  geom_signif(comparisons = comb, map_signif_level = FALSE) +
  theme(text = element_text(size = 10))
```

### Faith phylogenetic diversity {#faith-diversity}

The Faith index is returned by the function `estimateFaith`.

```{r phylo-div-1}
tse <- mia::estimateFaith(tse,
                          assay.type = "counts")
head(colData(tse)$faith)
```

**Note**: because `tse` is a `TreeSummarizedExperiment` object, its phylogenetic tree is used by default. However, the optional argument `tree` must be provided if `tse` does not contain one.

Below a visual comparison between shannon and faith indices is shown with a violin plot.

```{r phylo-div-2}
plots <- lapply(c("shannon", "faith"),
                plotColData,
                object = tse, colour_by = "SampleType")
plots[[1]] + plots[[2]] +
  plot_layout(guides = "collect")
```
 
Alternatively, the phylogenetic diversity can be calculated by `mia::estimateDiversity`. This is a faster re-implementation of   
the widely used function in _`picante`_ [@R_picante, @Kembel2010].  


Load `picante` R package and get the `phylo` stored in `rowTree`.

```{r phylo-div-3}
tse <- mia::estimateDiversity(tse, 
                              assay.type = "counts",
                              index = "faith", 
                              name = "faith")
```

### Evenness  

Evenness can be calculated with `estimateEvenness`.  

```{r evenness-1}
tse <- estimateEvenness(tse, 
                        assay.type = "counts", 
                        index="simpson")
head(colData(tse)$simpson)
```


### Dominance  

Dominance can be calculated with `estimateDominance`. Here, the `Relative index` is calculated which is the relative abundance of the most dominant species in the sample.   

```{r dominance-1}
tse <- estimateDominance(tse, 
                         assay.type = "counts", 
                         index="relative")

head(colData(tse)$relative)
```

### Rarity  

`mia` package provides one rarity index called log-modulo skewness. It can be 
calculated with `estimateDiversity`.

```{r rarity-1}
tse <- mia::estimateDiversity(tse, 
                              assay.type = "counts",
                              index = "log_modulo_skewness")

head(colData(tse)$log_modulo_skewness)
```

### Divergence

Divergence can be evaluated with `estimateDivergence`. Reference and algorithm for the calculation of divergence can be specified as `reference` and `FUN`, respectively. 

```{r}
tse <- mia::estimateDivergence(tse,
                               assay.type = "counts",
                               reference = "median",
                               FUN = vegan::vegdist)
```


## Visualization

A plot comparing all the diversity measures calculated above and stored in `colData` can then be constructed directly.

```{r plot-all-diversities, fig.width = 6.5}
plots <- lapply(c("observed", "shannon", "simpson", "relative", "faith", "log_modulo_skewness"),
                plotColData,
                object = tse,
                x = "SampleType",
                colour_by = "SampleType")

plots <- lapply(plots, "+", 
                theme(axis.text.x = element_blank(),
                      axis.title.x = element_blank(),
                      axis.ticks.x = element_blank()))

((plots[[1]] | plots[[2]] | plots[[3]]) / 
(plots[[4]] | plots[[5]] | plots[[6]])) +
  plot_layout(guides = "collect")
```

### Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```

<!--chapter:end:14_alpha_diversity.Rmd-->

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

```{r include=FALSE}
# global knitr options
knitr::opts_chunk$set(
  fig.width=10,
  dpi=300,
  dev = "png",
  dev.args = list(type = "cairo-png")
)
```


# Community similarity {#community-similarity}

Where alpha diversity focuses on community variation within a
community (sample), beta diversity quantifies (dis-)similarites
between communities (samples). Some of the most popular beta diversity
measures in microbiome research include Bray-Curtis index (for
compositional data), Jaccard index (for presence / absence data,
ignoring abundance information), Aitchison distance (Euclidean
distance for clr transformed abundances, aiming to avoid the
compositionality bias), and the Unifrac distances (that take into
account the phylogenetic tree information). Only some of the commonly
used beta diversity measures are actual _distances_; this is a
mathematically well-defined concept and many ecological beta diversity
measures, such as Bray-Curtis index, are not proper distances.
Therefore, the term dissimilarity or beta diversity is commonly used.

Technically, beta diversities are usually represented as `dist`
objects, which contain triangular data describing the distance between
each pair of samples. These distances can be further subjected to
ordination. Ordination is a common concept in ecology that aims to
reduce the dimensionality of the data for further evaluation or
visualization. Ordination techniques aim to capture as much of
essential information in the data as possible in a lower dimensional
representation.  Dimension reduction is bound to loose information but
the common ordination techniques aim to preserve relevant information
of sample similarities in an optimal way, which is defined in
different ways by different methods. [TODO add references and/or link
to ordination chapter instead?]

Some of the most common ordination methods in microbiome research
include Principal Component Analysis (PCA), metric and non-metric
multi-dimensional scaling (MDS, NMDS), The MDS methods are also known
as Principal Coordinates Analysis (PCoA). Other recently popular
techniques include t-SNE and UMAP. 


## Explained variance

The percentage of explained variance is typically shown for PCA
ordination plots. This quantifies the proportion of overall variance
in the data that is captured by the PCA axes, or how well the
ordination axes reflect the original distances.

Sometimes a similar measure is shown for MDS/PCoA. The interpretation
is generally different, however, and hence we do not recommend using
it. PCA is a special case of PCoA with Euclidean distances.  With
non-Euclidean dissimilarities PCoA uses a trick where the pointwise
dissimilarities are first cast into similarities in a Euclidean space
(with some information loss i.e. stress) and then projected to the
maximal variance axes. In this case, the maximal variance axes do not
directly reflect the correspondence of the projected distances and
original distances, as they do for PCA.

In typical use cases, we would like to know how well the ordination
reflects the original similarity structures; then the quantity of
interest is the so-called "stress" function, which measures the
difference in pairwise similarities between the data points in the
original (high-dimensional) vs. projected (low-dimensional) space.

Hence, we propose that for PCoA and other ordination methods, users
would report relative stress (varies in the unit interval; the smaller
the better). This can be calculated as shown below. For further
examples, check the [note from Huber
lab](https://www.huber.embl.de/users/klaus/Teaching/statisticalMethods-lab.pdf).


```{r relstress}
# Example data
library(mia)
data(GlobalPatterns, package="mia")

# Data matrix (features x samples)
tse <- GlobalPatterns
tse <- transformCounts(tse, method = "relabundance")

# Add group information Feces yes/no
tse$Group <- tse$SampleType=="Feces"

# Quantify dissimilarities in the original feature space
library(vegan)
x <- assay(tse, "relabundance") # Pick relabunance assay separately
d0 <- as.matrix(vegdist(t(x), "bray"))

# PCoA Ordination
pcoa <- as.data.frame(cmdscale(d0, k = 2))
names(pcoa) <- c("PCoA1", "PCoA2")

# Quantify dissimilarities in the ordination space
dp <- as.matrix(dist(pcoa))

# Calculate stress i.e. relative difference in the original and
# projected dissimilarities
stress <- sum((dp - d0)^2)/sum(d0^2)
```


Shepard plot visualizes the original versus projected (ordination)
dissimilarities between the data points:

```{r shepard}
ord <- order(as.vector(d0))
df <- data.frame(d0 = as.vector(d0)[ord],
                  dmds = as.vector(dp)[ord])

library(ggplot2)
ggplot(aes(x = d0, y = dmds), data=df) + 
       geom_smooth() +
       geom_point() +       
       labs(title = "Shepard plot",
       x = "Original distance",
       y = "MDS distance",       
            subtitle = paste("Stress:", round(stress, 2))) +
  theme_bw()
```


## Community comparisons by beta diversity analysis

A typical comparison of community composition starts with a visual
comparison of the groups on a 2D ordination.

Then we estimate relative abundances and MDS ordination based on
Bray-Curtis (BC) dissimilarity between the groups, and visualize the
results.

In the following examples dissimilarities are calculated by 
functions supplied to the `FUN` argument. This function can be defined by
the user. It must return a `dist` function, which can then be used to
calculate reduced dimensions either via ordination methods (such as MDS
or NMDS), and the results can be stored in the `reducedDim`.

This entire process is wrapped in the `runMDS` and `runNMDS`
functions.

```{r runMDS, message=FALSE}
library(scater)

# Bray-Curtis is usually applied to relative abundances
tse <- transformCounts(tse, method = "relabundance")
# Perform PCoA
tse <- runMDS(tse, FUN = vegan::vegdist, method = "bray", name = "PCoA_BC", assay.type = "relabundance")
```

Sample similarities can be visualized on a lower-dimensional display
(typically 2D) using the `plotReducedDim` function in the `scater`
package. This provides also further tools to incorporate additional
information using variations in color, shape or size. Are there
visible differences between the groups?

```{r plot-mds-bray-curtis, fig.cap="MDS plot based on the Bray-Curtis distances on the GlobalPattern dataset."}
# Create ggplot object
p <- plotReducedDim(tse, "PCoA_BC", colour_by = "Group")

# Add explained variance for each axis
e <- attr(reducedDim(tse, "PCoA_BC"), "eig");
rel_eig <- e/sum(e[e>0])		  
p <- p + labs(x = paste("PCoA 1 (", round(100 * rel_eig[[1]],1), "%", ")", sep = ""),
              y = paste("PCoA 2 (", round(100 * rel_eig[[2]],1), "%", ")", sep = ""))

print(p)
```




With additional tools from the `ggplot2` universe, comparisons can be 
performed informing on the applicability to visualize sample similarities in a 
meaningful way.

```{r plot-mds-nmds-comparison, fig.cap="Comparison of MDS and NMDS plots based on the Bray-Curtis or euclidean distances on the GlobalPattern dataset.", message=FALSE}
tse <- runMDS(tse, FUN = vegan::vegdist, name = "MDS_euclidean",
             method = "euclidean", assay.type = "counts")
tse <- runNMDS(tse, FUN = vegan::vegdist, name = "NMDS_BC")
tse <- runNMDS(tse, FUN = vegan::vegdist, name = "NMDS_euclidean",
               method = "euclidean")
plots <- lapply(c("PCoA_BC", "MDS_euclidean", "NMDS_BC", "NMDS_euclidean"),
                plotReducedDim,
                object = tse,
                colour_by = "Group")

library(patchwork)
plots[[1]] + plots[[2]] + plots[[3]] + plots[[4]] +
  plot_layout(guides = "collect")
```

The _Unifrac_ method is a special case, as it librarys data on the
relationship of features in form on a `phylo` tree. `calculateUnifrac`
performs the calculation to return a `dist` object, which can again be
used within `runMDS`.


```{r}
library(scater)
tse <- runMDS(tse, FUN = mia::calculateUnifrac, name = "Unifrac",
              tree = rowTree(tse),
              ntop = nrow(tse),
             assay.type = "counts")
```

```{r plot-unifrac, fig.cap="Unifrac distances scaled by MDS of the GlobalPattern dataset."}
plotReducedDim(tse, "Unifrac", colour_by = "Group")
```

## Other ordination methods

Other dimension reduction methods, such as `PCA`, `t-SNE` and `UMAP` are 
inherited directly from the `scater` package.

```{r}
tse <- runPCA(tse, name = "PCA", assay.type = "counts", ncomponents = 10)
```

```{r plot-pca, fig.cap="PCA plot on the GlobalPatterns data set containing sample from different sources."}
plotReducedDim(tse, "PCA", colour_by = "Group")
```

As mentioned before, applicability of the different methods depends on your
sample set.

FIXME: let us switch to UMAP for the examples?

```{r}
tse <- runTSNE(tse, name = "TSNE", assay.type = "counts", ncomponents = 3)
```

```{r plot-tsne, fig.cap="t-SNE plot on the GlobalPatterns data set containing sample from different sources."}
plotReducedDim(tse, "TSNE", colour_by = "Group", ncomponents = c(1:3))
```

As a final note, `mia` provides functions for the evaluation of additional dissimilarity indices, such as:
* `calculateJSD`, `runJSD` (Jensen-Shannon divergence)
* `calculateNMDS`, `plotNMDS` (non-metric multi-dimensional scaling)
* `calculateCCA`, `runCCA` (Canonical Correspondence Analysis)
* `calculateRDA`, `runRDA` (Redundancy Analysis)
* `calculateOverlap`, `runOverlap` ()
* `calculateDPCoA`, `runDPCoA` (Double Principal Coordinate Analysis)

Redundancy analysis is similar to PCA, however, it takes into account covariates. 
It aims to maximize the variance in respect of covariates. The results shows how much
each covariate affects.

```{r microbiome_RDA1}
# Load libraryd packages
library("vegan")
library("stringr")
library("knitr")
# Load data
data(enterotype, package="mia")
# Covariates that are being analyzed
variable_names <- c("ClinicalStatus", "Gender", "Age")

# Apply relative transform
enterotype <- transformCounts(enterotype, method = "relabundance")

# Create a formula
formula <- as.formula(paste0("assay ~ ", str_c(variable_names, collapse = " + ")) )

# # Perform RDA
rda <- calculateRDA(enterotype, assay.type = "relabundance",
                    formula = formula, distance = "bray", na.action = na.exclude)
# Get the rda object
rda <- attr(rda, "rda")
# Calculate p-value and variance for whole model
# Recommendation: use 999 permutations instead of 99
set.seed(436)
permanova <- anova.cca(rda, permutations = 99)
# Create a data.frame for results
rda_info <- as.data.frame(permanova)["Model", ]

# Calculate p-value and variance for each variable
# by = "margin" --> the order or variables does not matter
set.seed(4585)
permanova <- anova.cca(rda, by = "margin",  permutations = 99)
# Add results to data.frame
rda_info <- rbind(rda_info, permanova)

# Add info about total variance
rda_info[ , "Total variance"] <- rda_info["Model", 2] +
    rda_info["Residual", 2]

# Add info about explained variance
rda_info[ , "Explained variance"] <- rda_info[ , 2] / 
    rda_info[ , "Total variance"]

# Loop through variables, calculate homogeneity
homogeneity <- list()
# Get colDtaa
coldata <- colData(enterotype)
# Get assay
assay <- t(assay(enterotype, "relabundance"))
for( variable_name in rownames(rda_info) ){
    # If data is continuous or discrete
    if( variable_name %in% c("Model", "Residual") ||
        length(unique(coldata[[variable_name]])) /
        length(coldata[[variable_name]]) > 0.2 ){
        # Do not calculate homogeneity for continuous data
        temp <- NA
    } else{
        # Calculate homogeneity for discrete data
        # Calculate homogeneity
        set.seed(413)
        temp <- anova(
            betadisper( 
                vegdist(assay, method = "bray"),
                group = coldata[[variable_name]] ),
            permutations = permutations )["Groups", "Pr(>F)"]
    }
    # Add info to the list
    homogeneity[[variable_name]] <- temp
}
# Add homogeneity to information
rda_info[["Homogeneity p-value (NULL hyp: distinct/homogeneous --> permanova suitable)"]] <-
    homogeneity

kable(rda_info)
```

```{r microbiome_RDA2}
# Load ggord for plotting
library("ggord")
library("ggplot2")

# Since na.exclude was used, if there were rows missing information, they were 
# dropped off. Subset coldata so that it matches with rda.
coldata <- coldata[ rownames(rda$CCA$wa), ]

# Adjust names
# Get labels of vectors
vec_lab_old <- rownames(rda$CCA$biplot)

# Loop through vector labels
vec_lab <- sapply(vec_lab_old, FUN = function(name){
    # Get the variable name
    variable_name <- variable_names[ str_detect(name, variable_names) ]
    # If the vector label includes also group name
    if( !any(name %in% variable_names) ){
        # Get the group names
        group_name <- unique( coldata[[variable_name]] )[ 
        which( paste0(variable_name, unique( coldata[[variable_name]] )) == name ) ]
        # Modify vector so that group is separated from variable name
        new_name <- paste0(variable_name, " \U2012 ", group_name)
    } else{
        new_name <- name
    }
    # Add percentage how much this variable explains, and p-value
    new_name <- expr(paste(!!new_name, " (", 
                           !!format(round( rda_info[variable_name, "Explained variance"]*100, 1), nsmall = 1), 
                           "%, ",italic("P"), " = ", 
                           !!gsub("0\\.","\\.", format(round( rda_info[variable_name, "Pr(>F)"], 3), 
                                                       nsmall = 3)), ")"))

    return(new_name)
})
# Add names
names(vec_lab) <- vec_lab_old

# Create labels for axis
xlab <- paste0("RDA1 (", format(round( rda$CCA$eig[[1]]/rda$CCA$tot.chi*100, 1), nsmall = 1 ), "%)")
ylab <- paste0("RDA2 (", format(round( rda$CCA$eig[[2]]/rda$CCA$tot.chi*100, 1), nsmall = 1 ), "%)")

# Create a plot        
plot <- ggord(rda, grp_in = coldata[["ClinicalStatus"]], vec_lab = vec_lab,
              alpha = 0.5,
              size = 4, addsize = -4,
              #ext= 0.7, 
              txt = 3.5, repel = TRUE, 
              #coord_fix = FALSE
          ) + 
    # Adjust titles and labels
    guides(colour = guide_legend("ClinicalStatus"),
           fill = guide_legend("ClinicalStatus"),
           group = guide_legend("ClinicalStatus"),
           shape = guide_legend("ClinicalStatus"),
           x = guide_axis(xlab),
           y = guide_axis(ylab)) +
    theme( axis.title = element_text(size = 10) )
plot
```

From RDA plot, we can see that only age has significant affect on microbial profile. 

## Visualizing the most dominant genus on PCoA {#pcoa-genus}

In this section we visualize most dominant genus on PCoA. A similar visualization was proposed by Salosensaari et al. [-@Salosensaari2021].


Let us agglomerate the data at a Genus level and getting the dominant taxa per sample.

```{r}
# Agglomerate to genus level
tse_Genus <- agglomerateByRank(tse, rank="Genus")
# Convert to relative abundances
tse_Genus <- transformCounts(tse, method = "relabundance", assay.type="counts")
# Add info on dominant genus per sample
tse_Genus <- addPerSampleDominantTaxa(tse_Genus, assay.type="relabundance", name = "dominant_taxa")
```


Performing PCoA with Bray-Curtis dissimilarity.
```{r}
tse_Genus <- runMDS(tse_Genus, FUN = vegan::vegdist,
              name = "PCoA_BC", assay.type = "relabundance")
```


Getting top taxa and visualizing the abundance on PCoA.

```{r}
# Getting the top taxa
top_taxa <- getTopTaxa(tse_Genus,top = 6, assay.type = "relabundance")

# Naming all the rest of non top-taxa as "Other"
most_abundant <- lapply(colData(tse_Genus)$dominant_taxa,
                   function(x){if (x %in% top_taxa) {x} else {"Other"}})

# Storing the previous results as a new column within colData
colData(tse_Genus)$most_abundant <- as.character(most_abundant)

# Calculating percentage of the most abundant
most_abundant_freq <- table(as.character(most_abundant))
most_abundant_percent <- round(most_abundant_freq/sum(most_abundant_freq)*100, 1)

# Retrieving the explained variance
e <- attr(reducedDim(tse_Genus, "PCoA_BC"), "eig");
var_explained <- e/sum(e[e>0])*100

# Visualization
plot <-plotReducedDim(tse_Genus,"PCoA_BC", colour_by = "most_abundant") +
  scale_colour_manual(values = c("black", "blue", "lightblue", "darkgray", "magenta", "darkgreen", "red"),
                      labels=paste0(names(most_abundant_percent),"(",most_abundant_percent,"%)"))+
  labs(x=paste("PC 1 (",round(var_explained[1],1),"%)"),
       y=paste("PC 2 (",round(var_explained[2],1),"%)"),
       color="")
plot
```

Note: A 3D interactive version of the earlier plot can be found from \@ref(extras).

Similarly let's visualize and compare the sub-population.

```{r}
# Calculating the frequencies and percentages for both categories
freq_TRUE <- table(as.character(most_abundant[colData(tse_Genus)$Group==TRUE]))
freq_FALSE <- table(as.character(most_abundant[colData(tse_Genus)$Group==FALSE]))
percent_TRUE <- round(freq_TRUE/sum(freq_TRUE)*100, 1)
percent_FALSE <- round(freq_FALSE/sum(freq_FALSE)*100, 1)

# Visualization
plotReducedDim(tse_Genus[,colData(tse_Genus)$Group==TRUE],
                          "PCoA_BC", colour_by = "most_abundant") +
  scale_colour_manual(values = c("black", "blue", "lightblue", "darkgray", "magenta", "darkgreen", "red"),
                      labels=paste0(names(percent_TRUE),"(",percent_TRUE,"%)"))+
  labs(x=paste("PC 1 (",round(var_explained[1],1),"%)"),
       y=paste("PC 2 (",round(var_explained[2],1),"%)"),
       title = "Group = TRUE", color="")

plotReducedDim(tse_Genus[,colData(tse_Genus)$Group==FALSE],
                          "PCoA_BC", colour_by = "most_abundant") +
  scale_colour_manual(values = c("black", "blue", "lightblue", "darkgray", "magenta", "darkgreen", "red"),
                      labels=paste0(names(percent_FALSE),"(",percent_FALSE,"%)"))+
  labs(x=paste("PC 1 (",round(var_explained[1],1),"%)"),
       y=paste("PC 2 (",round(var_explained[2],1),"%)"),
       title = "Group = FALSE", color="")
```



### Testing differences in community composition between sample groups

The permutational analysis of variance (PERMANOVA) [@Anderson2001] is
a widely used non-parametric multivariate method that can be used to
estimate the actual statistical significance of differences in the
observed community composition between two groups of
samples.

PERMANOVA evaluates the hypothesis that the centroids and dispersion
of the community are equivalent between the compared groups. A small
p-value indicates that the compared groups have, on average, a
different community composition.

This method is implemented in the `vegan` package in the function
[`adonis2`](https://www.rdocumentation.org/packages/vegan/versions/2.4-2/topics/adonis).

**Note:**

It is recommended to `by = "margin"`. It specifies that each variable's marginal
effect is analyzed individually. 

When `by = "terms"` (the default)  the order of variables matters;
each variable is analyzed sequentially, and the result is different when more than 1 variable is
introduced and their order is differs. (Check [comparison](https://microbiome.github.io/OMA/extras.html#permanova-comparison))

We can perform PERMANOVA with `adonis2` function or by first performing distance-based 
redundancy analysis (dbRDA), and then applying permutational test for result of 
redundancy analysis. Advantage of the latter approach is that by doing so we can get 
coefficients: how much each taxa affect to the result.

```{r}
library(vegan)
# Agglomerate data to Species level
tse <- agglomerateByRank(tse, rank = "Species")

# Set seed for reproducibility
set.seed(1576)
# We choose 99 random permutations. Consider applying more (999 or 9999) in your
# analysis. 
permanova <- adonis2(t(assay(tse,"relabundance")) ~ Group,
                     by = "margin", # each term (here only 'Group') analyzed individually
                     data = colData(tse),
                     method = "euclidean",
                     permutations = 99)

# Set seed for reproducibility
set.seed(1576)
# Perform dbRDA
dbrda <- dbrda(t(assay(tse,"relabundance")) ~ Group, 
               data = colData(tse))
# Perform permutational analysis
permanova2 <- anova.cca(dbrda,
                        by = "margin", # each term (here only 'Group') analyzed individually
                        method = "euclidean",
                        permutations = 99)

# Get p-values
p_values <- c( permanova["Group", "Pr(>F)"], permanova2["Group", "Pr(>F)"] )
p_values <-as.data.frame(p_values)
rownames(p_values) <- c("adonis2", "dbRDA+anova.cca")
p_values
```

As we can see, the community composition is significantly different
between the groups (p < 0.05), and these two methods give equal p-values.

Let us visualize the model coefficients for species that exhibit the
largest differences between the groups. This gives some insights into
how the groups tend to differ from each other in terms of community
composition.

```{r}
# Add taxa info
sppscores(dbrda) <- t(assay(tse,"relabundance"))
# Get coefficients
coef <- dbrda$CCA$v
# Get the taxa with biggest weights
top.coef <- head( coef[rev(order(abs(coef))), , drop = FALSE], 20)
# Sort weights in increasing order
top.coef <- top.coef[ order(top.coef), ]
# Get top names
top_names <- names(top.coef)[ order(abs(top.coef), decreasing = TRUE) ]
```



```{r plot-top-coef-anova, fig.cap=""}
ggplot(data.frame(x = top.coef,
                  y = factor(names(top.coef),
                                      unique(names(top.coef)))),
        aes(x = x, y = y)) +
    geom_bar(stat="identity") +
    labs(x="",y="",title="Top Taxa") +
    theme_bw()
```

In the above example, the largest differences between the two groups
can be attributed to _`r top_names[1] `_ (elevated in the first
group) and _`r top_names[2] `_ (elevated in the second
group), and many other co-varying species.



### Checking the homogeneity condition 

It is important to note that the application of PERMANOVA assumes
homogeneous group dispersions (variances). This can be tested with the
PERMDISP2 method [@Anderson2006] by using the same assay and distance
method than in PERMANOVA.

```{r}
anova( betadisper(vegdist(t(assay(tse, "counts"))), colData(tse)$Group) )
```

If the groups have similar dispersion, PERMANOVA can be seen as an
appropriate choice for comparing community compositions.


## Further reading

 - [How to extract information from clusters](http://bioconductor.org/books/release/OSCA/clustering.html)

 - Chapter \@ref(clustering) on community typing

### Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```

<!--chapter:end:20_beta_diversity.Rmd-->

# Community composition {#microbiome-community}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

```{r load-pkg-data}
library(mia)
data("GlobalPatterns", package="mia")
tse <- GlobalPatterns
```

## Visualizing taxonomic composition {#visual-composition}

### Composition barplot

A typical way to visualize microbiome composition is by using
composition barplot. In the following, relative abundance is
calculated and top taxa are retrieved for the Phylum rank. Thereafter,
the barplot is visualized ordering rank by abundance values and
samples by "Bacteroidetes":

```{r}
library(miaViz)
# Computing relative abundance
tse <- transformCounts(tse, assay.type = "counts", method = "relabundance")

# Getting top taxa on a Phylum level
tse_phylum <- agglomerateByRank(tse, rank ="Phylum", onRankOnly=TRUE)
top_taxa <- getTopTaxa(tse_phylum,top = 5, assay.type = "relabundance")

# Renaming the "Phylum" rank to keep only top taxa and the rest to "Other"
phylum_renamed <- lapply(rowData(tse)$Phylum,
                   function(x){if (x %in% top_taxa) {x} else {"Other"}})
rowData(tse)$Phylum <- as.character(phylum_renamed)

# Visualizing the composition barplot, with samples order by "Bacteroidetes"
plotAbundance(tse, assay.type="relabundance", rank = "Phylum",
              order_rank_by="abund", 
              order_sample_by = "Bacteroidetes")
```

### Composition heatmap {#composition-heatmap}

Community composition can be visualized with heatmap, where the
horizontal axis represents samples and the vertical axis the
taxa. Color of each intersection point represents abundance of a taxon
in a specific sample.

Here,  abundances are  first CLR  (centered log-ratio)  transformed to
remove  compositionality bias. Then  Z  transformation  is applied  to
CLR-transformed  data. This  shifts all  taxa  to zero  mean and  unit
variance, allowing visual comparison  between taxa that have different
absolute  abundance  levels.  After  these  rough  visual  exploration
techniques, we can visualize the abundances at Phylum level.

```{r heatmap}
library(ggplot2)

# Add clr-transformation on samples
assay(tse_phylum, "pseudo") <- assay(tse_phylum, "counts") + 1
tse_phylum <- transformCounts(tse_phylum, assay.type = "pseudo",
                              method = "relabundance")

tse_phylum <- transformCounts(tse_phylum,
                  assay.type = "relabundance",
		  method = "clr")

# Add z-transformation on features (taxa)
tse_phylum <- transformCounts(tse_phylum, assay.type = "clr", 
                              MARGIN = "features",
                              method = "z", name = "clr_z")
```

Visualize as heatmap.

```{r heatmapvisu}
# Melt the assay for plotting purposes
df <- meltAssay(tse_phylum, assay.type = "clr_z")

# Determines the scaling of colours
maxval <- round(max(abs(df$clr_z)))
limits <- c(-maxval, maxval)
breaks <- seq(from = min(limits), to = max(limits), by = 0.5)
colours <- c("darkblue", "blue", "white", "red", "darkred")

# Creates a ggplot object
ggplot(df, aes(x = SampleID, y = FeatureID, fill = clr_z)) +
  geom_tile() +
  scale_fill_gradientn(name = "CLR + Z transform", 
                       breaks = breaks, limits = limits, colours = colours) + 
  theme(text = element_text(size=10),
        axis.text.x = element_text(angle=45, hjust=1),
        legend.key.size = unit(1, "cm")) +
  labs(x = "Samples", y = "Taxa")
```


_pheatmap_ is a package that provides methods to plot clustered heatmaps. 

```{r pheatmap1}
library(pheatmap)

# Takes subset: only samples from feces, skin, or tongue
tse_phylum_subset <- tse_phylum[ , tse_phylum$SampleType %in% c("Feces", "Skin", "Tongue") ]

# Add clr-transformation
tse_phylum_subset <- transformCounts(tse_phylum_subset,
                         method = "clr",
    			 pseudocount = 1)

tse_phylum_subset <- transformCounts(tse_phylum_subset, assay.type = "clr",
                                     MARGIN = "features", 
                                     method = "z", name = "clr_z")

# Get n most abundant taxa, and subsets the data by them
top_taxa <- getTopTaxa(tse_phylum_subset, top = 20)
tse_phylum_subset <- tse_phylum_subset[top_taxa, ]

# Gets the assay table
mat <- assay(tse_phylum_subset, "clr_z")

# Creates the heatmap
pheatmap(mat)
```

We can create clusters by hierarchical clustering and add them to the plot.

```{r pheatmap2}
library(ape)

# Hierarchical clustering
taxa_hclust <- hclust(dist(mat), method = "complete")

# Creates a phylogenetic tree
taxa_tree <- as.phylo(taxa_hclust)
```

```{r pheatmap3}
library(ggtree)

# Plot taxa tree
taxa_tree <- ggtree(taxa_tree) + 
  theme(plot.margin=margin(0,0,0,0)) # removes margins

# Get order of taxa in plot
taxa_ordered <- get_taxa_name(taxa_tree)

taxa_tree
```

Based on phylo tree, we decide to create three clusters. 

```{r pheatmap4}
# Creates clusters
taxa_clusters <- cutree(tree = taxa_hclust, k = 3)

# Converts into data frame
taxa_clusters <- data.frame(clusters = taxa_clusters)
taxa_clusters$clusters <- factor(taxa_clusters$clusters)

# Order data so that it's same as in phylo tree
taxa_clusters <- taxa_clusters[taxa_ordered, , drop = FALSE] 

# Prints taxa and their clusters
taxa_clusters
```

```{r pheatmap5}
# Adds information to rowData
rowData(tse_phylum_subset)$clusters <- taxa_clusters[order(match(rownames(taxa_clusters), rownames(tse_phylum_subset))), ]

# Prints taxa and their clusters
rowData(tse_phylum_subset)$clusters
```

```{r pheatmap6}
# Hierarchical clustering
sample_hclust <- hclust(dist(t(mat)), method = "complete")

# Creates a phylogenetic tree
sample_tree <- as.phylo(sample_hclust)

# Plot sample tree
sample_tree <- ggtree(sample_tree) + layout_dendrogram() + 
  theme(plot.margin=margin(0,0,0,0)) # removes margins

# Get order of samples in plot
samples_ordered <- rev(get_taxa_name(sample_tree))

sample_tree
```

```{r pheatmap7}
# Creates clusters
sample_clusters <- factor(cutree(tree = sample_hclust, k = 3))

# Converts into data frame
sample_data <- data.frame(clusters = sample_clusters)

# Order data so that it's same as in phylo tree
sample_data <- sample_data[samples_ordered, , drop = FALSE] 

# Order data based on 
tse_phylum_subset <- tse_phylum_subset[ , rownames(sample_data)]

# Add sample type data
sample_data$sample_types <- unfactor(colData(tse_phylum_subset)$SampleType)

sample_data
```

Now we can create heatmap with additional annotations.

```{r pheatmap8}
# Determines the scaling of colorss
# Scale colors
breaks <- seq(-ceiling(max(abs(mat))), ceiling(max(abs(mat))), 
              length.out = ifelse( max(abs(mat))>5, 2*ceiling(max(abs(mat))), 10 ) )
colors <- colorRampPalette(c("darkblue", "blue", "white", "red", "darkred"))(length(breaks)-1)

pheatmap(mat, annotation_row = taxa_clusters, 
         annotation_col = sample_data,
         breaks = breaks,
         color = colors)
```

In addition, there are also other packages that provide functions for
more complex heatmaps, such as
[_iheatmapr_](https://docs.ropensci.org/iheatmapr/articles/full_vignettes/iheatmapr.html)
and ComplexHeatmap [@ComplexHeatmap].
[sechm](http://www.bioconductor.org/packages/release/bioc/vignettes/sechm/inst/doc/sechm.html)
package provides wrapper for _ComplexHeatmap_ and its usage is
explained in chapter \@ref(viz-chapter) along with the `pheatmap`
package for clustered heatmaps.

### Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```


<!--chapter:end:21_microbiome_community.Rmd-->

# Community typing (clustering) {#clustering}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

```{r load-pkg-data1}
library(mia)
data("GlobalPatterns", package = "mia")
data("peerj13075", package = "mia")
tse <- peerj13075
```

Clustering is an unsupervised machine learning technique. The idea of
it is to find clusters from the data. A cluster is a group of
features/samples that share pattern.  For example, with clustering, we
can find group of samples that share similar community
composition. There are multiple clustering algorithms available.

## Introduction to bluster package

*bluster* is a Bioconductor package specialized in clustering. It offers 
multiple algorithms such as hierarchical clustering, DBSCAN, K-means, 
amongst others. The first thing to do when using this package is to load 
it, and transform the data if necessary, depending on your analysis goals.

```{r load_bluster}
# Load dependencies
library(bluster)

# Apply transformation
tse <- transformCounts(tse, method = "relabundance")
```

The clustering can be done on features or samples.

### Sample clustering

To cluster samples, we simply need to transpose the assay so that 
the samples are in rows.

```{r sample_clust_prep}
x <- t(assay(tse, "relabundance"))
```

Then, to perform clustering, use the `clusterRows` function. Depending 
on its parameters, it will perform a different algorithm. Here, we'll 
perform a hierarchical clustering.

```{r bluster_sample_hclust1}
# Simple use of the hierarchical clustering which sets the cut height to
# half the dendrogram height.
hclust.out <- clusterRows(x, HclustParam())

# Add cluster indices to the TSE
colData(tse)$clusters <- hclust.out

# Checking the result
hclust.out
```

Once the clustering on the samples is done, we can also plot the clusters.

```{r bluster_sample_plot}
library(scater)
# Add the MDS dimensions for plotting
tse <- runMDS(tse,
  assay.type = "relabundance",
  FUN = vegan::vegdist,
  method = "bray"
)

# Plot the clusters
plotReducedDim(tse, "MDS", colour_by = "clusters")
```

To change the clustering parameters, simply change the parameters in 
`HclustParam` function. To see the different parameters, check the 
[HclustParam documentation](https://rdrr.io/github/LTLA/bluster/man/HclustParam-class.html).

```{r bluster_sample_hclust2}
# More complex use of the algorithm with another method and cut height.
# We also add the full parameter to get the full hierarchical clustering
# information to plot the dendrogram
hclust.out <- clusterRows(x, HclustParam(method = "complete"), full = TRUE)

# Add data to the TSE
colData(tse)$clusters <- hclust.out$clusters

# Get the dendrogram object
dendro <- as.dendrogram(hclust.out$objects$hclust)

# Plot the dendrogram
plot(dendro)
```

Alternatively, it's also possible to use the `clusterCells` function from 
the *scran* package.This function uses directly the tree as a parameter 
instead of the assay itself. Here, we will use the PAM algorithm.

```{r scran_sample_clust}
library(scran)
pam.out <- clusterCells(tse,
  assay.type = "relabundance",
  BLUSPARAM = PamParam(centers = 5)
)
```

### Taxa clustering

Similarly to samples, it's also possible to cluster taxa. 
Prior to clustering, the data should be modified with compositionally 
aware transformations such as CLR. Then, simply follow the same steps as
with samples, using the `clusterRows` function, then putting the cluster 
indices in rowData. At the end, you can also merge the rows using the 
clusters indices.

## Hierarchical clustering

Hierarchical clustering aims to find hiearchy between
samples/features. There are to approaches: agglomerative ("bottom-up")
and divisive ("top-down").

In agglomerative approach, each observation is first unique cluster.
Algorithm continues by agglomerating similar clusters. Divisive
approach starts with one cluster that contains all the
observations. Clusters are splitted recursively to clusters that
differ the most. Clustering ends when each cluster contains only one
observation.

Hierarchical clustering can be visualized with dendrogram tree. In each
splitting point, the three is divided into two clusters leading to
hierarchy.

Let's load data from mia package.

```{r hclust1}
library(mia)
library(vegan)

# Load experimental data
data(peerj13075)
(tse <- peerj13075)
```

Hierarchical clustering librarys 2 steps. In the fist step, dissimilarities are 
calculated. In prior to that, data transformation is applied if needed. Since
sequencing data is compositional, relative transformation is applied.
In the second step, clustering is performed based on dissimilarities. 

```{r hclust2}
library(NbClust)
library(cobiclust)

# Apply transformation
tse <- transformCounts(tse, method = "relabundance")
# Get the assay
assay <- assay(tse, "relabundance")
# Transpose assay --> samples are now in rows --> we are clustering samples
assay <- t(assay)

# Calculate distances
diss <- vegdist(assay, method = "bray")

# Perform hierarchical clustering
hc <- hclust(diss, method = "complete")

# To visualize, convert hclust object into dendrogram object
dendro <- as.dendrogram(hc)

# Plot dendrogram
plot(dendro)
```

We can use dendrogram to determine the number of clusters. Usually the
tree is splitted where the branch length is the largest. However, as
we can see from the dendrogram, clusters are not clear. Algorithms are
available to identify the optimal number of clusters.

```{r hclust3}
# Determine the optimal number of clusters
res <- NbClust(
  diss = diss, distance = NULL, method = "ward.D2",
  index = "silhouette"
)

res$Best.nc
```

Based on the result, let's divide observations into 15 clusters.

```{r hclust4}
library(dendextend)

# Find clusters
cutree(hc, k = 15)

# Making colors for 6 clusters
col_val_map <- randomcoloR::distinctColorPalette(15) %>%
  as.list() %>%
  setNames(paste0("clust_", seq(15)))

dend <- color_branches(dendro, k = 15, col = unlist(col_val_map))
labels(dend) <- NULL
plot(dend)
```

## K-means clustering

Hierarchical clustering did not yield clusters. Let's try k-means
clustering instead. Here observations are divided into clusters so
that the distances between observations and cluster centers are
minimized; an observation belongs to cluster whose center is the
nearest.

The algorithm starts by dividing observation to random clusters whose
number is defined by user. The centroids of clusters are then
calculated. After that, observations' allocation to clusters are
updated so that the means are minimized. Again, centroid are
calculated, and algorithm continues iteratively until the assignments
do not change.

The number of clusters can be determined based on algorithm. Here we
utilize silhouette analysis.

```{r kmeans1}
library(factoextra)


# Convert dist object into matrix
diss <- as.matrix(diss)
# Perform silhouette analysis and plot the result
fviz_nbclust(diss, kmeans, method = "silhouette")
```

Based on the result of silhouette analysis, we choose 3 to be the number of clusters
in k-means clustering.

```{r kmeans2}
library(scater)

# The first step is random, add seed for reproducibility
set.seed(15463)
# Perform k-means clustering with 3 clusters
km <- kmeans(diss, 3, nstart = 25)
# Add the result to colData
colData(tse)$clusters <- as.factor(km$cluster)

# Perform PCoA so that we can visualize clusters
tse <- runMDS(tse, assay.type = "relabundance", FUN = vegan::vegdist, method = "bray")

# Plot PCoA and color clusters
plotReducedDim(tse, "MDS", colour_by = "clusters")
```

## Dirichlet Multinomial Mixtures (DMM)

This section focus on DMM analysis. 

One technique that allows to search for groups of samples that are
similar to each other is the [Dirichlet-Multinomial Mixture
Model](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030126). In
DMM, we first determine the number of clusters (k) that best fit the
data (model evidence) using Laplace approximation. After fitting the
model with k clusters, we obtain for each sample k probabilities that
reflect the probability that a sample belongs to the given cluster.

Let's cluster the data with DMM clustering. 

```{r dmm1}
# Runs model and calculates the most likely number of clusters from 1 to 7.
# Since this is a large dataset it takes long computational time.
# For this reason we use only a subset of the data; agglomerated by Phylum as a rank.
tse <- GlobalPatterns
tse <- agglomerateByRank(tse, rank = "Phylum", agglomerateTree = TRUE)
```

```{r dmm2}
tse_dmn <- mia::runDMN(tse, name = "DMN", k = 1:7)
```

```{r}
# It is stored in metadata
tse_dmn
```

Return information on metadata that the object contains.

```{r}
names(metadata(tse_dmn))
```

This returns a list of DMN objects for a closer investigation.

```{r}
getDMN(tse_dmn)
```


Show Laplace approximation (model evidence) for each model of the k models.

```{r}
library(miaViz)
plotDMNFit(tse_dmn, type = "laplace")
```

Return the model that has the best fit.

```{r}
getBestDMNFit(tse_dmn, type = "laplace")
```

### PCoA for ASV-level data with Bray-Curtis; with DMM clusters shown with colors

Group samples and return DMNGroup object that contains a summary.
Patient status is used for grouping.

```{r}
dmn_group <- calculateDMNgroup(tse_dmn,
  variable = "SampleType", assay.type = "counts",
  k = 2, seed = .Machine$integer.max
)

dmn_group
```

Mixture weights  (rough measure of the cluster size).


```{r}
DirichletMultinomial::mixturewt(getBestDMNFit(tse_dmn))
```


Samples-cluster assignment probabilities / how probable it is that sample belongs
to each cluster

```{r}
head(DirichletMultinomial::mixture(getBestDMNFit(tse_dmn)))
```

Contribution of each taxa to each component

```{r}
head(DirichletMultinomial::fitted(getBestDMNFit(tse_dmn)))
```
Get the assignment probabilities


```{r}
prob <- DirichletMultinomial::mixture(getBestDMNFit(tse_dmn))
# Add column names
colnames(prob) <- c("comp1", "comp2")

# For each row, finds column that has the highest value. Then extract the column
# names of highest values.
vec <- colnames(prob)[max.col(prob, ties.method = "first")]
```

Computing the euclidean PCoA and storing it as a data frame

```{r}
# Does clr transformation. Pseudocount is added, because data contains zeros.
assay(tse, "pseudo") <- assay(tse, "counts") + 1
tse <- transformCounts(tse, assay.type = "pseudo", method = "relabundance")
tse <- transformCounts(tse, "relabundance", method = "clr")

library(scater)

# Does principal coordinate analysis
df <- calculateMDS(tse, assay.type = "clr", method = "euclidean")

# Creates a data frame from principal coordinates
euclidean_pcoa_df <- data.frame(
  pcoa1 = df[, 1],
  pcoa2 = df[, 2]
)
```

```{r}
# Creates a data frame that contains principal coordinates and DMM information
euclidean_dmm_pcoa_df <- cbind(euclidean_pcoa_df,
  dmm_component = vec
)
# Creates a plot
euclidean_dmm_plot <- ggplot(
  data = euclidean_dmm_pcoa_df,
  aes(
    x = pcoa1, y = pcoa2,
    color = dmm_component
  )
) +
  geom_point() +
  labs(
    x = "Coordinate 1",
    y = "Coordinate 2",
    title = "PCoA with Aitchison distances"
  ) +
  theme(title = element_text(size = 12)) # makes titles smaller

euclidean_dmm_plot
```

## Community Detection

Another approach for discovering communities within the samples of the
data, is to run community detection algorithms after building a
graph. The following demonstration builds a graph based on the k
nearest-neighbors and performs the community detection on the fly.

_`bluster`_ [@R_bluster] package offers several clustering methods,
among which graph-based are present, enabling the community detection
task.

Installing package:

```{r}
library(bluster)
```

The algorithm used is "short random walks" [@Pons2006]. Graph is
constructed using different k values (the number of nearest neighbors
to consider during graph construction) using the robust centered log
ratio (rclr) assay data. Then plotting the communities using UMAP
[@McInnes2018] ordination as a visual exploration aid.  In the
following demonstration we use the `enterotype` dataset from the
[@R_mia] package.


```{r, message=FALSE, warning=FALSE}
library(bluster)
library(patchwork) # For arranging several plots as a grid
library(scater)

data("enterotype", package = "mia")
tse <- enterotype
tse <- transformCounts(tse, method = "rclr")

# Performing and storing UMAP
tse <- runUMAP(tse, name = "UMAP", assay.type = "rclr")

k <- c(2, 3, 5, 10)
ClustAndPlot <- function(x) {
  # Creating the graph and running the short random walks algorithm
  graph_clusters <- clusterRows(t(assays(tse)$rclr), NNGraphParam(k = x))

  # Results of the clustering as a color for each sample
  plotUMAP(tse, colour_by = I(graph_clusters)) +
    labs(title = paste0("k = ", x))
}

# Applying the function for different k values
plots <- lapply(k, ClustAndPlot)

# Displaying plots in a grid
(plots[[1]] + plots[[2]]) / (plots[[3]] + plots[[4]])
```

Similarly, the _`bluster`_ [@R_bluster] package offers clustering

diagnostics that can be used for judging the clustering quality (see
[Assorted clustering
diagnostics](http://bioconductor.org/packages/release/bioc/vignettes/bluster/inst/doc/diagnostics.html)).
In the following, Silhouette width as a diagnostic tool is computed
and results are visualized for each case presented earlier. For more
about Silhouettes read [@Rousseeuw1987].

```{r, message=FALSE, warning=FALSE}
ClustDiagPlot <- function(x) {
  # Getting the clustering results
  graph_clusters <- clusterRows(t(assays(tse)$rclr), NNGraphParam(k = x))

  # Computing the diagnostic info
  sil <- approxSilhouette(t(assays(tse)$rclr), graph_clusters)

  # Plotting as a boxlpot to observe cluster separation
  boxplot(split(sil$width, graph_clusters), main = paste0("k = ", x))
}
# Applying the function for different k values
res <- lapply(k, ClustDiagPlot)
```

## Biclustering

Biclustering methods cluster rows and columns simultaneously in order
to find subsets of correlated features/samples.

Here, we use following packages:

-   [biclust](https://cran.r-project.org/web/packages/biclust/index.html)
-   [cobiclust](https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13582)

_cobiclust_ is especially developed for microbiome data whereas _biclust_ is more
general method. In this section, we show three different cases and example 
solutions to apply biclustering to them. 

1.   Taxa vs samples
2.   Taxa vs biomolecule/biomarker
3.   Taxa vs taxa

Biclusters can be visualized using heatmap or boxplot, for
instance. For checking purposes, also scatter plot might be valid
choice.

Check more ideas for heatmaps from chapters \@ref(viz-chapter) and
\@ref(microbiome-community.

### Taxa vs samples

When you have microbial abundance matrices, we suggest to use
_cobiclust_ which is designed for microbial data.

Load example data
```{r load-pkg-data2}
library(mia)
data("HintikkaXOData")
mae <- HintikkaXOData
```

Only the most prevalent taxa are included in analysis. 

```{r cobiclust_1}
# Subset data in the first experiment
mae[[1]] <- subsetByPrevalentTaxa(mae[[1]], rank = "Genus", prevalence = 0.2, detection = 0.001)
# clr-transform in the first experiment
mae[[1]] <- transformCounts(mae[[1]], method = "relabundance")
mae[[1]] <- transformCounts(mae[[1]], "relabundance", method = "rclr")
```

_cobiclust_ takes counts table as an input and gives _cobiclust_ object as an output.
It includes clusters for taxa and samples. 

```{r cobiclust_2}
# Do clustering; use counts tableÂ´
clusters <- cobiclust(assay(mae[[1]], "counts"))

# Get clusters
row_clusters <- clusters$classification$rowclass
col_clusters <- clusters$classification$colclass

# Add clusters to rowdata and coldata
rowData(mae[[1]])$clusters <- factor(row_clusters)
colData(mae[[1]])$clusters <- factor(col_clusters)

# Order data based on clusters
mae[[1]] <- mae[[1]][order(rowData(mae[[1]])$clusters), order(colData(mae[[1]])$clusters)]

# Print clusters
clusters$classification
```

Next we can plot clusters. Annotated heatmap is a common choice.

```{r cobiclust_3, fig.width=14, fig.height=12}
library(pheatmap)
# z-transform for heatmap
mae[[1]] <- transformCounts(mae[[1]],
  assay.type = "rclr",
  MARGIN = "features",
  method = "z", name = "clr_z"
)

# Create annotations. When column names are equal, they should share levels.
# Here samples include 3 clusters, and taxa 2. That is why we have to make
# column names unique.
annotation_col <- data.frame(colData(mae[[1]])[, "clusters", drop = F])
colnames(annotation_col) <- "col_clusters"

annotation_row <- data.frame(rowData(mae[[1]])[, "clusters", drop = F])
colnames(annotation_row) <- "row_clusters"
```


Plot the heatmap.

```{r cobiclust_3b, fig.width=14, fig.height=12}
pheatmap(assay(mae[[1]], "clr_z"),
  cluster_rows = F, cluster_cols = F,
  annotation_col = annotation_col,
  annotation_row = annotation_row
)
```


Boxplot is commonly used to summarize the results:

```{r cobiclust_4}
library(ggplot2)
library(patchwork)

# ggplot librarys data in melted format
melt_assay <- meltAssay(mae[[1]], assay.type = "rclr", add_col_data = T, add_row_data = T)

# patchwork two plots side-by-side
p1 <- ggplot(melt_assay) +
  geom_boxplot(aes(x = clusters.x, y = rclr)) +
  labs(x = "Taxa clusters")

p2 <- ggplot(melt_assay) +
  geom_boxplot(aes(x = clusters.y, y = rclr)) +
  labs(x = "Sample clusters")

p1 + p2
```

### Taxa vs biomolecules

Here, we analyze cross-correlation between taxa and metabolites. This
is a case, where we use _biclust_ method which is suitable for numeric
matrices in general.

```{r biclust_1}
# Samples must be in equal order
# (Only 1st experiment  was ordered in cobiclust step leading to unequal order)
mae[[1]] <- mae[[1]][, colnames(mae[[2]])]

# Make rownames unique since it is library by other steps
rownames(mae[[1]]) <- make.unique(rownames(mae[[1]]))
# Calculate correlations
corr <- getExperimentCrossCorrelation(mae, 1, 2,
  assay.type1 = "rclr",
  assay.type2 = "nmr",
  mode = "matrix",
  cor_threshold = 0.2
)
```


_biclust_ takes matrix as an input and returns _biclust_ object. 

```{r biclust_2}
# Set seed for reproducibility
set.seed(3973)

# Find biclusters
library(biclust)
bc <- biclust(corr,
  method = BCPlaid(), fit.model = y ~ m,
  background = TRUE, shuffle = 100, back.fit = 0, max.layers = 10,
  iter.startup = 10, iter.layer = 100, verbose = FALSE
)

bc
```

The object includes cluster information. However compared to
_cobiclust_, _biclust_ object includes only information about clusters
that were found, not general cluster.

Meaning that if one cluster size of 5 features was found out of 20 features, 
those 15 features do not belong to any cluster. That is why we have to create an
additional cluster for features/samples that are not assigned into any cluster.

```{r biclust_3}
# Functions for obtaining biclust information

# Get clusters for rows and columns
.get_biclusters_from_biclust <- function(bc, assay) {
  # Get cluster information for columns and rows
  bc_columns <- t(bc@NumberxCol)
  bc_columns <- data.frame(bc_columns)
  bc_rows <- bc@RowxNumber
  bc_rows <- data.frame(bc_rows)

  # Get data into right format
  bc_columns <- .manipulate_bc_data(bc_columns, assay, "col")
  bc_rows <- .manipulate_bc_data(bc_rows, assay, "row")

  return(list(bc_columns = bc_columns, bc_rows = bc_rows))
}

# Input clusters, and how many observations there should be, i.e.,
# the number of samples or features
.manipulate_bc_data <- function(bc_clusters, assay, row_col) {
  # Get right dimension
  dim <- ifelse(row_col == "col", ncol(assay), nrow(assay))
  # Get column/row names
  if (row_col == "col") {
    names <- colnames(assay)
  } else {
    names <- rownames(assay)
  }

  # If no clusters were found, create one. Otherwise create additional
  # cluster which
  # contain those samples that are not included in clusters that were found.
  if (nrow(bc_clusters) != dim) {
    bc_clusters <- data.frame(cluster = rep(TRUE, dim))
  } else {
    # Create additional cluster that includes those samples/features that
    # are not included in other clusters.
    vec <- ifelse(rowSums(bc_clusters) > 0, FALSE, TRUE)
    # If additional cluster contains samples, then add it
    if (any(vec)) {
      bc_clusters <- cbind(bc_clusters, vec)
    }
  }
  # Adjust row and column names
  rownames(bc_clusters) <- names
  colnames(bc_clusters) <- paste0("cluster_", 1:ncol(bc_clusters))
  return(bc_clusters)
}
```


```{r biclust_4}
# Get biclusters
bcs <- .get_biclusters_from_biclust(bc, corr)

bicluster_rows <- bcs$bc_rows
bicluster_columns <- bcs$bc_columns

# Print biclusters for rows
head(bicluster_rows)
```

Let's collect information for the scatter plot. 

```{r biclust_5}
# Function for obtaining sample-wise sum, mean, median, and mean variance
# for each cluster

.sum_mean_median_var <- function(tse1, tse2, assay.type1, assay.type2, clusters1, clusters2) {
  list <- list()
  # Create a data frame that includes all the information
  for (i in 1:ncol(clusters1)) {
    # Subset data based on cluster
    tse_subset1 <- tse1[clusters1[, i], ]
    tse_subset2 <- tse2[clusters2[, i], ]
    # Get assay
    assay1 <- assay(tse_subset1, assay.type1)
    assay2 <- assay(tse_subset2, assay.type2)
    # Calculate sum, mean, median, and mean variance
    sum1 <- colSums2(assay1, na.rm = T)
    mean1 <- colMeans2(assay1, na.rm = T)
    median1 <- colMedians(assay1, na.rm = T)
    var1 <- colVars(assay1, na.rm = T)

    sum2 <- colSums2(assay2, na.rm = T)
    mean2 <- colMeans2(assay2, na.rm = T)
    median2 <- colMedians(assay2, na.rm = T)
    var2 <- colVars(assay2, na.rm = T)

    list[[i]] <- data.frame(
      sample = colnames(tse1), sum1, sum2, mean1, mean2,
      median1, median2, var1, var2
    )
  }

  return(list)
}

# Calculate info
df <- .sum_mean_median_var(mae[[1]], mae[[2]], "rclr", "nmr", bicluster_rows, bicluster_columns)
```

Now we can create a scatter plot. X-axis includes median clr abundance
of microbiome and y-axis median absolute concentration of each
metabolite. Each data point represents a single sample.

From the plots, we can see that there is low negative correlation in
both cluster 1 and 3.  This means that when abundance of bacteria
belonging to cluster 1 or 3 is higher, the concentration of
metabolites of cluster 1 or 3 is lower, and vice versa.

```{r biclust_6, fig.width=14, fig.height=6, fig.show="keep", out.width="33%"}
pics <- list()
for (i in seq_along(df)) {
  pics[[i]] <- ggplot(df[[i]]) +
    geom_point(aes(x = median1, y = median2)) +
    labs(
      title = paste0("Cluster ", i),
      x = "Taxa (rclr median)",
      y = "Metabolites (abs. median)"
    )
  print(pics[[i]])
}
# pics[[1]] + pics[[2]] + pics[[3]]
```

_pheatmap_ does not allow boolean values, so they must be converted into factors.

```{r biclust_7}
bicluster_columns <- data.frame(apply(bicluster_columns, 2, as.factor))
bicluster_rows <- data.frame(apply(bicluster_rows, 2, as.factor))
```

Again, we can plot clusters with heatmap.

```{r biclust_8, fig.width=10, fig.height=10}
# Adjust colors for all clusters
if (ncol(bicluster_rows) > ncol(bicluster_columns)) {
  cluster_names <- colnames(bicluster_rows)
} else {
  cluster_names <- colnames(bicluster_columns)
}
annotation_colors <- list()
for (name in cluster_names) {
  annotation_colors[[name]] <- c("TRUE" = "red", "FALSE" = "white")
}

# Create a heatmap
pheatmap(corr,
  cluster_cols = F, cluster_rows = F,
  annotation_col = bicluster_columns,
  annotation_row = bicluster_rows,
  annotation_colors = annotation_colors
)
```

### Taxa vs taxa

Third and final example deals with situation where we want to analyze
correlation between taxa. _biclust_ is suitable for this.

```{r biclust_9}
# Calculate cross-correlation
corr <- getExperimentCrossCorrelation(mae, 1, 1,
  assay.type1 = "rclr", assay.type2 = "rclr",
  mode = "matrix",
  cor_threshold = 0.2, verbose = F, show_warning = F
)

# Find biclusters
library(biclust)
bc <- biclust(corr,
  method = BCPlaid(), fit.model = y ~ m,
  background = TRUE, shuffle = 100, back.fit = 0, max.layers = 10,
  iter.startup = 10, iter.layer = 100, verbose = FALSE
)
```

```{r biclust_10}
# Get biclusters
bcs <- .get_biclusters_from_biclust(bc, corr)

bicluster_rows <- bcs$bc_rows
bicluster_columns <- bcs$bc_columns
```


```{r biclust_11}
# Create a column that combines information
# If row/column includes in multiple clusters, cluster numbers are separated with "_&_"
bicluster_columns$clusters <- apply(
  bicluster_columns, 1,
  function(x) {
    paste(paste(which(x)), collapse = "_&_")
  }
)
bicluster_columns <- bicluster_columns[, "clusters", drop = FALSE]

bicluster_rows$clusters <- apply(
  bicluster_rows, 1,
  function(x) {
    paste(paste(which(x)), collapse = "_&_")
  }
)
bicluster_rows <- bicluster_rows[, "clusters", drop = FALSE]
```

```{r biclust_12, fig.width=14, fig.height=12}
# Convert boolean values into factor
bicluster_columns <- data.frame(apply(bicluster_columns, 2, as.factor))
bicluster_rows <- data.frame(apply(bicluster_rows, 2, as.factor))

pheatmap(corr,
  cluster_cols = F, cluster_rows = F,
  annotation_col = bicluster_columns,
  annotation_row = bicluster_rows
)
```

## Additional Community Typing

For more community typing techniques applied to the 'SprockettTHData'
data set, see the attached .Rmd file.

Link:

   * [Rmd](add-comm-typing.Rmd)

<!--chapter:end:24_clustering.Rmd-->

# Differential abundance {#differential-abundance}
```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```


## Differential abundance analysis


This section provides an overview and examples of *differential
abundance analysis (DAA)* based on one of the [openly available
datasets](https://microbiome.github.io/mia/reference/mia-datasets.html)
in mia to illustrate how to perform differential abundance analysis
(DAA). DAA identifies differences in the abundances of individual
taxonomic groups between two or more groups (e.g. treatment vs
control). This can be performed at any phylogenetic level.

We perform DAA to identify biomarkers and/or gain understanding of a
complex system by looking at its isolated components. For example,
identifying that a bacterial taxon is different between a patient
group with disease *X* vs a healthy control group might lead to
important insights into the pathophysiology. Changes in the microbiota
might be cause or a consequence of a disease. Either way, it can
help to understand the system as a whole. Be aware that this approach
has also been criticized recently [@Quinn2021].


### Examples and tools

Due to the complex data characteristics of microbiome sequencing data,
differential abundance analysis of microbiome data faces many
statistical challenges [@Yang2022], including:
  
- Highly variable. The abundance of a specific taxon could range over
  several orders of magnitude.
  
- Zero-inflated. In a typical microbiome dataset, more than 70% of the
  values are zeros. Zeros could be due to either physical absence
  (structural zeros) or insufficient sampling effort (sampling zeros).
  
- Compositional. Increase or decrease in the (absolute) abundance of
  one taxon at the sampling site will lead to apparent changes in the
  relative abundances of other taxa in the sample.

As summarized in @Yang2022, to address the above statistical
chanllenegs:

- Over-dispersed count models has been proposed to address zero
  inflation, such as the negative binomial model used by edgeR
  [@Chen2016] and DESeq2 [@Love2014], the beta-binomial model used by
  corncorb [@Martin2021].

- Zero-inflated mixture models has aslo been proposed to address zero
  inflation, such as zero-inflated log-normal/normal mixture model
  used by metagenomeSeq [@Paulson2017] and RAIDA [@Sohn2015],
  zero-inflated beta-binomial model used by ZIBB [@ZIBB2018], and
  zero-inflated negative binomial model used by Omnibus
  [@Omnibus2018].

- Bayesian methods have been used to impute the zeros for methods
  working on proportion data, accounting for sampling variability and
  sequencing depth variation. Examples include ALDEx2 [@Gloor2016] and
  eBay [@Liu2020].

- Other methods use the pseudo-count approach to impute the zeros,
  such as MaAsLin2 [@Mallick2020] and ANCOMBC [@ancombc2020].

- Different strategies have been used to address compositional
  effects, including:

  - Robust normalization. For example, trimmed mean of M-values (TMM)
    normalization used by edgeR, relative log expression (RLE)
    normalization used by DESeq2 [@Love2014], cumulative sum scaling
    (CSS) normalization used by metagenomeSeq, centered log-ratio
    transformation (CLR) normalization used by ALDEx2 [@Gloor2016] and
    geometric mean of pairwise ratios (GMPR) normalization used by
    Omnibus [@Omnibus2018]. Wrench normalization [@Kumar2018] corrects
    the compositional bias by an empirical Bayes approach, which has
    been recommended in metagenomeSeq [@Paulson2017] tutorial.
  
  - Reference taxa approach used by DACOMP [@Brill2019] and RAIDA [@Sohn2015].
  
  - Analyzing the pattern of pairwise log ratios, such as ANCOM [@Mandal2015].
  
  - Bias-correction used by ANCOMBC [@ancombc2020].

Some of the popular tools for differential abundance analysis include:

- ALDEx2 [@Gloor2016] 
- ANCOMBC [@ancombc2020]
- corncob [@Martin2021]
- DESeq2 [@Love2014] 
- edgeR [@Chen2016]
- lefser [@Khlebrodova2021]
- MaAsLin2 [@Mallick2020]
- metagenomeSeq [@Paulson2017]
- limma [@Ritchie2015]
- LinDA [@Zhou2022]
- ZicoSeq [@Yang2022]
- LDM [@Hu2020]
- RAIDA [@Sohn2015]
- DACOMP [@Brill2019]
- Omnibus [@Omnibus2018]
- eBay [@Liu2020]
- ZINQ [@Ling2021]
- ANCOM [@Mandal2015]
- fastANCOM [@fastANCOM2022]
- [t-test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/t.test)  
- [Wilcoxon test](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/wilcox.test)  


We recommend to have a look at @Nearing2022 who compared all these
methods across 38 different datasets. Because different methods use
different approaches (parametric vs non-parametric, different
normalization techiniques, assumptions etc.), the results may differ
between methods, sometimes substantially as @Nearing2022 pointed
out. More recently @Yang2022 comprehensively evaluated these methods
via a semi-parametric framework and 106 real datasets. @Yang2022 also
concluded that different DA methods can sometimes produce discordant
results, opening to the possibility for cherry-picking tools in favor
of oneâ€™s own hypothesis. Therefore it is highly recommended to pick
several methods to assess how robust and potentially reproducible your
findings are with different methods.

In this section we demonstrate the use of four methods that can be
recommended based on recent literature (ANCOM-BC [@ancombc2020],
_ALDEx2_ [@Gloor2016], _Maaslin2_ [@Mallick2020], _LinDA_ [@Zhou2022]
and _ZicoSeq_ [@Yang2022]).

The purpose of this section is to show how to perform DAA in R, not
how to correctly do causal inference. Depending on your experimental
setup and your theory, you must determine how to specify any model
exactly.  E.g., there might be confounding factors that might drive
(the absence of) differences between the shown groups that we ignore
here for simplicity. Or your dataset is repeated sampling design,
matched-pair design or the general longitudianl design.  We will
demonstrate how to include covariates in those models. We picked a
dataset that merely has microbial abundances in a TSE object as well
as a grouping variable in the sample data. We simplify the examples by
only including two of the three groups.


```{r load-pkg-data}
library(mia)
library(patchwork)
library(tidySummarizedExperiment)
library(knitr)
library(tidyverse)
library(ALDEx2)
library(Maaslin2)
library(MicrobiomeStat)
library(ANCOMBC)
library(GUniFrac)

# set random seed because some tools can randomly vary and then produce 
# different results:
set.seed(13253)

# we use a demo dataset and restrict it to two geo locations
# for easy illustration
data(peerj13075)
tse0 <- peerj13075
tse0 <- tse0[ ,tse0$Geographical_location %in% c("Pune", "Nashik")]
# Let us make this a factor
tse0$Geographical_location <- factor(tse0$Geographical_location)

# how many observations do we have per group?
as.data.frame(colData(tse0)) %>% 
count(Geographical_location) %>%
  kable()
```

### Prevalence Filtering 

Before we jump to our analyses, we may want to perform some data
manipulation.

Let us here do aggregation to genus level, add relative abundance
assay, and perform prevalence filtering.

```{r datasetup}
tse <- agglomerateByRank(tse0, rank = "genus") %>%
       transformCounts(assay.type = "counts",
                       method = "relabundance",
		       MARGIN = "samples") %>%
       # subset based on the relative abundance assay		       
       subsetByPrevalentTaxa(detection = 0,
                             prevalence = 10/100,
			     assay.type = "relabundance")

# Add also clr abundances
tse <- transformCounts(tse, method="clr", pseudocount=1) # not bale to run
```

Regarding prevalence filtering, @Nearing2022 found that applying a 10%
threshold for the prevalence of the taxa generally resulted in more
robust results. Some tools have builtin arguments for that. By
applying the threshold to our input data, we can make sure it is
applied for all tools. 



### ALDEx2

In this section, we will show how to perform a simple ALDEx2 analysis. 
If you wanted to pick a single method, this method could be recommended to use.
According to the developers experience, it tends to identify the common
features identified by other methods. This statement is in line with a recent
independent evaluation by @Nearing2022.  
Please also have a look at the more extensive 
[vignette](https://bioconductor.org/packages/release/bioc/vignettes/ALDEx2/inst/doc/ALDEx2_vignette.html) 
that covers this flexible tool in more depth. ALDEx2 estimates technical
variation within each sample per taxon by utilizing the Dirichlet distribution.
It furthermore applies the centered-log-ratio transformation (or closely
related log-ratio transforms). Depending on the experimental setup, it will
perform a two sample Welch's T-test and Wilcoxon-test or a one-way ANOVA and
Kruskal-Wallis-test. For more complex study designs, there is a possibility to 
utilize the `glm` functionality within ALDEx2.

The Benjamini-Hochberg procedure is applied by default to correct for
multiple testing. Below we show a simple example that illustrates the
workflow.


```{r, aldex2, eval=TRUE}
# Generate Monte Carlo samples of the Dirichlet distribution for each sample.
# Convert each instance using the centered log-ratio transform.
# This is the input for all further analyses.
set.seed(254)
x <- aldex.clr(assay(tse), tse$Geographical_location)     
```


The t-test:

```{r, aldex2_ttest, eval=TRUE}
# calculates expected values of the Welch's t-test and Wilcoxon rank
# test on the data returned by aldex.clr
x_tt <- aldex.ttest(x, paired.test = FALSE, verbose = FALSE)
```


Effect sizes:

```{r, aldex2_efs, eval=TRUE}
# Determines the median clr abundance of the feature in all samples and in
# groups, the median difference between the two groups, the median variation
# within each group and the effect size, which is the median of the ratio
# of the between group difference and the larger of the variance within groups
x_effect <- aldex.effect(x, CI = TRUE, verbose = FALSE)

# combine all outputs 
aldex_out <- data.frame(x_tt, x_effect)
```

Now, we can create a so called Bland-Altman or MA plot (left). It shows the
association between the relative abundance and the magnitude of the difference
per sample. Next to that, we can also create a plot that shows the dispersion
on the x-axis instead of log-ratio abundance. Red dots represent genera that are
differentially abundant ($q \leq 0.1$) between the 2 groups. Black points are
rare taxa and grey ones are abundant taxa. The dashed line represent an effect
size of 1. See @Gloor2016 to learn more about these plots.

```{r, eval=TRUE}
par(mfrow = c(1, 2))
  aldex.plot(
    aldex_out, 
    type = "MA", 
    test = "welch", 
    xlab = "Log-ratio abundance",
    ylab = "Difference",
    cutoff = 0.05
  )
  aldex.plot(
    aldex_out, 
    type = "MW", 
    test = "welch",
    xlab = "Dispersion",
    ylab = "Difference",
    cutoff = 0.05
  )
```

The evaluation as differential abundant in above plots is based on the
corrected p-value. According to the ALDEx2 developers, the safest
approach is to identify those features where the 95% CI of the effect
size does not cross 0. As we can see in below table, this is not the
case for any of the identified genera (see overlap column, which
indicates the proportion of overlap). Also, the authors recommend to
focus on effect sizes and CIs rather than interpreting the p-value. To
keep the comparison simple, we will here use the p-value as decision
criterion. But please be aware that the effect size together with the
CI is a better answer to the question we are typically interested in
(see also [this
article](https://www.nature.com/articles/d41586-019-00857-9)).


```{r, eval=TRUE}
rownames_to_column(aldex_out, "genus") %>%
  filter(wi.eBH <= 0.05)  %>% # here we chose the wilcoxon output rather than tt
  dplyr::select(genus, we.eBH, wi.eBH, effect, overlap) %>%
  kable()
```

### ANCOM-BC

The analysis of composition of microbiomes with bias correction
(ANCOM-BC) [@ancombc2020] is a recently developed method for differential
abundance testing. It is based on an earlier published approach
[@Mandal2015].  The previous version of ANCOM was among the methods
that produced the most consistent results and is probably a
conservative approach [@Nearing2022].  However, the new ANCOM-BC
method operates quite differently compared to the former ANCOM method.

As the only method, ANCOM-BC incorporates the so called *sampling
fraction* into the model. The latter term could be empirically
estimated by the ratio of the library size to the microbial
load. According to the authors, variations in this sampling fraction
would bias differential abundance analyses if ignored.  Furthermore,
this method provides p-values and confidence intervals for each
taxon. It also controls the FDR and it is computationally simple to
implement.

Note that the original method was implemented in the `ancombc()` function (see 
[extended tutorial](https://www.bioconductor.org/packages/release/bioc/vignettes/ANCOMBC/inst/doc/ANCOMBC.html)).
The method has since then been updated and new features have been added to enable
multi-group comparisons and repeated measurements among other improvements. 
We do not cover the more advanced features of ANCOMBC in this tutorial 
as these features are documented in detail in this 
[tutorial](https://www.bioconductor.org/packages/release/bioc/vignettes/ANCOMBC/inst/doc/ANCOMBC2.html).

We now proceed with a simple example.  First, we specify a formula. In this 
formula, other covariates could potentially be included to adjust for 
confounding. We show this further below. Again, please make sure to check the 
[function documentation](https://rdrr.io/github/FrederickHuangLin/ANCOMBC/man/ancombc.html)
as well as the linked tutorials to learn about the additional arguments 
that we specify.


```{r ancombc2, warning = FALSE, eval=TRUE}
# Run ANCOM-BC 
out <- ancombc2(
  data = tse,
  assay_name = "counts", 
  tax_level = "genus", 
  fix_formula = "Geographical_location", 
  p_adj_method = "fdr", 
  lib_cut = 0,
  prv_cut = 0,
  group = "Geographical_location", 
  struc_zero = TRUE, 
  neg_lb = TRUE,
  alpha = 0.05, 
  global = TRUE # multi group comparison will be deactivated automatically 
)
```

```{r ancombc2b, warning = FALSE, eval=TRUE}
# store the FDR adjusted results [test on v2.0.3] 
ancombc_result <- cbind.data.frame(taxid = out$res$taxon,
                       ancombc = as.vector(out$res$q_Geographical_locationPune))
```




```{r ancombc2c, warning = FALSE, eval=TRUE}
# store the FDR adjusted results [test on v1.2.2]
ancombc_result <- out$res %>% dplyr::select(starts_with(c("taxon", "lfc", "q")))
```

The object `out` contains all model output. Again, see the
[documentation of the
function](https://rdrr.io/github/FrederickHuangLin/ANCOMBC/man/ancombc.html)
under **Value** for details. Our question whether taxa are
differentially abundant can be answered by looking at the `res`
object, which contains dataframes with the coefficients, standard
errors, p-values and q-values. Below we show the first entries of this
dataframe.

```{r ancom_kable, eval=TRUE}
kable(head(ancombc_result))
```



### MaAsLin2 

Let us next illustrate MaAsLin2 [@Mallick2020]. This method is based on
generalized linear models and flexible for different study designs
and covariate structures. For details, check their
[Biobakery tutorial](https://github.com/biobakery/biobakery/wiki/maaslin2).

```{r maaslin2, results = "hide", eval=TRUE}
# maaslin expects features as columns and samples as rows 
# for both the abundance table as well as metadata 

# We can specify different GLMs/normalizations/transforms.
# Let us use similar settings as in Nearing et al. (2021):
maaslin2_out <- Maaslin2(
  t(assay(tse)),
  data.frame(colData(tse)),
  output = "DAA example",
  transform = "AST",
  fixed_effects = "Geographical_location",
  # random_effects = c(...), # you can also fit MLM by specifying random effects
  # specifying a ref is especially important if you have more than 2 levels
  reference = "Geographical_location,Pune",  
  normalization = "TSS",
  standardize = FALSE,
  min_prevalence = 0 # prev filterin already done
)
```

Which genera are identified as differentially abundant? (leave out "head" to see all).

```{r, maaslin2kable, eval=TRUE}
kable(head(filter(maaslin2_out$results, qval <= 0.05)))
```

This will create a folder that is called like in the output specified
above. It contains also figures to visualize difference between
significant taxa.


### LinDA 

Lastly, we cover linear models for differential abundance analysis of
microbiome compositional data (@Zhou2022). This is very similar to
ANCOMBC with few differences: 1) LinDA correct for the compositional
bias differently using the mode of all regression coefficients. 2) it
is faster (100x-1000x than ANCOMBC and according to the authors); 3)
it supports hierarchical models. The latest ANCOMBC versions are also
supporting hierarchical models. Nevertheless, LinDA seems a promising
tool that achieves a very good power/fdr trade-off together with
ANCOMBC according to the review. The speed improvements might make it
critical especially for datasets that have higher sample or feature
set sizes.


```{r linda, eval=TRUE}
meta <- as.data.frame(colData(tse)) %>% dplyr::select(Geographical_location)
linda.res <- linda(
  as.data.frame(assay(tse)), 
  meta, 
  formula = '~Geographical_location', 
  alpha = 0.05, 
  prev.filter = 0, 
  mean.abund.filter = 0)

linda_out <- linda.res$output$Geographical_locationPune
```

```{r linda2, eval=TRUE}
# to scan the table for genera where H0 could be rejected:
kable(head(filter(as.data.frame(linda_out), reject)))
```

### ZicoSeq

Subsequently, we add a linear model and permutation-based method, see
details at [tutorial](https://cran.r-project.org/web/packages/GUniFrac/vignettes/ZicoSeq.html).

This approach has been assessed to exhibit high power and a low false
discovery rate, which has the following components:

  1. Winsorization to decrease the influence of outliers;
  
  1. Posterior sampling based on a beta mixture prior to address
  sampling variability and zero inflation;
  
  1. Reference-based multiple-stage normalization to address
  compositional effects;


```{r ZicoSeq, eval=TRUE}
set.seed(123)
meta <- as.data.frame(colData(tse))
zicoseq.obj <- GUniFrac::ZicoSeq(meta.dat = meta, 
                                 feature.dat = as.matrix(assay(tse)),
                                 grp.name = 'Geographical_location',
                                 adj.name = NULL, 
                                 feature.dat.type = 'count',
                                 prev.filter = 0,
                                 perm.no = 999,
                                 mean.abund.filter = 0,
                                 max.abund.filter = 0,
                                 return.feature.dat = T)
zicoseq_out <- cbind.data.frame(p.raw=zicoseq.obj$p.raw, p.adj.fdr=zicoseq.obj$p.adj.fdr) 


```{r ZicoSeq_kable, eval=TRUE}
kable(head(filter(zicoseq_out, p.adj.fdr<0.05)))
```


```{r ZicoSeqplot, eval=TRUE}
## x-axis is the effect size: R2 * direction of coefficient
ZicoSeq.plot(ZicoSeq.obj = zicoseq.obj,
             meta.dat = meta,
	     pvalue.type ='p.adj.fdr')
```



### Comparison of methods

The different methods yield somewhat different results but they could
be also expected to overlap to a substantial degree. As an exercirse,
you can compare the outcomes between the different methods in terms of
effect sizes, significances, or other aspects that are comparable
between the methods.




## Confounding variables

Confounders are common in experimental research. In general, these can be
classified into 3 types:

- Biological confounder, such as age, sex, etc. 

- Technical confounder that caused by data collection, storage, DNA
  extraction, sequencing process, etc.

- Confounder caused by experimental models, such as cage effect,
  sample background, etc.

Adjusting confounder is necessary and important to reach a valid
conclusion. To perform causal inference, it is crucial that the method
is able to include covariates in the model. This is not possible with
e.g. the Wilcoxon test. Other methods such as DESeq2, edgeR, ANCOMBC,
LDM, Aldex2, Corncob, MaAsLin2, ZicoSeq, fastANCOM and ZINQ allow
this. Below we show how to include a confounder/covariate in ANCOMBC,
LinDA and ZicoSeq.


### ANCOMBC

```{r ancombc_adj, eval=TRUE}
# perform the analysis 
ancombc_cov <- ancombc2(
  data = tse,
  assay_name = "counts",
  tax_level = "genus",
  fix_formula = "Geographical_location + Age", 
  p_adj_method = "fdr", 
  lib_cut = 0, 
  group = "Geographical_location", 
  struc_zero = TRUE, 
  neg_lb = TRUE,
  alpha = 0.05, 
  global = TRUE # multi group comparison will be deactivated automatically 
)
# now the model answers the question: holding Age constant, are 
# bacterial taxa differentially abundant? Or, if that is of interest,
# holding phenotype constant, is Age associated with bacterial abundance?
# Again we only show the first 6 entries.
```

```{r ancombc_adj2, eval=TRUE}
tab <- ancombc_cov$res %>% dplyr::select(starts_with(c("taxon", "lfc", "q")))
kable(head(tab))
```

### LinDA


```{r linda_adj, eval=TRUE}

linda_cov <- linda(
  as.data.frame(assay(tse, "counts")), 
  as.data.frame(colData(tse)), 
  formula = '~ Geographical_location + Age', 
  alpha = 0.05, 
  prev.filter = 0, 
  mean.abund.filter = 0)
linda.res <- linda_cov$output$Geographical_locationPune
```

```{r linda_adj2, eval=TRUE}
kable(head(filter(linda.res, reject==T)))
```



### ZicoSeq

```{r ZicoSeq_adj, eval=TRUE}
set.seed(123)
zicoseq.obj <- GUniFrac::ZicoSeq(meta.dat = as.data.frame(colData(tse)) , 
                                 feature.dat = as.matrix(assay(tse)),
                                 grp.name = 'Geographical_location',
                                 adj.name = 'Gender', 
                                 feature.dat.type = 'count',
                                 prev.filter = 0,
                                 perm.no = 999,
                                 mean.abund.filter = 0,
                                 max.abund.filter = 0,
                                 return.feature.dat = T)
zicoseq_out <- cbind.data.frame(p.raw=zicoseq.obj$p.raw,
                                p.adj.fdr=zicoseq.obj$p.adj.fdr) 
```

```{r ZicoSeq_adj2, eval=TRUE}
kable(head(filter(zicoseq_out, p.adj.fdr<0.05)))
```



## Tree-based methods

Let us next cover phylogeny-aware methods to perform group-wise
associations.

### Group-wise associations testing based on balances

For testing associations based on balances, check the philr
R/Bioconductor package.


### Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```



<!--chapter:end:30_differential_abundance.Rmd-->

# Machine learning {#machine_learning}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

Machine learning (ML) is a part of artificial intelligence. There are multiple
definitions, but "machine" refers to computation and "learning" to improving 
performance based on the data by finding patterns from it. Machine learning
includes wide variety of methods from simple statistical methods to more 
complex methods such as neural-networks. 

Machine learning can be divided into supervised and unsupervised machine learning.
Supervised ML is used to predict outcome based on the data. Unsupervised ML is used, 
for example, to reduce dimensionality (e.g. PCA) and to find clusters from the 
data (e.g., k-means clustering). 


## Supervised machine learning

"Supervised" means that the training data is introduced before. The training data
contains labels (e.g., patient status), and the model is fitted based on the 
training data. After fitting, the model is utilized to predict labels of data whose 
labels are not known. 

```{r superML1}
library(mia)

# Load experimental data
data(peerj13075, package="mia")
tse <- peerj13075
```

Let's first preprocess the data.

```{r super2}
# Agglomerate data
tse <- agglomerateByRank(tse, rank = "order")

# Apply CLR transform
tse <- transformCounts(tse, assay.type = "counts", method = "clr",
                       MARGIN="samples", pseudocount=1)

# Get assay
assay <- assay(tse, "clr")
# Transpose assay
assay <- t(assay)

# Convert into data.frame
df <- as.data.frame(assay)

# Add labels to assay
labels <- colData(tse)$Diet
labels <- as.factor(labels)
df$diet <- labels 

df[5, 5]
```

In the example below, we use [mikropml](https://journals.asm.org/doi/10.1128/mBio.00434-20)
package. We try to predict the diet type based on the data.

```{r super3}
library(mikropml)

# Run random forest 
results <- run_ml(df, "rf", outcome_colname = "diet", 
                  kfold = 2, cv_times = 5, training_frac = 0.8)

# Print result
confusionMatrix(data = results$trained_model$finalModel$predicted, 
                reference = results$trained_model$finalModel$y)
```

mikropml offers easier interface to [caret](https://cran.r-project.org/web/packages/caret/index.html) 
package. However, we can also use it directly.

Let's use xgboost model which is another commonly used algorithm in bioinformatics.

```{r super4}
# Set seed for reproducibility
set.seed(6358)

# Specify train control
train_control <- trainControl(method = "cv", number = 5,
                              classProbs = TRUE, 
                              savePredictions = "final",
                              allowParallel = TRUE)

# Specify hyperparameter tuning grid
tune_grid <- expand.grid(nrounds = c(50, 100, 200),
                         max_depth = c(6, 8, 10),
                         colsample_bytree = c(0.6, 0.8, 1),
                         eta = c(0.1, 0.3),
                         gamma = 0,
                         min_child_weight = c(3, 4, 5),
                         subsample = c(0.6, 0.8)
                         )

# Train the model, use LOOCV to evaluate performance
model <- train(x = assay, 
               y = labels, 
               method = "xgbTree",
               objective = "binary:logistic",
               trControl = train_control,
               tuneGrid = tune_grid,
               metric = "AUC",
               verbosity = 0
)

```

Let's create ROC curve which is a commonly used method in binary classification.
For unbalanced data, you might want to plot precision-recall curve. 

```{r super5}
library(MLeval)

# Calculate different evaluation metrics
res <- evalm(model, showplots = FALSE)

# Use patchwork to plot ROC and precision-recall curve side-by-side
library(patchwork)
res$roc + res$proc + 
    plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```

## Unsupervised machine learning

"Unsupervised" means that the labels (e.g., patient status is not known), 
and patterns are learned based only the abundance table, for instance. 
Unsupervised ML is also known as a data mining where patterns are extracted 
from big datasets. 

For unsupervised machine learning, please refer to chapters that are listed below:

- Chapter \@ref(clustering)
- Chapter \@ref(community-similarity) 

### Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```


<!--chapter:end:40_machine_learning.Rmd-->

# Multi-assay analyses {#multi-assay-analyses}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

```{r load-pkg-data}
library(mia)
```

Multi-omics approaches integrate data from multiple sources. For
example, we can integrate taxonomic abundance profiles with
metabolomic or other biomolecular profiling data to observe
associations, make predictions, or aim at causal
inferences. Integrating evidence across multiple sources can lead to
enhanced predictions, more holistic understanding, or facilitate the
discovery of novel biomarkers. In this section we demonstrate common
multi-assay data integration tasks.

Cross-correlation analysis is a straightforward approach that can
reveal assocation strengths and types between data sets. For instance,
we can analyze if higher presence of a specific taxon equals to higher
levels of a biomolecule.

The analyse can be facilitated by the multi-assay data containers,
_TreeSummarizedExperiment_ and _MultiAssayExperiment_. These are
scalable and contain different types of data in a single container,
making this framework particularly suited for multi-assay microbiome
data incorporating different types of complementary data sources in a
single reproducible workflow. Th different solutions for varying data
integration needs are discussed in more detail in Section
\@ref(containers). Another experiment can be stored in _altExp_ slot
of SE data container or both experiments can be stored side-by-side in
MAE data container (see the sections \@ref(alt-exp) and \@ref(mae) to
learn more about altExp and MAE objects, respectively). Different
experiments are first imported into these data containers similarly to
the case when only one experiment is present. After that, the
different experiments can be combined into the same multi-assay data
container. The result is one TreeSE object with alternative experiment in
altExp slot, or MAE object with multiple experiment in its experiment
slot, for instance.

As an example data, we use data from the following publication: Hintikka L
_et al._ (2021) Xylo-oligosaccharides in prevention of hepatic
steatosis and adipose tissue inflammation: associating taxonomic and
metabolomic patterns in fecal microbiota with biclustering

[@Hintikka2021].

In this study, mice were fed with high-fat and low-fat diets with or
without prebiotics.  The purpose of this was to study whether prebiotics
reduce negative impacts of a high-fat diet.

This example data can be loaded from microbiomeDataSets. The data is
already in MAE format. It includes three different experiments:
microbial abundance data, metabolite concentrations, and data about
different biomarkers.
=======
[@Hintikka2021]. In this article, mice were fed with high-fat and
low-fat diets with or without prebiotics.  The purpose of this was to
study if prebiotics would reduce the negative impact of high-fat diet.

This example data is readily available in the MultiAssayExperiment
format. It includes three different experiments: microbial abundance
data, metabolite concentrations, and data about different
biomarkers. If you like to construct the same data object from the
original files instead, [Here](https://microbiome.github.io/OMA/containers.html#loading-experimental-microbiome-data) 
you can find help for importing data into a SE object.

```{r cross-correlation1}
# Load the data
data(HintikkaXOData, package = "mia")
mae <- HintikkaXOData
mae
```

```{r cross-correlation2}
library(stringr)
# Drop off those bacteria that do not include information in Phylum or lower levels
mae[[1]] <- mae[[1]][!is.na(rowData(mae[[1]])$Phylum), ]
# Clean taxonomy data, so that names do not include additional characters
rowData(mae[[1]]) <- DataFrame(apply(rowData(mae[[1]]), 2, 
                                     str_remove, pattern = "._[0-9]__"))
# Microbiome data
mae[[1]]
```

```{r cross-correlation3}
# Metabolite data
mae[[2]]
```

```{r cross-correlation4}
# Biomarker data
mae[[3]]
```

## Cross-correlation Analysis


Next we can do the cross-correlation analysis. Let us analyse if
individual bacteria genera correlates with concentrations of
individual metabolites. This helps to answer the question: "If this
bacteria is present, is this metabolite's concentration then low or
high"?

```{r cross-correlation5}
# Agglomerate microbiome data at family level
mae[[1]] <- agglomerateByPrevalence(mae[[1]], rank = "Family")
# Does log10 transform for microbiome data
mae[[1]] <- transformCounts(mae[[1]], method = "log10", pseudocount = 1)

# Give unique names so that we do not have problems when we are creating a plot
rownames(mae[[1]]) <- getTaxonomyLabels(mae[[1]])

# Cross correlates data sets
correlations <- testExperimentCrossCorrelation(mae, 
                                               experiment1 = 1,
                                               experiment2 = 2,
                                               assay.type1 = "log10", 
                                               assay.type2 = "nmr",
                                               method = "spearman", 
                                               p_adj_threshold = NULL,
                                               cor_threshold = NULL,
                                               # Remove when mia is fixed
                                               mode = "matrix",
                                               sort = TRUE,
                                               show_warnings = FALSE)
```

Creates the heatmap

```{r cross-correlation6, fig.width=10, fig.height=8}
library("ComplexHeatmap") 

# Create a heatmap and store it
plot <- Heatmap(correlations$cor,
                # Print values to cells
                cell_fun = function(j, i, x, y, width, height, fill) {
                    # If the p-value is under threshold
                    if( !is.na(correlations$p_adj[i, j]) & correlations$p_adj[i, j] < 0.05 ){
                        # Print "X"
                        grid.text(sprintf("%s", "X"), x, y, gp = gpar(fontsize = 8, col = "black"))
                        }
                    },
                heatmap_legend_param = list(title = "", legend_height = unit(5, "cm"))
                )
plot
```

## Multi-Omics Factor Analysis

Multi-Omics Factor Analysis [@Argelaguet2018] (MOFA) is an
unsupervised method for integrating multi-omic data sets in a
downstream analysis.  It could be seen as a generalization of
principal component analysis. Yet, with the ability to infer a latent
(low-dimensional) representation, shared among the multiple (-omics)
data sets in hand.

We use the R [MOFA2](https://biofam.github.io/MOFA2/index.html)
package for the analysis, and
[install](https://biofam.github.io/MOFA2/installation.html) the
corresponding dependencies.

```{r MOFA2, message=FALSE, warning=FALSE}
library(MOFA2)

# For inter-operability between Python and R, and setting Python dependencies,
# reticulate package is needed
library(reticulate)
# Let us assume that these have been installed already.
#reticulate::install_miniconda(force = TRUE)
#reticulate::use_miniconda(condaenv = "env1", libraryd = FALSE)
#reticulate::py_install(packages = c("mofapy2"), pip = TRUE, python_version=3.6)
```

The `mae` object could be used straight to create the MOFA model. Yet,
we transform our assays since the model assumes normality per
default. We can also use Poisson or Bernoulli distributions among others.

```{r, message=FALSE, warning=FALSE}
library(MOFA2)
# For simplicity, classify all high-fat diets as high-fat, and all the low-fat 
# diets as low-fat diets
colData(mae)$Diet <- ifelse(colData(mae)$Diet == "High-fat" | 
                              colData(mae)$Diet == "High-fat + XOS", 
                            "High-fat", "Low-fat")

# Removing duplicates at the microbiome data
# which are also in form e.g. "Ambiguous" and "uncultured" taxa
mae[[1]] <- mae[[1]][!duplicated(rownames(assay(mae[[1]]))), ]

# Transforming microbiome data with rclr
mae[[1]] <- transformCounts(mae[[1]], method = "relabundance")
mae[[1]] <- transformCounts(mae[[1]], assay.type = "relabundance", method = "rclr")

# Transforming metabolomic data with log10
mae[[2]] <- transformCounts(mae[[2]], assay.type = "nmr",
                            MARGIN = "samples",
                            method = "log10")

# Transforming biomarker data with z-transform
mae[[3]] <- transformCounts(mae[[3]], assay.type = "signals",
                            MARGIN = "features",
                            method = "z", pseudocount = 1)

# Removing assays no longer needed
assay(mae[[1]], "counts") <- NULL
assay(mae[[1]], "log10") <- NULL
assay(mae[[2]], "nmr") <- NULL
assay(mae[[3]], "signals") <- NULL

# Building our mofa model
model <- create_mofa_from_MultiAssayExperiment(mae,
                                               groups = "Diet", 
                                               extract_metadata = TRUE)
model
```

Model options could be defined as follows:

```{r, message=FALSE, warning=FALSE}
model_opts <- get_default_model_options(model)
model_opts$num_factors <- 5
head(model_opts)
```

Model's training options are defined with the following:

```{r, message=FALSE, warning=FALSE}
train_opts <- get_default_training_options(model)
head(train_opts)
```

Preparing and training the model:

```{r, message=FALSE, warning=FALSE}
model.prepared <- prepare_mofa(
  object = model,
  model_options = model_opts
)

# Some systems may library the specification `use_basilisk = TRUE`
# so it has been added to the following code
model.trained <- run_mofa(model.prepared, use_basilisk = TRUE)
```

Visualizing the variance explained:

```{r, message=FALSE, warning=FALSE, fig.height=8, fig.width=10}
library(patchwork)
library(ggplot2)
wrap_plots(
    plot_variance_explained(model.trained, x="view", y="factor", plot_total = T),
    nrow = 2
) + plot_annotation(title = "Variance Explained per factor and assay",
                    theme = theme(plot.title = element_text(hjust = 0.5)))
```

The top weights for each assay using all five factors:

```{r, warning=FALSE, message=FALSE, fig.height=10, fig.width=10}
plots <- lapply(c("microbiota", "metabolites","biomarkers"), function(name) {
    plot_top_weights(model.trained,
                     view = name,
                     factors = "all",
                     nfeatures = 10) +
        labs(title = paste0("Top weights of the ", name," assay"))
})
wrap_plots(plots, nrow = 3) & theme(text = element_text(size = 8))
```

More tutorials and examples of using the package are found at [link](https://biofam.github.io/MOFA2/tutorials.html)



### Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```


<!--chapter:end:23_multi-assay_analyses.Rmd-->

# Visualization {#viz-chapter}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

Data visualization will inevitably shape interpretation and motivate
the next steps of the analysis. A variety of visualization methods are
available for microbiome analysis but the application librarys careful
attention to details. Knowledge on the available tools and their
limitations plays an important role in selecting the most suitable
methods to address a given question.

This chapter introduces the reader to a number of visualization
techniques found in this book, such as:

* barplots
* boxplots
* heatmaps
* ordination charts
* regression charts
* trees

The toolkit which provides the essential plotting functionality
includes the following packages:

* _patchwork_, _cowplot_, _ggpubr_ and _gridExtra_: plot layout and multi-panel plotting
* _miaViz_: specific visualization tools for `TreeSummaizedExperiment` objects
* _scater_: specific visualization tools for `SingleCellExperiment` objects
* _ggplot2_, _pheatmap_, _ggtree_, _sechm_: composition heatmaps
* _ANCOMBC_, _ALDEx2_ and _Maaslin2_: visual differential abundance
* _fido_: tree-based methods for differential abundance
* _plotly_: animated and 3D plotting

For systematic and extensive tutorials on the visual tools available
in _mia_, readers can refer to the following material:

* [microbiome tutorials](https://microbiome.github.io/tutorials/)

```{r, include = FALSE}
library(ggplot2)
theme_set(theme_classic())
library(mia)
library(scater)
library(patchwork)
library(miaViz)
library(sechm)
library(reshape2)
library(pheatmap)
library(ape)
library(ggtree)
# essential data
data("GlobalPatterns", package = "mia")
tse <- GlobalPatterns
```

## Pre-analysis exploration

### Accessing row and column data

`SCE` and `TreeSE` objects contain multiple layers of information in the
form of rows, columns and meta data. The _scater_ package supports in
accessing, modifying and graphing the meta data related to features as
well as samples.

```{r}
# list row meta data
names(rowData(tse))
# list column meta data
names(colData(tse))
```

Such meta data can be directly plotted with the functions
`plotRowData` and `plotColData`.

```{r, warning = FALSE}
# obtain QC data
tse <- addPerCellQC(tse)
tse <- addPerFeatureQC(tse)
# plot QC Mean against Species
plotRowData(tse, "mean", "Species") +
  theme(axis.text.x = element_blank()) +
  labs(x = "Species", y = "QC Mean")
# plot QC Sum against Sample ID, colour-labeled by Sample Type
plotColData(tse, "sum", "X.SampleID", colour_by = "SampleType") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Sample ID", y = "QC Sum")
```

Alternatively, they can be converted to a `data.frame` object and
passed to `ggplot`.

```{r}
# store colData into a data frame
coldata <- as.data.frame(colData(tse))
# plot Number of Samples against Sampling Site
ggplot(coldata, aes(x = SampleType)) +
  geom_bar(width = 0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Sampling Site",
       y = "Number of Samples")
```

Further methods of application can be found in the chapters \@ref(qc)
and \@ref(richness) and in a few [external
tutorials](https://github.com/davismcc/scater_tutorials_open_data)
with open data. Additionally, `rowData` and `colData` allow
manipulation and subsetting of large data sets into smaller units, as
explained in chapter \@ref(datamanipulation).

### Viewing abundance and prevalence patterns

Prior-to-analysis exploration may involve questions such as how microorganisms
are distributed across samples (abundance) and what microorganisms are present
in most of the samples (prevalence). The information on abundance and prevalence
can be summarized into a **jitter** or **density plot** and a **tree**,
respectively, with the _miaViz_ package.

Specifically, the functions `plotAbundance`, `plotAbundanceDensity`
and `plotRowTree` are used, and examples on their usage are discussed
throughout chapter \@ref(quality-control).

## Diversity estimation

Alpha diversity is commonly measured as one of the diversity indices
explained in chapter \@ref(community-diversity). Because the focus
lies on each sample separately, one-dimensional plots, such as
**scatter**, **violin** and **box plots**, are suitable.

Beta diversity is generally evaluated as one of the dissimilarity
indices reported in chapter \@ref(community-similarity). Unlike alpha
diversity, samples are compared collectively to estimate the
heterogeneity across them, therefore multidimensional plots, such as
**Shepard** and **ordination plots** are suitable.

|                         | alpha diversity            | beta diversity            |
|:-----------------------:|:--------------------------:|:-------------------------:|
| used metrics            | diversity indices          | dissimilarity indices     |
|                         |                            |                           | 
| metric dimensionality   | one-dimensional            | multidimensional          |
|                         |                            |                           | 
| suitable visualization  | scatter, violin, box plots | Shepard, ordination plots |

In conclusion, visualization techniques for alpha and beta diversity
significantly differ from one another.

### Alpha diversity with scatter, violin and box plots

The basic method to visualize the diversity values assigned to the
different samples in a `TSE` object includes the following, where each
data point represents one sample:

```{r}
# estimate shannon diversity index
tse <- mia::estimateDiversity(tse, 
                              assay.type = "counts",
                              index = "shannon", 
                              name = "shannon")
# plot shannon diversity index, colour-labeled by Sample Type
plotColData(tse, "shannon", colour_by = "SampleType")
```

The several indices available for the evaluation of alpha diversity
often return slightly divergent results, which can be visually
compared with a multiple violin or box plot. For this purpose,
`plotColData` (for violin plots) or `ggplot` (for box plots) are
recursively applied to a number of diversity indices with the function
`lapply` and the multi-panel plotting functionality of the _patchwork_
package is then exploited.

```{r}
# estimate faith diversity index
tse <- mia::estimateFaith(tse,
                          assay.type = "counts")
# store colData into a data frame
coldata <- as.data.frame(colData(tse))
# generate plots for shannon and faith indices
# and store them into a list
plots <- lapply(c("shannon", "faith"),
                function(i) ggplot(coldata, aes_string(y = i)) +
                  geom_boxplot() +
                  theme(axis.text.x = element_blank(),
                        axis.ticks.x = element_blank()))
# combine plots with patchwork
plots[[1]] + plots[[2]]
```

The analogous output in the form of a violin plot is obtained in
chapter \@ref(faith-diversity). In addition, box plots that group
samples according to certain information, such as origin, sex, age and
health condition, can be labeled with p-values for significant
differences with the package _ggsignif_ package, as shown in chapter
\@ref(estimate-diversity).

### Beta diversity with Shepard and coordination plots

The _scater_ package offers the general function `plotReducedDim`. In
its basic form, it takes a `TSE` object and the results on sample
similarity stored in the same object, which can be evaluated with the
following coordination methods:

* `runMDS`
* `runNMDS`
* `runPCA`
* `runTSNE`
* `runUMAP`

Since these clustering techniques allow for multiple coordinates or
components, **coordination plots** can also span multiple dimensions,
which is explained in chapter \@ref(extras).

```{r}
# perform NMDS coordination method
tse <- runNMDS(tse,
               FUN = vegan::vegdist,
               name = "NMDS")
# plot results of a 2-component NMDS on tse,
# coloured-scaled by shannon diversity index
plotReducedDim(tse, "NMDS", colour_by = "shannon")
```

Multiple combinations of coordinates or dimensions can also be integrated into a multi-panel arrangement.

```{r}
# perform MDS coordination method
tse <- runMDS(tse,
              FUN = vegan::vegdist,
              method = "bray",
              name = "MDS",
              assay.type = "counts",
              ncomponents = 3)
# plot results of a 3-component MDS on tse,
# coloured-scaled by faith diversity index
plotReducedDim(tse, "MDS", ncomponents = c(1:3), colour_by = "faith")
```

Similarly to iterating `plotColData` over indices of alpha diversity,
`lapply` can be used in combination with _patchwork_ to recursively
apply `plotReducedDim` and visually compare results among various
coordination methods.

```{r}
# generate plots for MDS and NMDS methods
# and store them into a list
plots <- lapply(c("MDS", "NMDS"),
                plotReducedDim,
                object = tse,
                colour_by = "shannon")
# combine plots with patchwork
plots[[1]] + plots[[2]] +
  plot_layout(guides = "collect")
```

For similar examples, readers are referred to chapter
\@ref(community-similarity).  Further material on the graphic
capabilities of _patchwork_ is available in its [official package
tutorial](https://patchwork.data-imaginist.com/articles/patchwork.html).

## Statistical analysis

### Heatmaps

As described in chapter \@ref(visual-composition), bar plots and
heatmaps can offer a useful insight into the composition of a
community. Simple methods involve the functions `plotAbundance` and
`geom_tile` in combination with `scale_fill_gradientn` from the
packages _miaViz_ and _ggplot2_, respectively.

For instance, below the composition of multiple samples (x axis) is
reported in terms of relative abundances (y axis) for the top 10 taxa
at the Order rank.  Bar plots and heatmaps with analogous information
at the Phylum level are available in the aforementioned chapter.

```{r plotAbundance1}
# agglomerate tse by Order
tse_order <- agglomerateByRank(tse,
                                rank = "Order",
                                onRankOnly = TRUE)
# transform counts into relative abundance
tse_order <- transformCounts(tse_order,
                              assay.type = "counts",
                              method = "relabundance")
# get top orders
top_taxa <- getTopTaxa(tse_order,
                       top = 10,
                       assay.type = "relabundance")
# leave only names for top 10 orders and label the rest with "Other"
order_renamed <- lapply(rowData(tse_order)$Order,
                   function(x){if (x %in% top_taxa) {x} else {"Other"}})
rowData(tse_order)$Order <- as.character(order_renamed)
# plot composition as a bar plot
plotAbundance(tse_order,
              assay.type = "relabundance",
              rank = "Order",
              order_rank_by = "abund",
              order_sample_by = "Clostridiales")
```

To add a sample annotation, you can combine plots that you get from the output
of _plotAbundance_.

```{r plotAbundance2}
# Create plots
plots <- plotAbundance(tse_order,
      	    assay.type = "relabundance",
	    rank = "Order",
            order_rank_by = "abund",
	    order_sample_by = "Clostridiales",
            features = "SampleType")

# Modify the legend of the first plot to be smaller 
plots[[1]] <- plots[[1]] +
    theme(legend.key.size = unit(0.3, 'cm'),
          legend.text = element_text(size = 6),
          legend.title = element_text(size = 8))

# Modify the legend of the second plot to be smaller 
plots[[2]] <- plots[[2]] +
    theme(legend.key.height = unit(0.3, 'cm'),
          legend.key.width = unit(0.3, 'cm'),
          legend.text = element_text(size = 6),
          legend.title = element_text(size = 8),
          legend.direction = "vertical")

# Load required packages
library("ggpubr")
library("patchwork") 
# Combine legends
legend <- wrap_plots(as_ggplot(get_legend(plots[[1]])), as_ggplot(get_legend(plots[[2]])), ncol = 1) 

# Remove legends from the plots
plots[[1]] <- plots[[1]] + theme(legend.position = "none")
plots[[2]] <- plots[[2]] + theme(legend.position = "none", axis.title.x=element_blank()) 

# Combine plots
plot <- wrap_plots(plots[[2]], plots[[1]], ncol = 1, heights = c(2, 10))
# Combine the plot with the legend
wrap_plots(plot, legend, nrow = 1, widths = c(2, 1))
```

For more sophisticated visualizations than those produced with `plotAbundance`
and _ggplot2_, the packages _pheatmap_ and _sechm_ provide methods to include
feature and sample clusters in a heatmap, along with further functionality.

```{r pheatmap1}
# Agglomerate tse by phylum
tse_phylum <- agglomerateByRank(tse,
                                rank = "Phylum",
                                onRankOnly = TRUE)

# Add clr-transformation on samples
tse_phylum <- transformCounts(tse_phylum, MARGIN = "samples", method = "clr", assay.type = "counts", pseudocount=1)

# Add z-transformation on features (taxa)
tse_phylum <- transformCounts(tse_phylum, assay.type = "clr",
                              MARGIN = "features", 
                              method = "z", name = "clr_z")

# Take subset: only samples from feces, skin, or tongue
tse_phylum_subset <- tse_phylum[ , tse_phylum$SampleType %in% c("Feces", "Skin", "Tongue") ]

# Add clr-transformation
tse_phylum_subset <- transformCounts(tse_phylum_subset, method = "clr",
                                     MARGIN="samples",
                                     assay.type = "counts", pseudocount=1)
# Does z-transformation
tse_phylum_subset <- transformCounts(tse_phylum_subset, assay.type = "clr",
                                     MARGIN = "features", 
                                     method = "z", name = "clr_z")

# Get n most abundant taxa, and subsets the data by them
top_taxa <- getTopTaxa(tse_phylum_subset, top = 20)
tse_phylum_subset <- tse_phylum_subset[top_taxa, ]

# Gets the assay table
mat <- assay(tse_phylum_subset, "clr_z")

# Creates the heatmap
pheatmap(mat)
```

We can cluster both samples and features hierarchically and add them to the
x and y axes of the heatmap, respectively.

```{r pheatmap2}
# Hierarchical clustering
taxa_hclust <- hclust(dist(mat), method = "complete")

# Creates a phylogenetic tree
taxa_tree <- as.phylo(taxa_hclust)

# Plot taxa tree
taxa_tree <- ggtree(taxa_tree) + 
  theme(plot.margin=margin(0,0,0,0)) # removes margins

# Get order of taxa in plot
taxa_ordered <- get_taxa_name(taxa_tree)

# to view the tree, run
# taxa_tree
```

Based on phylo tree, we decide to create three clusters. 

```{r pheatmap3}
# Creates clusters
taxa_clusters <- cutree(tree = taxa_hclust, k = 3)

# Converts into data frame
taxa_clusters <- data.frame(clusters = taxa_clusters)
taxa_clusters$clusters <- factor(taxa_clusters$clusters)

# Order data so that it's same as in phylo tree
taxa_clusters <- taxa_clusters[taxa_ordered, , drop = FALSE] 

# Prints taxa and their clusters
taxa_clusters
```

The information on the clusters is then added to the feature meta data.

```{r pheatmap4}
# Adds information to rowData
rowData(tse_phylum_subset)$clusters <- taxa_clusters[order(match(rownames(taxa_clusters), rownames(tse_phylum_subset))), ]

# Prints taxa and their clusters
rowData(tse_phylum_subset)$clusters
```

Similarly, samples are hierarchically grouped into clusters, the most suitable
number of clusters for the plot is selected and the new information is stored
into the sample meta data.

```{r pheatmap5}
# Hierarchical clustering
sample_hclust <- hclust(dist(t(mat)), method = "complete")

# Creates a phylogenetic tree
sample_tree <- as.phylo(sample_hclust)

# Plot sample tree
sample_tree <- ggtree(sample_tree) + layout_dendrogram() + 
  theme(plot.margin=margin(0,0,0,0)) # removes margins

# Get order of samples in plot
samples_ordered <- rev(get_taxa_name(sample_tree))

# to view the tree, run
# sample_tree

# Creates clusters
sample_clusters <- factor(cutree(tree = sample_hclust, k = 3))

# Converts into data frame
sample_data <- data.frame(clusters = sample_clusters)

# Order data so that it's same as in phylo tree
sample_data <- sample_data[samples_ordered, , drop = FALSE] 

# Order data based on 
tse_phylum_subset <- tse_phylum_subset[ , rownames(sample_data)]

# Add sample type data
sample_data$sample_types <- unfactor(colData(tse_phylum_subset)$SampleType)

sample_data
```

Now we can create heatmap with additional annotations.

```{r pheatmap6}
# Determines the scaling of colorss
# Scale colors
breaks <- seq(-ceiling(max(abs(mat))), ceiling(max(abs(mat))), 
              length.out = ifelse( max(abs(mat))>5, 2*ceiling(max(abs(mat))), 10 ) )
colors <- colorRampPalette(c("darkblue", "blue", "white", "red", "darkred"))(length(breaks)-1)

pheatmap(mat, annotation_row = taxa_clusters, 
         annotation_col = sample_data,
         breaks = breaks,
         color = colors)
```

The package _sechm_ allows for further visual capabilities and flexibility.
In this case, the clustering step is automatically performed by the plotting
function and does not need to be executed in advance.

```{r sechm}
# Stores annotation colros to metadata
metadata(tse_phylum_subset)$anno_colors$SampleType <- c(Feces = "blue", 
                                                        Skin = "red", 
                                                        Tongue = "gray")

# Create a plot
sechm(tse_phylum_subset, 
      features = rownames(tse_phylum_subset), 
      assayName = "clr", 
      do.scale = TRUE, 
      top_annotation = c("SampleType"), 
      gaps_at = "SampleType",
      cluster_cols = TRUE, cluster_rows = TRUE)
```

It is also possible to create an analogous heatmap by just using the
_ggplot2_ package. However, a relatively long code is libraryd to
generate an identical output.

```{r more_complex_heatmap}
# Add feature names to column as a factor
taxa_clusters$Feature <- rownames(taxa_clusters)
taxa_clusters$Feature <- factor(taxa_clusters$Feature, levels = taxa_clusters$Feature)

# Create annotation plot
row_annotation <- ggplot(taxa_clusters) + 
  geom_tile(aes(x = NA, y = Feature, fill = clusters)) +
  coord_equal(ratio = 1) +
  theme(
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.y=element_blank(),
        axis.title.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        plot.margin=margin(0,0,0,0),
        ) +
      labs(fill = "Clusters", x = "Clusters")

# to view the notation, run
# row_annotation

# Add sample names to one of the columns
sample_data$sample <- factor(rownames(sample_data), levels = rownames(sample_data))

# Create annotation plot
sample_types_annotation <- ggplot(sample_data) +
  scale_y_discrete(position = "right", expand = c(0,0)) +
  geom_tile(aes(y = NA, x = sample, fill = sample_types)) +
  coord_equal(ratio = 1) +
  theme(
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.title.x=element_blank(),
        axis.ticks.x=element_blank(),
        plot.margin=margin(0,0,0,0),
        axis.title.y.right = element_text(angle=0, vjust = 0.5)
        ) +
      labs(fill = "Sample types", y = "Sample types")
# to view the notation, run
# sample_types_annotation

# Create annotation plot
sample_clusters_annotation <- ggplot(sample_data) +
  scale_y_discrete(position = "right", expand = c(0,0)) +
  geom_tile(aes(y = NA, x = sample, fill = clusters)) +
  coord_equal(ratio = 1) +
  theme(
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.title.x=element_blank(),
        axis.ticks.x=element_blank(),
        plot.margin=margin(0,0,0,0),
        axis.title.y.right = element_text(angle=0, vjust = 0.5)
        ) +
      labs(fill = "Clusters", y = "Clusters")
# to view the notation, run
# sample_clusters_annotation

# Order data based on clusters and sample types
mat <- mat[unfactor(taxa_clusters$Feature), unfactor(sample_data$sample)]

# ggplot librarys data in melted format
melted_mat <- melt(mat)
colnames(melted_mat) <- c("Taxa", "Sample", "clr_z")

# Determines the scaling of colorss
maxval <- round(max(abs(melted_mat$clr_z)))
limits <- c(-maxval, maxval)
breaks <- seq(from = min(limits), to = max(limits), by = 0.5)
colours <- c("darkblue", "blue", "white", "red", "darkred")

heatmap <- ggplot(melted_mat) + 
  geom_tile(aes(x = Sample, y = Taxa, fill = clr_z)) +
  theme(
    axis.title.y=element_blank(),
    axis.title.x=element_blank(),
    axis.ticks.y=element_blank(),
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
    
    plot.margin=margin(0,0,0,0), # removes margins
    legend.key.height= unit(1, 'cm')
    ) +
  scale_fill_gradientn(name = "CLR + Z transform", 
                       breaks = breaks, 
                       limits = limits, 
                       colours = colours) + 
  scale_y_discrete(position = "right")

heatmap
```

```{r more_complex_heatmap2, fig.width = 10, fig.height = 8, eval=FALSE}
library(patchwork)

# Create layout
design <- c(
  patchwork::area(3, 1, 4, 1),
  patchwork::area(1, 2, 1, 3),
  patchwork::area(2, 2, 2, 3),
  patchwork::area(3, 2, 4, 3)
)
# to view the design, run
# plot(design)

# Combine plots
plot <- row_annotation + sample_clusters_annotation +
                         sample_types_annotation +
			 heatmap  +
    plot_layout(design = design, guides = "collect",
                # Specify layout, collect legends
                
                # Adjust widths and heights to align plots.
                # When annotation plot is larger, it might not fit into
		# its column/row.
                # Then you need to make column/row larger.
                
                # Relative widths and heights of each column and row:
                # Currently, the width of the first column is 15 % and the height of
                # first two rows are 30 % the size of others
                
                # To get this work most of the times, you can adjust all sizes to be 1, i.e. equal, 
                # but then the gaps between plots are larger.
                widths = c(0.15, 1, 1),
                heights = c(0.3, 0.3, 1, 1))

# plot
```

```{r more_complex_heatmap3, fig.width = 10, fig.height = 8, eval=FALSE}
# Create layout
design <- c(
  patchwork::area(4, 1, 5, 1),
  patchwork::area(4, 2, 5, 2),
  patchwork::area(1, 3, 1, 4),
  patchwork::area(2, 3, 2, 4),
  patchwork::area(3, 3, 3, 4),
  patchwork::area(4, 3, 5, 4)
)

# to view the design, run
# plot(design)

# Combine plots
plot <- taxa_tree + 
  row_annotation +
  sample_tree + 
  sample_clusters_annotation +
  sample_types_annotation +
  heatmap +
    plot_layout(design = design, guides = "collect", # Specify layout, collect legends
                widths = c(0.2, 0.15, 1, 1, 1),
                heights = c(0.1, 0.15, 0.15, 0.25, 1, 1))

plot
```

Heatmaps find several other applications in biclustering and
multi-assay analyses. These are discussed further in chapters
\@ref(clustering) and \@ref(multi-assay-analyses).


### Session Info {-}

```{r sessionInfo, echo = FALSE, results = "asis"}
prettySessionInfo()
```

<!--chapter:end:19_visualization_techniques.Rmd-->

# (PART) Training {-}

# Training {#training}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

The page provides practical information to support training and self-study.


## Checklist {#checklist}

Brief checklist to prepare for training (see below for links).

 * Install the recommended software
 * Watch the short online videos and familiarize with the other available material
 * Join Gitter online chat for support


## Recommended software {#software}

We recommend to install and set up the relevant software packages on
your own computer as this will support later use. The essential
components to install include:

* [R (the latest official release)](https://www.r-project.org/) 

* [RStudio](https://www.rstudio.com/products/rstudio/download/);
  choose "Rstudio Desktop" to download the latest version. Check the
  [Rstudio home page](https://www.rstudio.com/) for more
  information. RStudio is optional.

* Install key R packages (Section \@ref(packages) provides an installation script)


* After a successful installation you can consider trying out examples
  from Section \@ref(exercises) already before training. **You can run
  the workflows by simply copy-pasting examples.** You can then test
  further examples from this tutorial, modifying and applying these
  techniques to your own data. Plain source code for the individual chapters of this book are available via [Github](https://github.com/microbiome/OMA/tree/master/R)

## Study material {#material}

We encourage to familiarize with the material and test examples in advance.

 * [Short online videos](https://www.youtube.com/playlist?list=PLjiXAZO27elAJEptP59BN3whVJ61XIkST) on microbiome data science with R/Bioconductor  

 * [Lecture slides](https://microbiome.github.io/)

 * [Orchestrating Microbiome Analysis with R/Bioconductor (OMA)](https://microbiome.github.io/OMA/) (this book)

 * [Exercises](https://microbiome.github.io/OMA/exercises.html) for self-study

 * [Resources](https://microbiome.github.io/OMA/resources.html) and links to complementary external material



## Support and resources

For online support on installation and other matters, join us at
[Gitter](https://gitter.im/microbiome/miaverse?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge).

You are also welcome to connect through various channels with our
broader [developer and user community](https://microbiome.github.io).


## Code of Conduct {#coc}

We support the [Bioconductor Code of Conduct](https://bioconductor.github.io/bioc_coc_multilingual/). The community values an open approach to science that promotes 

  - sharing of ideas, code, software and expertise
  - a kind and welcoming environment, diversity and inclusivity
  - community contributions and collaboration


<!--chapter:end:80_training.Rmd-->


# Resources {#resources}

## Data containers

### Resources for TreeSummarizedExperiment

 * SingleCellExperiment [@R_SingleCellExperiment]
   + [Online tutorial](https://bioconductor.org/packages/release/bioc/vignettes/SingleCellExperiment/inst/doc/intro.html)
   + [Project page](https://bioconductor.org/packages/release/bioc/html/SingleCellExperiment.html)
 * SummarizedExperiment [@R_SummarizedExperiment]
   + [Online tutorial](https://bioconductor.org/packages/release/bioc/vignettes/SummarizedExperiment/inst/doc/SummarizedExperiment.html)
   + [Project page](https://bioconductor.org/packages/release/bioc/html/SummarizedExperiment.html)
 * TreeSummarizedExperiment [@R_TreeSummarizedExperiment]
   + [Online tutorial](https://bioconductor.org/packages/release/bioc/vignettes/TreeSummarizedExperiment/inst/doc/Introduction_to_treeSummarizedExperiment.html)
   + [Project page](https://www.bioconductor.org/packages/release/bioc/html/TreeSummarizedExperiment.html)
   + Publication: [@Huang2021]
   

### Other relevant containers

* [DataFrame](https://rdrr.io/bioc/S4Vectors/man/DataFrame-class.html) which behaves similarly to `data.frame`, yet efficient and fast when used with large datasets.
* [DNAString](https://rdrr.io/bioc/Biostrings/man/DNAString-class.html) along with `DNAStringSet`,`RNAString` and `RNAStringSet`  efficient storage and handling of long biological sequences are offered within the Biostrings package [@R_Biostrings].
* GenomicRanges ([@GenomicRanges2013]) offers an efficient representation and manipulation of genomic annotations and alignments, see e.g. `GRanges` and `GRangesList` at [An Introduction to the GenomicRangesPackage](https://bioconductor.org/packages/release/bioc/vignettes/GenomicRanges/inst/doc/GenomicRangesIntroduction.html).

[NGS Analysis Basics](http://girke.bioinformatics.ucr.edu/GEN242/tutorials/rsequences/rsequences/) provides a walk-through of the above-mentioned features with detailed examples.


### Alternative containers for microbiome data

The `phyloseq` package and class became the first widely used data
container for microbiome data science in R. Many methods for taxonomic
profiling data are readily available for this class. We provide here a
short description how `phyloseq` and `*Experiment` classes relate to
each other.

`assays` : This slot is similar to `otu_table` in `phyloseq`. In a
               `SummarizedExperiment` object multiple assays, raw
               counts, transformed counts can be stored. See also
               [@Ramos2017]
               for storing data from multiple `experiments` such as
               RNASeq, Proteomics, etc.  `rowData` : This slot is
               similar to `tax_table` in `phyloseq` to store taxonomic
               information.  `colData` : This slot is similar to
               `sample_data` in `phyloseq` to store information
               related to samples.  `rowTree` : This slot is similar
               to `phy_tree` in `phyloseq` to store phylogenetic tree.

In this book, you will come across terms like `FeatureIDs` and
`SampleIDs`.  `FeatureIDs` : These are basically OTU/ASV ids which are
row names in `assays` and `rowData`.  `SampleIDs` : As the name
suggests, these are sample ids which are column names in `assays` and
row names in `colData`.

`FeatureIDs` and `SampleIDs` are used but the technical terms
`rownames` and `colnames` are encouraged to be used, since they relate
to actual objects we work with.

<img
src="https://raw.githubusercontent.com/FelixErnst/TreeSummarizedExperiment
/2293440c6e70ae4d6e978b6fdf2c42fdea7fb36a/vignettes/tse2.png"
width="100%"/>


### Resources for phyloseq

The (Tree)SummarizedExperiment objects can be converted into the alternative phyloseq format, for which further methods are available.

 * [List of R tools for microbiome analysis](https://microsud.github.io/Tools-Microbiome-Analysis/)
 * phyloseq [@McMurdie2013]
 * [microbiome tutorial](http://microbiome.github.io/tutorials/)
 * [microbiomeutilities](https://microsud.github.io/microbiomeutilities/)
 * Bioconductor Workflow for Microbiome Data Analysis: from raw reads to community analyses [@Callahan2016].





## R programming resources

If you are new to R, you could try [swirl](https://swirlstats.com/)
for a kickstart to R programming. Further support resources are
available through the Bioconductor
project [@Huber2015].

 * R programming basics: [Base R](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)
 * Basics of R programming: [Base R](https://raw.githubusercontent.com/rstudio/cheatsheets/master/base-r.pdf)
 * [R cheat sheets](https://www.rstudio.com/resources/cheatsheets/)
 * R visualization with [ggplot2](https://www.rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf) 
 * [R graphics cookbook](http://www.cookbook-r.com/Graphs/)

Rmarkdown

* Rmarkdown tips [@Xie2020]


RStudio

* [RStudio cheat sheet](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf) 



### Bioconductor Classes {#bioc_intro}

**S4 system**

S4 class system has brought several useful features to the
object-oriented programming paradigm within R, and it is constantly
deployed in R/Bioconductor packages [@Huber2015].
    
|   Online Document:

* HervÃ© PagÃ¨s, [A quick overview of the S4 class system](https://bioconductor.org/packages/release/bioc/vignettes/S4Vectors/inst/doc/S4QuickOverview.pdf).
* Laurent Gatto, [A practical tutorial on S4 programming](https://bioconductor.org/help/course-materials/2013/CSAMA2013/friday/afternoon/S4-tutorial.pdf)
* How S4 Methods Work [@Chambers2006]

|   Books:

* John M. Chambers. Software for Data Analysis: Programming with R. Springer, New York, 2008. ISBN-13 978-0387759357 [@Chambers2008]
* I Robert Gentleman. R Programming for Bioinformatics. Chapman & Hall/CRC, New York, 2008. ISBN-13 978-1420063677 [@gentleman2008r]



## Reproducible reporting with Quarto {#quarto}

Reproducible reporting is the starting point for robust interactive
data science. Perform the following tasks:

 * If you are entirely new to literate programming and reproducible
   reporting, take [this](https://www.markdowntutorial.com/) 10 minute
   tutorial to get introduced to the most important functions within
   Markdown. 

 * Create a Quarto template in RStudio, and render it into a
   document (markdown, PDF, docx or other format). In case you are new
   to Quarto, see [this page](https://quarto.org/).

 * Examples are tips for closely related Rmarkdown are available in
   the online tutorial to reproducible reporting by [Dr. C Titus
   Brown](https://rpubs.com/marschmi/RMarkdown); see also [Rmarkdown
   cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)


**Figure sources:** 

**Original article**
-   Huang R _et al_. (2021) TreeSummarizedExperiment: a S4 class 
for data with hierarchical structure. F1000Research 9:1246. [@Huang2021]

**Reference Sequence slot extension**
- Lahti L _et al_. (2020) [Upgrading the R/Bioconductor ecosystem for microbiome 
research](https://doi.org/10.7490/
f1000research.1118447.1) F1000Research 9:1464 (slides).

<!--chapter:end:95_resources.Rmd-->

# Exercises {#exercises}

Here you can find assignments on different topics. 

**Tips for exercises:**

   - Add comments that explain what each line or lines of code do. This helps you and others to understand your code and find bugs. Furthermore, it is easier for you to reuse the code, and it promotes transparency.
   - Interpret results by comparing them to literature. List main findings, so that results can easily be understood by others without advanced data analytics knowledge.
   - Avoid hard-coding. Use variables which get values in the beginning of the pipeline. That way it is easier for you to modify parameters and reuse the code.

## Workflows

### Reproducible reporting with Quarto

1. Create a new Quarto file

* Rstudio has ready-made templates for this
* [Creating Quarto Documents](https://quarto.org/docs/tools/rstudio.html#creating-documents)

2. Add a code chunk and name it.
3. Render (or knit) the file into pdf or html format (Hint: [Quarto Rstudio rendering](https://quarto.org/docs/tools/rstudio.html#render-and-preview))
4. Import e.g., iris dataset, and add a dotplot with a title (Hint: [geom_dotplot](https://ggplot2.tidyverse.org/reference/geom_dotplot.html))
5. Create another code chunk and plot. 
6. Adjust figure size and hide the code chunk from output report.

  * [Sizing](https://quarto.org/docs/authoring/diagrams.html#sizing)
  * [chunk-options](https://quarto.org/docs/computations/r.html#chunk-options)

7. Add some text.
8. Add R commands within the text (Hint: [code-chunks](https://quarto.org/docs/visual-editor/technical.html#code-chunks))
9. Update HTML file from the qmd file (Hint: [Quarto Rstudio rendering](https://quarto.org/docs/tools/rstudio.html#render-and-preview))


For tips on Quarto, see [Quarto tutorial](https://quarto.org/docs/authoring/markdown-basics.html)


## Data containers: TreeSE


### Constructing a data object

Import data from CSV files to TreeSE (see shared data folder for example data sets).

1. Import the data files in R
2. Construct a TreeSE data object (see [Ch. 2](https://microbiome.github.io/OMA/containers.html#loading-experimental-microbiome-data))
3. Check that importing is done correctly. E.g., choose random samples and features,
and check that their values equal between raw files and TreeSE.

Useful functions: DataFrame, TreeSummarizedExperiment, matrix, rownames, colnames, SimpleList

### Importing data


You can also check the [function reference in the mia package](https://microbiome.github.io/mia/reference/index.html)

1. Import data from another format (functions: loadFromMetaphlan | loadFromMothur | loadFromQIIME2 | makeTreeSummarizedExperimentFromBiom | makeTreeSummarizedExperimentFromDADA2 ...)
2. Try out conversions between TreeSE and phyloseq data containers (makeTreeSummarizedExperimentFromPhyloseq; makephyloseqFromTreeSummarizedExperiment)

### Basic summaries

1. Load experimental dataset from mia (e.g. `peerj13075` with the `data()` command; see OMA [section 2.4 Demonstration Data](https://microbiome.github.io/OMA/containers.html#example-data); also see [loading experimental data](https://microbiome.github.io/OMA/containers.html#assay-data)).
2. Check a summary about the TreeSE object loaded (Hint: `summary()`)
3. What are the dimensions? (How many samples there are, and how many taxa in each taxonomic rank?) (Hint: material in [Section 2](https://microbiome.github.io/OMA/containers.html#data-science-framework) may help)
4. List sample and features names for the data (rownames, colnames..)

### Taxonomic abundance table (assay)

1. (Load example data)
2. Fetch the list of available assays (Hints: [assayNames](https://microbiome.github.io/OMA/containers.html#assay-data))
3. Fetch the `counts` assay, and show part of it. (Hint: [assay-data](https://microbiome.github.io/OMA/containers.html#assay-data))


### Sample side information 

1. (Load example data)
2. Fetch and show data about samples (Hint: [colData](https://microbiome.github.io/OMA/containers.html#coldata))
3. Get abundance data for all taxa for a specific sample (sample names: function `colnames(tse)`)

  * [example](https://microbiome.github.io/OMA/taxonomic-information.html#abundances-of-all-taxa-in-specific-sample)


### Feature side information

1. (Load example data)
2. Fetch and show data on the (taxonomic) features of the analyzed samples (Hint: [rowData](https://microbiome.github.io/OMA/containers.html#rowdata))
3. Get abundance data for all samples given a specific features (Hint: [example](https://microbiome.github.io/OMA/taxonomic-information.html#abundances-of-specific-taxa-in-all-samples))

Optional:

4. Create taxonomy tree based on the taxonomy mappings display its information:

 * [generate a taxonomic tree on the fly](https://microbiome.github.io/OMA/taxonomic-information.html#generate-a-taxonomic-tree-on-the-fly)
 * [rowtree](https://microbiome.github.io/OMA/containers.html#rowtree)


### Other elements

Try to extract some of the [other TreeSE elements](https://f1000research.com/articles/9-1246/v2). These are not always included:

* Experiment metadata
* Sample tree (colTree)
* Phylogenetic tree (rowTree)
* Feature sequences information (DNA sequence slot)



## Data manipulation


### Subsetting

1. Subset the TreeSE object to specific samples
2. Subset the TreeSE object to specific features
3. Subset the TreeSE object to specific samples and features



### Library sizes

1. Calculate library sizes
2. Subsample / rarify the counts (see: subsampleCounts)

Useful functions: nrow, ncol, dim, summary, table, quantile, unique, addPerCellQC, agglomerateByRank

### Prevalent and core taxonomic features

1. Estimate prevalence for your chosen feature (row, taxonomic group)
2. Identify all prevalent features and subset the data accordingly 
3. Report the thresholds and the dimensionality of the data before and after subsetting
4. Visualize prevalence

Useful functions: getPrevalence, getPrevalentTaxa, subsetByPrevalentTaxa


### Data exploration

1. Summarize sample metadata variables. (How many age groups, how they are distributed? 0%, 25%, 50%, 75%, and 100% quantiles of library size?)
2. Create two histograms. Another shows the distribution of absolute counts, another shows how CLR transformed values are distributed.
3. Visualize how relative abundances are distributed between taxa in samples.

Useful functions: nrow, ncol, dim, summary, table, quantile, unique, transformCounts, ggplot, wilcox.test, agglomerateByRank, plotAbundance

### Other functions

1. Merge data objects (merge, mergeSEs)
2. Melt the data for visualization purposes (meltAssay)


### Transformations

1. Transform abundance data with relative abundance and add a relative abundance assay (see [data-transformation](https://microbiome.github.io/OMA/taxonomic-information.html#data-transformation))
2. Transform abundance data with clr transformation and add a new assay
3. List the available assays by name
4. Pick one of the assays and show a subset of it
5. Subset the entire TreeSE data object, and check how this affects individual (transformed) assays

Optional:

6. If the data has phylogenetic tree, perform the phILR transformation



## Abundance tables

### Taxonomic levels

1. List the available taxonomic ranks in the data
2. Merge the data to Phylum level
3. Report dimensionality before and after aggomeration

Optional:

4. Perform CLR transformation on the data; does this affect agglomeration?
5. List full taxonomic information for some given taxa (Hint: [mapTaxonomy](https://microbiome.github.io/mia/reference/taxonomy-methods.html))

Useful functions: [taxonomyRanks](https://microbiome.github.io/mia/reference/taxonomy-methods.html), agglomerateByRank, mergeRows


### Alternative experiments (altExp)

1. Create taxonomic abundance tables for all different levels (splitByRanks)
2. Check the available alternative experiment (altExp) names before and after splitByRanks
3. Pick specific "experiment" (taxonomic rank) from specific altExp; and a specific assay

Optional:

4. Split the data based on other features (splitOn)



## Community diversity (alpha diversity)

### Alpha diversity basics

1. Calculate alpha diversity indices
2. Test if data agglomeration to higher taxonomic ranks affects the indices
3. Look for differences in alpha diversity between groups or correlation with a continuous variable

Useful functions: estimateDiversity, colSums, agglomerateByRank, kruskal.test, cor


### Alpha diversity extra

1. Estimate Shannon diversity for the data
2. Try also another diversity index and compare the results with a scatterplot
3. Compare Shannon diversity between groups (boxplot)
4. Is diversity significantly different between vegan and mixed diet?
5. Calculate and visualize library size, compare with diversity

Useful functions: estimateDiversity, colSums, geom_point, geom_boxplot




## Community composition (beta diversity)

### Beta diversity basics

1. Visualize community variation with different methods (PCA, MDS, NMDS, etc.) with plotReduceDim and with different dissimilarities and transformations,plot also other than the first two axes.
2. Use PERMANOVA to test differences in beta diversity. You can also try including continuous and/or categorical covariates
3. If there are statistical differences in PERMANOVA, test PERMDISP2 (betadisper function)
4. Do clustering
5. Try RDA to test the variance explained by external variables


### Beta diversity extra

1. Install the latest development version of mia from GitHub.
2. Load experimental dataset from mia.
3. Create a PCoA with Aitchison dissimilarities. How much coordinate 1 explains the differences? How about coordinate 2?
4. Create dbRDA with Bray-Curtis dissimilarities on relative abundances. Use PERMANOVA. Can differences between samples be explained with variables of sample meta data? 
5. Analyze diets' association on beta diversity. Calculate dbRDA and then PERMANOVA. Visualize coefficients. Which taxa's abundances differ the most between samples? 
6. Interpret your results. Is there association between community composition and location? What are those taxa that differ the most; find information from literature.

Useful functions: runMDS, runRDA, anova.cca, transformCounts, agglomerateByRank, ggplot, plotReducedDim, vegan::adonis2




## Differential abundance

### Univariate analyses

1. Get the abundances for an individual feature (taxonomic group / row)
2. Visualize the abundances per group with boxplot / jitterplot
3. Is the difference significant (Wilcoxon test)?
4. Is the difference significant (linear model with covariates)? 
5. How do transformations affect the outcome (log10, clr..)?
6. Get p-values for all features (taxa), for instance with a for loop
7. Do multiple testing correction
9. Compare the results from different tests with a scatterplot

Useful functions: [], ggplot2::geom_boxplot, ggplot2::geom_jitter, wilcox.test, lm.test, transformCounts, p.adjust


### Differential abundance analysis

1. install the latest development version of mia from GitHub.
2. Load experimental dataset from mia.
3. Compare abundances of each taxa between groups. First, use Wilcoxon or Kruskall-Wallis test. Then use some other method dedicated to microbiome data. 
4. Summarize findings by plotting a taxa vs samples heatmap. Add column annotation that tells the group of each sample, and row annotation that tells whether the difference of certain taxa was statistically significant.
5. Choose statistically significant taxa and visualize their abundances with boxplot & jitterplot.

Useful functions: wilcox.test, kruskal.test, ggplot, pheatmap, ComplexHeatMap::Heatmap, ancombc, aldex2, maaslin2, agglomerateByRank, transformCounts, subsetByPrevalentTaxa


## Visualization

### Multivariate ordination

1. Load experimental dataset from mia.
2. Create PCoA with Bray-Curtis dissimilarities
3. Create PCA with Aitchison dissimilarities
4. Visualize and compare both
5. Test other transformations, dissimilarities, and ordination methods

Useful functions: runMDS, runNMDS, transformCounts, ggplot, plotReducedDim


### Heatmap visualization

1. Load experimental dataset from mia.
2. Visualize abundances with heatmap
3. Visualize abundances with heatmap after CLR + Z transformation 

See the OMA book for examples.



## Multiomics

### MultiAssayExperiment (MAE) data container

1. Create TreeSE data containers from individual CSV files.
2. Combine TreeSE into MAE.
3. Check that each individual experiment of MAE equals corresponding TreeSE.
4. Take a subset of MAE (e.g., 10 first samples), and observe the subsetted MAE.

Useful functions: DataFrame, TreeSummarizedExperiment, matrix, rownames, colnames, MultiAssayExperiment, ExperimentList, SimpleList


### Multi-omic data exploration

1. Load experimental dataset from microbiomeDataSets (e.g., HintikkaXOData).
2. Analyze correlations between experiments. (Taxa vs lipids, Taxa vs biomarkers, Lipids vs biomarkers)
3. Agglomerate taxa data.
4. Apply CLR to taxa data, apply log10 to lipids and biomarkers.
5. Perform cross-correlation analyses and visualize results with heatmaps. (Use Spearman coefficients)
6. Is there significant correlations? Interpret your results.

Useful functions: pheatmap, ComplexHeatMap::Heatmap, ggplot, transformCounts, testExperimentCrossAssociation







<!--chapter:end:98_exercises.Rmd-->

# (PART) Appendix {-}

# Extra material {#extras}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```


## PERMANOVA comparison

Here we present two possible uses of the `adonis2` function which performs PERMANOVA. The
optional argument `by` has an effect on the statistical outcome, so its two options are
compared here.

```{r permanova_import, warning = FALSE, message = FALSE}
# import necessary packages
library(gtools)
library(purrr)
library(vegan)
library(gtools)
library(purrr)
```

Let us load the _enterotype_ TSE object and run PERMANOVA for
different orders of three variables with two different approaches:
`by = "margin"` or `by = "terms"`.


```{r permanova_prep, message = FALSE, warning = FALSE}
# load and prepare data
library(mia)
data("enterotype", package="mia")
enterotype <- transformCounts(enterotype, method = "relabundance")
# drop samples missing meta data
enterotype <- enterotype[ , !rowSums(is.na(colData(enterotype)[, c("Nationality", "Gender", "ClinicalStatus")]) > 0)]
# define variables and list all possible combinations
vars <- c("Nationality", "Gender", "ClinicalStatus")
var_perm <- permutations(n = 3, r = 3, vars)
formulas <- apply(var_perm, 1, function(row) purrr::reduce(row, function(x, y) paste(x, "+", y)))
# create empty data.frames for further storing p-values
terms_df <- data.frame("Formula" = formulas,
                       "ClinicalStatus" = rep(0, 6),
                       "Gender" = rep(0, 6),
                       "Nationality" = rep(0, 6))
margin_df <- data.frame("Formula" = formulas,
                        "ClinicalStatus" = rep(0, 6),
                        "Gender" = rep(0, 6),
                        "Nationality" = rep(0, 6))
```



```{r permanova_loop, message = FALSE, warning = FALSE}
for (row_idx in 1:nrow(var_perm)) {
  
  # generate temporary formula (i.e. "assay ~ ClinicalStatus + Nationality + Gender")
  tmp_formula <- purrr::reduce(var_perm[row_idx, ], function(x, y) paste(x, "+", y))
  tmp_formula <- as.formula(paste0('t(assay(enterotype, "relabundance")) ~ ',
                            tmp_formula))

  # multiple variables, default: by = "terms"
  set.seed(75)
  with_terms <- adonis2(tmp_formula, 
                by = "terms",
                data = colData(enterotype),
                permutations = 99)
  
  # multiple variables, by = "margin"
  set.seed(75)
  with_margin <- adonis2(tmp_formula, 
                 by = "margin",
                 data = colData(enterotype),
                 permutations = 99)

  # extract p-values
  terms_p <- with_terms[["Pr(>F)"]]
  terms_p <- terms_p[!is.na(terms_p)]
  margin_p <- with_margin[["Pr(>F)"]]
  margin_p <- margin_p[!is.na(margin_p)]
  
  # store p-values into data.frames
  for (col_idx in 1:ncol(var_perm)) {
    
    terms_df[var_perm[row_idx, col_idx]][row_idx, ] <- terms_p[col_idx]
    margin_df[var_perm[row_idx, col_idx]][row_idx, ] <- margin_p[col_idx]
    
  }
  
}
```




The following table displays the p-values for the three variables
ClinicalStatus, Gender and Nationality obtained by PERMANOVA with
`adonis2`. Note that the p-values remain identical when `by =
"margin"`, but change with the order of the variables in the
formula when `by = "terms"` (default).


```{r permanova_table, message = FALSE, warning = FALSE}

df <- terms_df %>%
  dplyr::inner_join(margin_df, by = "Formula", suffix = c(" (terms)", " (margin)"))

knitr::kable(df)
```


## Bayesian Multinomial Logistic-Normal Models

Analysis using such model could be performed with the function
`pibble` from the `fido` package, wihch is in form of a Multinomial
Logistic-Normal Linear Regression model; see
[vignette](https://jsilve24.github.io/fido/articles/introduction-to-fido.html)
of package.


The following presents such an exemplary analysis based on the 
data of @Sprockett2020 available
through `microbiomeDataSets` package.


```{r, message=FALSE, warning=FALSE}
library(fido)
```

Loading the libraries and importing data:

```{r, message=FALSE, warning=FALSE}
library(fido)
```

```{r, message=FALSE, warning=FALSE, eval=FALSE}
library(microbiomeDataSets)
tse <- SprockettTHData()
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# saveRDS(tse, file="data/SprockettTHData.Rds")
# Hidden reading of the saved data
tse <- readRDS("data/SprockettTHData.Rds")
```


We pick three covariates ("Sex","Age_Years","Delivery_Mode") during this
analysis as an example, and beforehand we check for missing data:


```{r, message=FALSE, warning=FALSE}
library(mia)
cov_names <- c("Sex","Age_Years","Delivery_Mode")
na_counts <- apply(is.na(colData(tse)[,cov_names]), 2, sum)
na_summary<-as.data.frame(na_counts,row.names=cov_names)
```

We drop missing values of the covariates:

```{r, message=FALSE, warning=FALSE}
tse <- tse[ , !is.na(colData(tse)$Delivery_Mode) ]
tse <- tse[ , !is.na(colData(tse)$Age_Years) ]
```

We agglomerate microbiome data to Phylum:

```{r, message=FALSE, warning=FALSE}
tse_phylum <- agglomerateByRank(tse, "Phylum")
```

We extract the counts assay and covariate data to build the model
matrix:

```{r, message=FALSE, warning=FALSE}
Y <- assays(tse_phylum)$counts
# design matrix
# taking 3 covariates
sample_data<-as.data.frame(colData(tse_phylum)[,cov_names])
X <- t(model.matrix(~Sex+Age_Years+Delivery_Mode,data=sample_data))
```

Building the parameters for the `pibble` call to build the model; see more at [vignette](https://jsilve24.github.io/fido/articles/introduction-to-fido.html):

```{r, message=FALSE, warning=FALSE}
n_taxa<-nrow(Y)
upsilon <- n_taxa+3
Omega <- diag(n_taxa)
G <- cbind(diag(n_taxa-1), -1)
Xi <- (upsilon-n_taxa)*G%*%Omega%*%t(G)
Theta <- matrix(0, n_taxa-1, nrow(X))
Gamma <- diag(nrow(X))
```

Automatically initializing the priors and visualizing their distributions:

```{r, message=FALSE, warning=FALSE}
priors <- pibble(NULL, X, upsilon, Theta, Gamma, Xi)
names_covariates(priors) <- rownames(X)
plot(priors, pars="Lambda") + ggplot2::xlim(c(-5, 5))
```

Estimating the posterior by including our response data `Y`.
Note: Some computational failures could occur (see [discussion](https://github-wiki-see.page/m/jsilve24/fido/wiki/Frequently-Asked-Questions))
the arguments `multDirichletBoot` `calcGradHess` could be passed in such case.

```{r, message=FALSE, warning=FALSE}
priors$Y <- Y 
posterior <- refit(priors, optim_method="adam", multDirichletBoot=0.5) #calcGradHess=FALSE
```

Printing a summary about the posterior:

```{r, message=FALSE, warning=FALSE}
ppc_summary(posterior)
```
Plotting the summary of the posterior distributions of the regression parameters:

```{r, message=FALSE, warning=FALSE}
names_categories(posterior) <- rownames(Y)
plot(posterior,par="Lambda",focus.cov=rownames(X)[2:4])
```

Taking a closer look at "Sex" and "Delivery_Mode":

```{r, message=FALSE, warning=FALSE}
plot(posterior, par="Lambda", focus.cov = rownames(X)[c(2,4)])
```


## Interactive 3D Plots

```{r, message=FALSE, warning=FALSE}
# Installing libraryd packages
library(rgl)
library(plotly)
```

```{r setup2, warning=FALSE, message=FALSE}
library(knitr)
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```


In this section we make a 3D version of the earlier  Visualizing the most dominant genus on PCoA (see \@ref(quality-control)), with the help of the plotly [@Sievert2020].

```{r, message=FALSE, warning=FALSE}
# Installing the package
library(curatedMetagenomicData)
# Importing necessary libraries
library(curatedMetagenomicData)
library(dplyr)
library(DT)
library(mia)
library(scater)

# Querying the data
tse <- sampleMetadata %>%
    filter(age >= 18) %>% # taking only data of age 18 or above
    filter(!is.na(alcohol)) %>% # excluding missing values
    returnSamples("relative_abundance")

tse_Genus <- agglomerateByRank(tse, rank="genus")
tse_Genus <- addPerSampleDominantTaxa(tse_Genus,assay.type="relative_abundance", name = "dominant_taxa")

# Performing PCoA with Bray-Curtis dissimilarity.
tse_Genus <- runMDS(tse_Genus, FUN = vegan::vegdist, ncomponents = 3,
              name = "PCoA_BC", assay.type = "relative_abundance")

# Getting the 6 top taxa
top_taxa <- getTopTaxa(tse_Genus,top = 6, assay.type = "relative_abundance")

# Naming all the rest of non top-taxa as "Other"
most_abundant <- lapply(colData(tse_Genus)$dominant_taxa,
                   function(x){if (x %in% top_taxa) {x} else {"Other"}})

# Storing the previous results as a new column within colData
colData(tse_Genus)$most_abundant <- as.character(most_abundant)

# Calculating percentage of the most abundant
most_abundant_freq <- table(as.character(most_abundant))
most_abundant_percent <- round(most_abundant_freq/sum(most_abundant_freq)*100, 1)

# Retrieving the explained variance
e <- attr(reducedDim(tse_Genus, "PCoA_BC"), "eig");
var_explained <- e/sum(e[e>0])*100
```


<!--chapter:end:97_extra_materials.Rmd-->


# Acknowledgments {#acknowledgments}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

This work would not have been possible without the countless
contributions and interactions with other researchers, developers, and
users. We express our gratitude to the entire Bioconductor community
for developing this high-quality open research software repository for
life science analytics, continuously pushing the limits in emerging
fields [@Gentleman2004, @Huber2015]. The developers and contributors
of this online tutorial are listed in Chapter \@ref(contributors).

The base ecosystem of data containers, packages, and tutorials was set
up as a collaborative effort by Tuomas Borman, Henrik Eckermann,
Chouaib Benchraka, Chandler Ross, Shigdel Rajesh, YaÄŸmur ÅžimÅŸek,
Giulio Benedetti, Sudarshan Shetty, Felix Ernst, and [Leo
Lahti](http://www.iki.fi/Leo.Lahti).

The work has been supported by the COST Action network on Statistical
and Machine Learning Techniques for Human Microbiome Studies
([ML4microbiome](https://www.ml4microbiome.eu/)) [@MorenoIndias2021].

The framework is based on the _TreeSummarizedExperiment_ data
container created by Ruizhu Huang and others
[@R_TreeSummarizedExperiment], and on the MultiAssayExperiment by
Marcel Ramos et al. [@Ramos2017]. The idea of using these containers
as a basis for microbiome data science was initially advanced by the
groundwork of Domenick Braccia, HÃ©ctor Corrada Bravo and others, and
subsequently brought together with other microbiome data science
developers [@Shetty2019].

Ample demonstration data resources have been made available as the
[curatedMetagenomicData](https://waldronlab.io/curatedMetagenomicData/)
project by Edoardo Pasolli, Lucas Schiffer, Levi Waldron and others
[@Pasolli2017] adding important support.
A number of other contributors have advanced the ecosystem
further, and will be acknowledged in the individual
packages, [pull
requests](https://github.com/microbiome/OMA/graphs/contributors),
[issues](https://github.com/microbiome/OMA/issues), and other work.

The work has drawn inspiration from many sources, most notably from
the work on _phyloseq_ by Paul McMurdie and Susan Holmes
[@McMurdie2013] who pioneered the work on rigorous and reproducible
microbiome data science ecosystems in R/Bioconductor. The phyloseq
framework continues to provide a vast array of complementary packages
and methods for microbiome studies, and we aim to support full
interoperability.

The open source books by Susan Holmes and Wolfgang Huber, Modern
Statistics for Modern Biology [@Holmes2019] and by Garret Grolemund
and Hadley Wickham, the R for Data Science [@Grolemund2017], and
Richard McElreath's Statistical Rethinking and the associated online
resources by Solomon Kurz [@McElreath2020] are key references that
advanced reproducible data science training and dissemination. The
Orchestrating Single-Cell Analysis with Bioconductor, or _OSCA_ book
by Robert Amezquita, Aaron Lun, Stephanie Hicks, and Raphael Gottardo
[@Amezquita2020natmeth] has implemented closely related work on the
_SummarizedExperiment_ data container and its derivatives in the field
of single cell sequencing studies. Many approaches used in this book
have been derived from the [OSCA
framework](https://bioconductor.org/books/release/OSCA/), with various
adjustments and extensions dedicated to microbiome data science.


<!--chapter:end:90_acknowledgments.Rmd-->

`r if (knitr::is_html_output()) '
# Bibliography {-}
'`


<!--chapter:end:99_bibliography.Rmd-->

