# Exercises {#exercises}

Here you can find assignments on different topics. 

**Tips for exercises:**

   - Add comments that explain what each line or lines of code do. This helps you and others understand your code and find bugs. Furthermore, it is easier for you to reuse the code, and it promotes transparency.
   - Interpret results by comparing them to literature. List main findings, so that results can easily be understood by others without advanced knowledge on data analytics.
   - Avoid hard-coding. Use variables which get values in the beginning of the pipeline. That way it is easier for you to modify parameters and reuse the code.

## Workflows

### Reproducible reporting with Quarto

The following batch of exercises walks you through typical use cases of Quarto
in RStudio. Before heading to the exercises, it is recommended to read the
[Quarto guidelines for RStudio](https://quarto.org/docs/tools/rstudio.html)

#### New document

This exercise gets you started with creating a Quarto document and adding
text to it with typing conventions borrowed from the
[markdown syntax](https://quarto.org/docs/authoring/markdown-basics.html).
Feel free to render the document with the **Render** button after each step to
see the changes in the final report.

1. Open RStudio and create a new Quarto file named `My first Quarto`.
2. Add the subtitle `My first section` and write some text of your choice
   underneath. You can choose the level of headings by the number of preceding
   hashes (`#`).
3. Add a subsection named `List of items` and list three items underneath, both
   ordered and unordered. You can initialize items with numbers (`1.`, `2.`,
   `3.`, ...) or stars (`*`) for the ordered and unordered case, respectively.
4. Add another subsection named `Link to web` and add a clickable link to the
   [OMA book](https://microbiome.github.io/OMA/), using the pattern `[text](url)`.
5. Render the document and check its appearance

Nice start! You are now able to create a Quarto document, understand its syntax
and can render it into a reproducible report. If you got stuck, you can look up
the docs on [creating](https://quarto.org/docs/tools/rstudio.html#creating-documents)
and [rendering](https://quarto.org/docs/tools/rstudio.html#render-and-preview)
Quarto documents.

#### Code chunks

While customizable text is nothing new by itself, the advantage of Quarto (and
previously Rmakdown) is to combine text with code in R or other programming
languages, so that both the analytical pipeline and verbal description can be
put in one place. In this exercise, we learn how to write and run code in Quarto.

1. Open RStudio and create a new Quarto file.
2. Initialize a code chunk by pressing `alt` + `cmd` + `i` and define the variables
   `A <- "my name"` and `B <- 0` in it.
3. Write the text `Below is my first code chunk` just above the code chunk.
4. Initialize one more code chunk and add 100 to the variable `B` in it. 
5. Write the text `Below I change variable B` just above the second chunk.
6. **Extra**: Write the following line of text: `my name is A and I am B years old`,
   where A and B are variables defined in the code chunks upstream and change if
   those variables are modified. Be aware that inline code can be added as
   \`r my_inline_command\`.
   
Good job. You are now able to combine text and code in a Quarto document. If you
got stuck, you can refer to the Quarto docs on
[using code chunks](https://quarto.org/docs/visual-editor/technical.html#code-chunks).

#### Knitr options

Code chunks can be greatly customized in terms of visibility and execution,
output size and location and much more. This is possible with the knitr chunk
options, which usually appear at the beginning of the target chunk with the
syntax `#| option-name: value`, also described
[here](https://quarto.org/docs/computations/r.html#chunk-options).
In this exercise, we explore part of the knitr potential.

1. Open RStudio and create a new Quarto file.
2. Initialize three code chunks and label them as `setup`, `fig-box` and
   `tbl-coldata`, respectively. Remember that the name of a chunk can be
   specified with the `label` option.
3. Write the following code in the corresponding chunk and render the document.

```{r setup-example, eval = FALSE}
# setup
library(mia)
data("GlobalPatterns", package = "mia")
tse <- GlobalPatterns

# this line sets some options for all the chunks (global chunk options)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r fig-example, eval = FALSE}
# fig-box
boxplot(colSums(assay(tse)) ~ tse$SampleType)
```

```{r tbl-example, eval = FALSE}
# tbl-coldata
knitr::kable(head(colData(tse)))
```

4. Set `include: false` in the `setup` chunk, `fig-width: 10` in the `fig-box`
   chunk and `echo: false` in the `tbl-coldata` chunk. Render the document again
   and find the differences from before.
5. Add the options `fig-cap` and `tab-cap` to the `fig-box` and `tbl-coldata`
   chunks, respectively. They require some text input, which makes for the
   caption of the figures or tables.
6. **Extra**: Create a cross-reference to `fig-box` and `tbl-coldata` in the
   text above the respective code chunk. You can do that with the syntax
   `@chunk-name`.
7. **Extra**: Define a custom folder for storing figures with `fig-path`. Insert
   it in `knitr::opts_chunk$set`, so that it applies globally to all the figures
   generated in the document.
   
Congratulations! You are now familiar with the great potential and flexibility
of knitr chunk options. An exhaustive list of available options can be found
in the [knitr documentation](https://yihui.org/knitr/options/).

#### YAML instructions

The box at the beginning of every Quarto document contains yaml options that let
you define the metadata of the document. They will affect the appearance of the
document when it is rendered. By default, the box includes yaml options for the
title, format and editor to be used, but much more information on layout, code
execution and figures can be specified. A comprehensive list of yaml options is
available [here](https://quarto.org/docs/reference/formats/html.html). In this
exercise, we will get a tiny taste of such functionality.

1. Open RStudio and create a new Quarto file.
2. In the yaml box at the beginning of the document, change the title from
   `Untitled` to `My first Quarto`.
4. In the same box, add the two lines `author` and `date` followed by your name
   and today's date, respectively.
5. Render the document and check its appearance.
6. **Extra**: Set `toc: true` to produce a table of contents. This line should
   follow `format` and `html` at the second level of indentation.
   
Well done! Now you are able to specify yaml options and understand how they
affect your Quarto document. If you got stuck, you can check
[this section](https://quarto.org/docs/tools/rstudio.html#yaml-intelligence) of
the Quarto documentation.

#### Quarto parameters

An advanced feature of Quarto consists of execution parameters, which are
externally pre-defined variables that are also accessible in the Quarto document.
They can be specified in the yaml box as `params`. Here we learn how to use them.

1. Open RStudio and create a new Quarto file.
2. In the yaml box at the beginning of the document, add a line named `params`
   followed by an indented line with `gamma: 10`
3. Initialize a code chunk and type `str(params$gamma)` in it.
4. Render the document and check what happened.
5. Define one more parameter `beta: 3` and multiply `gamma` by `beta` in a code
   chunk below.
6. Render the document again and check what happened.

Well done! You can now use an advanced feature of Quarto such as parameters.
If you got stuck, [here](https://quarto.org/docs/computations/parameters.html#knitr)
you can find more information about parameter definition and usage.


## Data containers: TreeSE


### Constructing a data object {#construct-TreeSE}

Here we cover how to construct a TreeSE from CSV files, using the components
of OKeefeDSData from the microbiomeDataSets package as an example dataset.

1. Download the files with the prefix _DS_ from
   [this directory](https://github.com/JuliaTurkuDataScience/MicrobiomeAnalysis.jl/tree/main/src/assets)
2. Read in the csv files with `read.csv` and store them into the variables
   `assays`, `rowdata` and `coldata`, respectively
3. Create a TreeSE from the individual components with `TreeSummarizedExperiment`.
   Note that the function requires three arguments: assays, rowData and colData,
   to which you can give the appropriate item
4. Check that importing is done correctly. E.g., choose random samples and features,
and check that their values equal between raw files and TreeSE.

Usefuls functions: DataFrame, TreeSummarizedExperiment, matrix, rownames, colnames, SimpleList

### Importing data

You can also check the [function reference in the mia package](https://microbiome.github.io/mia/reference/index.html)

1. Import data from another format (functions: loadFromMetaphlan | loadFromMothur | loadFromQIIME2 | makeTreeSummarizedExperimentFromBiom | makeTreeSummarizedExperimentFromDADA2 ...)
2. Try out conversions between TreeSE and phyloseq data containers (makeTreeSummarizedExperimentFromPhyloseq; makephyloseqFromTreeSummarizedExperiment)

### Basic summaries

1. Load experimental dataset from mia (e.g. `peerj13075` with the `data()` command; see OMA [section 2.4 Demonstration Data](https://microbiome.github.io/OMA/containers.html#example-data); also see [loading experimental data](https://microbiome.github.io/OMA/containers.html#assay-data)).
2. Check a summary about the TreeSE object loaded (Hint: `summary()`)
3. What are the dimensions? (How many samples there are, and how many taxa in each taxonomic rank?) (Hint: material in [Section 2](https://microbiome.github.io/OMA/containers.html#data-science-framework) may help)
4. List sample and features names for the data (rownames, colnames..)

### Taxonomic abundance table (assay)

1. (Load example data)
2. Fetch the list of available assays (Hints: [assayNames](https://microbiome.github.io/OMA/containers.html#assay-data))
3. Fetch the `counts` assay, and show part of it. (Hint: [assay-data](https://microbiome.github.io/OMA/containers.html#assay-data))


### Sample side information 

1. (Load example data)
2. Fetch and show data about samples (Hint: [colData](https://microbiome.github.io/OMA/containers.html#coldata))
3. Get abundance data for all taxa for a specific sample (sample names: function `colnames(tse)`)

  * [example](https://microbiome.github.io/OMA/taxonomic-information.html#abundances-of-all-taxa-in-specific-sample)


### Feature side information

1. (Load example data)
2. Fetch and show data on the (taxonomic) features of the analyzed samples (Hint: [rowData](https://microbiome.github.io/OMA/containers.html#rowdata))
3. Get abundance data for all samples given a specific features (Hint: [example](https://microbiome.github.io/OMA/taxonomic-information.html#abundances-of-specific-taxa-in-all-samples))

Optional:

4. Create taxonomy tree based on the taxonomy mappings display its information:

 * [generate a taxonomic tree on the fly](https://microbiome.github.io/OMA/taxonomic-information.html#generate-a-taxonomic-tree-on-the-fly)
 * [rowtree](https://microbiome.github.io/OMA/containers.html#rowtree)


### Other elements

Try to extract some of the [other TreeSE elements](https://f1000research.com/articles/9-1246/v2). These are not always included:

* Experiment metadata
* Sample tree (colTree)
* Phylogenetic tree (rowTree)
* Feature sequences information (DNA sequence slot)



## Data manipulation


### Subsetting

1. Subset the TreeSE object to specific samples
2. Subset the TreeSE object to specific features
3. Subset the TreeSE object to specific samples and features



### Library sizes

1. Calculate library sizes
2. Subsample / rarify the counts (see: subsampleCounts)

Useful functions: nrow, ncol, dim, summary, table, quantile, unique, addPerCellQC, agglomerateByRank

### Prevalent and core taxonomic features

1. Estimate prevalence for your chosen feature (row, taxonomic group)
2. Identify all prevalent features and subset the data accordingly 
3. Report the thresholds and the dimensionality of the data before and after subsetting
4. Visualize prevalence

Useful functions: getPrevalence, getPrevalentTaxa, subsetByPrevalentTaxa


### Data exploration

1. Summarize sample metadata variables. (How many age groups, how they are distributed? 0%, 25%, 50%, 75%, and 100% quantiles of library size?)
2. Create two histograms. Another shows the distribution of absolute counts, another shows how CLR transformed values are distributed.
3. Visualize how relative abundances are distributed between taxa in samples.

Useful functions: nrow, ncol, dim, summary, table, quantile, unique, transformCounts, ggplot, wilcox.test, agglomerateByRank, plotAbundance

### Other functions

1. Merge data objects (merge, mergeSEs)
2. Melt the data for visualization purposes (meltAssay)


### Transformations

1. Transform abundance data with relative abundance and add a relative abundance assay (see [data-transformation](https://microbiome.github.io/OMA/taxonomic-information.html#data-transformation))
2. Transform abundance data with clr transformation and add a new assay
3. List the available assays by name
4. Pick one of the assays and show a subset of it
5. Subset the entire TreeSE data object, and check how this affects individual (transformed) assays

Optional:

6. If the data has phylogenetic tree, perform the phILR transformation



## Abundance tables

### Taxonomic levels

1. List the available taxonomic ranks in the data
2. Merge the data to Phylum level
3. Report dimensionality before and after aggomeration

Optional:

4. Perform CLR transformation on the data; does this affect agglomeration?
5. List full taxonomic information for some given taxa (Hint: [mapTaxonomy](https://microbiome.github.io/mia/reference/taxonomy-methods.html))

Useful functions: [taxonomyRanks](https://microbiome.github.io/mia/reference/taxonomy-methods.html), agglomerateByRank, mergeRows


### Alternative experiments (altExp)

1. Create taxonomic abundance tables for all different levels (splitByRanks)
2. Check the available alternative experiment (altExp) names before and after splitByRanks
3. Pick specific "experiment" (taxonomic rank) from specific altExp; and a specific assay

Optional:

4. Split the data based on other features (splitOn)



## Community diversity (alpha diversity)

### Alpha diversity basics

1. Calculate alpha diversity indices
2. Test if data agglomeration to higher taxonomic ranks affects the indices
3. Look for differences in alpha diversity between groups or correlation with a continuous variable

Useful functions: estimateDiversity, colSums, agglomerateByRank, kruskal.test, cor


### Alpha diversity extra

1. Estimate Shannon diversity for the data
2. Try also another diversity index and compare the results with a scatterplot
3. Compare Shannon diversity between groups (boxplot)
4. Is diversity significantly different between vegan and mixed diet?
5. Calculate and visualize library size, compare with diversity

Useful functions: estimateDiversity, colSums, geom_point, geom_boxplot




## Community composition (beta diversity)

### Beta diversity basics

1. Visualize community variation with different methods (PCA, MDS, NMDS, etc.) with plotReduceDim and with different dissimilarities and transformations,plot also other than the first two axes.
2. Use PERMANOVA to test differences in beta diversity. You can also try including continuous and/or categorical covariates
3. If there are statistical differences in PERMANOVA, test PERMDISP2 (betadisper function)
4. Do clustering
5. Try RDA to test the variance explained by external variables


### Beta diversity extra

1. Install the latest development version of mia from GitHub.
2. Load experimental dataset from mia.
3. Create a PCoA with Aitchison dissimilarities. How much coordinate 1 explains the differences? How about coordinate 2?
4. Create dbRDA with Bray-Curtis dissimilarities on relative abundances. Use PERMANOVA. Can differences between samples be explained with variables of sample meta data? 
5. Analyze diets' association on beta diversity. Calculate dbRDA and then PERMANOVA. Visualize coefficients. Which taxa's abundances differ the most between samples? 
6. Interpret your results. Is there association between community composition and location? What are those taxa that differ the most; find information from literature.

Useful functions: runMDS, runRDA, anova.cca, transformCounts, agglomerateByRank, ggplot, plotReducedDim, vegan::adonis2




## Differential abundance

### Univariate analyses

1. Get the abundances for an individual feature (taxonomic group / row)
2. Visualize the abundances per group with boxplot / jitterplot
3. Is the difference significant (Wilcoxon test)?
4. Is the difference significant (linear model with covariates)? 
5. How do transformations affect the outcome (log10, clr..)?
6. Get p-values for all features (taxa), for instance with a for loop
7. Do multiple testing correction
9. Compare the results from different tests with a scatterplot

Useful functions: [], ggplot2::geom_boxplot, ggplot2::geom_jitter, wilcox.test, lm.test, transformCounts, p.adjust


### Differential abundance analysis

1. install the latest development version of mia from GitHub.
2. Load experimental dataset from mia.
3. Compare abundances of each taxa between groups. First, use Wilcoxon or Kruskall-Wallis test. Then use some other method dedicated to microbiome data. 
4. Summarize findings by plotting a taxa vs samples heatmap. Add column annotation that tells the group of each sample, and row annotation that tells whether the difference of certain taxa was statistically significant.
5. Choose statistically significant taxa and visualize their abundances with boxplot & jitterplot.

Useful functions: wilcox.test, kruskal.test, ggplot, pheatmap, ComplexHeatMap::Heatmap, ancombc, aldex2, maaslin2, agglomerateByRank, transformCounts, subsetByPrevalentTaxa


## Visualization

### Multivariate ordination

1. Load experimental dataset from mia.
2. Create PCoA with Bray-Curtis dissimilarities
3. Create PCA with Aitchison dissimilarities
4. Visualize and compare both
5. Test other transformations, dissimilarities, and ordination methods

Useful functions: runMDS, runNMDS, transformCounts, ggplot, plotReducedDim


### Heatmap visualization

1. Load experimental dataset from mia.
2. Visualize abundances with heatmap
3. Visualize abundances with heatmap after CLR + Z transformation 

See the OMA book for examples.



## Multiomics

### Basic exploration

Here we learn how to conduct preliminary exploration on a MAE, using
HintikkaXOData as an example dataset.

1. Import the mia package, load HintikkaXOData with `data` and store it into a
   variable named `mae`
2. Which experiments make up the MAE? How many samples and features are contained
   by each experiment? You can get a summary for all experiments with `experiments`,
   and check for each individual experiment with `dim`, `nrow` and `ncol`
3. What are the names of the features and samples of the different experiments?
   You can see that with `rownames` and `colnames`, respectively
4. What information is known about the samples? Remember that information about
   samples is stored in the `colData` of the MAE
5. **Extra**: How do the samples of the individual experiments map to the
   columns of the MAE? You can find the sample mapping in the `sampleMap`
   of the MAE

So far so good. You explored a MAE and its experiments, getting a taste of how
information is organized in its internal structure.

### Experiment agglomeration

Here we learn how to manipulate an experiment contained by a MAE and save the
new modified version of the experiment in a suitable place (the altExp slot).

1. Import the mia package, load HintikkaXOData with `data` and store it into a
   variable named `mae`
2. Agglomerate the microbiota experiment by Genus and store the output into the
   `altExp` slot of the microbiota experiment, with the custom name
   `microbiota_genus`
3. How many features remain after agglomerating? What are their names?
4. **Extra**: create one more alternative experiment named
   `prevalent_microbiota_family`, which contains the microbiota experiment
   agglomerated by Family with a prevalence threshold of 10%. You can
   agglomerate and in parallel select by prevalence with
   `agglomerateByPrevalence`

Good job! You agglomerated one of the experiments in the MAE and stored it as
an alternative experiment.

### Experiment transformation

We proceed with an exercise on a different type of data manipulation, that is,
transformation of assays of individual experiments in the MAE.

1. Import the mia package, load HintikkaXOData with `data` and store it into a
   variable named `mae`
2. What assays are contained by each individual experiment? You can check their
   names with `assays`
3. Apply a log10 transformation to the assay of the metabolite experiment.
   For that you can use `transformCounts` and don't forget to specify the assay
   to be transformed with the argument `assay.type`
4. Apply a CLR transformation to the counts assay of the microbiota experiment.
   To ensure non-null values in the assay, set `pseudocount` equal to 1.
   
You made it! You learnt how to apply different transformations to the assays
of individual experiments in a MAE with `transformCounts`, specifying optional
arguments based on the used method.

### Assay extraction

The following exercise walks you through disassembling a MAE object in
order to retrieve a specific assay, or to store its components as
multiple separate csv files.

1. Import the mia package, load HintikkaXOData with `data` and store it into a
   variable named `mae`
2. Extract the individual metabolite experiment from the MAE into a distinct
   TreeSE object named `metabolites`
3. Which and how many assays are contained by `metabolites`? You can check that
   with `assays` or `assayNames`
4. Write a csv file for the nmr assay with `write.csv`. You can access an
   individual assay of a TreeSE with `assay` by specifying the name of the
   desired assay
5. **Extra**: Repeat step 1 - 4 also for the microbiota and biomarkers experiments,
   so that a completely disassembled version of the MAE is available
6. **Extra**: Besides experiments, MAEs also include a sampleData and a sampleMap,
   which are accessible with `colData(mae)` and `sampleMap(mae)`, respectively.
   Save also each of these two elements into a csv file.

Well done! You just splitted a MAE into its components and stored them as csv files.
[This script](https://github.com/JuliaTurkuDataScience/MicrobiomeAnalysis.jl/blob/main/src/assets/XO_preprocess.R)
shows a possible approach.

### MAE reconstruction

Next, we will try to reconstruct the same MAE from the files you created.
Make sure you know their names and location! Alternatively, you can download
[this directory](https://github.com/microbiome/course_2023_oulu/tree/main/data)
with the readily disassembled components of HintikkaXOData.

1. Read in the csv files containing assays with `read.csv` and save each
   of them into a variable named `<assay name>_assays`
2. Create one TreeSE from each assays object with the `TreeSummarizedExperiment`
   function, as explained in the exercise \@ref(construct-TreeSE)
3. Read in the sampleData and the sampleMap and store them into the
   variables `sample_data` and `sample_map`, respectively
4. Combine the components with `MultiAssayExperiment`, where the first argument
   is an `ExperimentList` (for now include only the microbiota and metabolites
   TreeSEs), the second is colData and the third is sampleMap
5. Make sure that the MAE experiments are identical to the original TreeSEs. You
   can do that qualitatively by checking their `head` and quantitatively by 
   looking at their `dim`
6. **Extra**: Add the biomarkers TreeSE as a new experiment to the MAE.
   Note that new experiments can be added to a MAE through simple concatenation
   with `c(mae, experiment)`

Good job! Now you are aware of how MAEs are built and we can proceed to some
analytical exercises.

### Cross-correlation analysis

Now we will perform a cross-correlation analysis between two of the
experiments in the MAE.

1. Import the mia package, load HintikkaXOData with `data` and store it into a
   variable named `mae`
2. Analyze correlations between the microbiota and the biomarkers experiments
   with `getExperimentCrossAssociation`. Don't forget to specify the experiments
   you want to compare with the arguments `experiment1` and `experiment2`, and
   which of their assays with `assay.type1` and `assay.type2`
3. What does the output look like? By default, correlation is measured in terms
   of Kendall tau coefficients. Repeat point 2, but this time change `method`
   to Spearman coefficients.
4. Are you able to infer significance from the output? In order to also obtain
   p-values from the cross-correlation analysis, repeat point 2 with the
   additional argument `test_significance = TRUE`
5. Visualize results with a heatmap similarly to the example in section
   \@ref(cross-correlation). Do you see any significant correlations?
   Interpret your results.
6. **Extra**: Perform cross-correlation analysis between the remaining
   experiments (microbiota vs metabolites and metabolites vs biomarkers) and
   visualize results with heatmaps
   
Great job! You performed a cross-correlation analysis between two experiments of
a MAE and visualized the results with a heatmap. You are also able to customise
the correlation method and significance testing used for the analysis.

