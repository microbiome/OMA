[["index.html", "Orchestrating Microbiome Analysis with Bioconductor Welcome", " Orchestrating Microbiome Analysis with Bioconductor Authors: Leo Lahti [aut], Tuomas Borman [aut, cre], Felix GM Ernst [aut], and others (see the full list of contributors) [ctb] Version: 0.98.16 Modified: 2023-07-29 Compiled: 2023-12-03 Environment: R version 4.3.1 (2023-06-16), Bioconductor 3.17 License: CC BY-NC-SA 3.0 US Copyright: Source: https://github.com/microbiome/OMA Welcome You are reading the online book, Orchestrating Microbiome Analysis with Bioconductor (Leo Lahti et al. 2021), where we walk through common strategies and workflows in microbiome data science. The book shows through concrete examples how you can take advantage of the latest developments in R/Bioconductor for the manipulation, analysis, and reproducible reporting of hierarchical and heterogeneous microbiome profiling data sets. The book was borne out of necessity, while updating microbiome analysis tools to work with Bioconductor classes that provide support for multi-modal data collections. Many of these techniques are generic and widely applicable in other contexts as well. This work has been heavily influenced by other similar resources, in particular the Orchestrating Single-Cell Analysis with Bioconductor (R. Amezquita et al. 2020), phyloseq tutorials (Ben J. Callahan et al. 2016) and microbiome tutorials (Shetty and Lahti 2019). This book extends these resources to teach the grammar of Bioconductor workflows in the context of microbiome data science. As such, it supports the adoption of general skills in the analysis of large, hierarchical, and multi-modal data collections. We focus on microbiome analysis tools, including entirely new, partially updated as well as previously established methods. This online resource and its associated ecosystem of microbiome data science tools are a result of a community-driven development process, and welcoming new contributors. Several individuals have contributed methods, workflows and improvements as acknowledged in the Introduction. You can find more information on how to find us online and join the developer community through the project homepage at microbiome.github.io. This online resource has been written in RMarkdown with the bookdown R package. The material is free to use with the Creative Commons Attribution-NonCommercial 3.0 License. "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } This work - Orchestrating Microbiome Analysis with Bioconductor (Leo Lahti et al. 2021) - contributes novel methods and educational resources for microbiome data science. It aims to teach the grammar of Bioconductor workflows in the context of microbiome data science. We show through concrete examples how to use the latest developments and data analytical strategies in R/Bioconductor for the manipulation, analysis, and reproducible reporting of hierarchical, heterogeneous, and multi-modal microbiome profiling data. The data science methodology is tightly integrated with the broader R/Bioconductor ecosystem that focuses on the development of high-quality open research software for life sciences (R. C. Gentleman et al. (2004), Huber et al. (2015)). The support for modularity and interoperability is a key to efficient resource sharing and collaborative development both within and across research fields. The central data infrastructure, the SummarizedExperiment data container and its derivatives, have already been widely adopted in microbiome research, single cell sequencing, and in other fields, allowing a rapid adoption and extensions of emerging data science techniques across application domains. We assume that the reader is already familiar with R programming. For references and tips on introductory material for R and Bioconductor, see Chapter 18. This online resource and its associated ecosystem of microbiome data science tools are a result of a community-driven development process, and welcoming new users and contributors. You can find more information on how to find us online and join the developer community through the project homepage at microbiome.github.io. The book is organized into three parts. We start by introducing the material and link to further resources for learning R and Bioconductor. We describe the key data infrastructure, the TreeSummarizedExperiment class that provides a container for microbiome data, and how to get started by loading microbiome data set in the context of this new framework. The second section, Focus Topics, covers the common steps in microbiome data analysis, beginning with the most common steps and progressing to more specialized methods in subsequent sections. Third, Workflows, provides case studies for the various datasets used throughout the book. Finally, Appendix, links to further resources and acknowledgments. "],["packages.html", "Chapter 2 Packages 2.1 Package installation 2.2 Package ecosystem", " Chapter 2 Packages .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } The Bioconductor microbiome data science framework consists of: data containers, designed to organize multi-assay microbiome data R/Bioconductor packages that provide dedicated methods community of users and developers This section provides an overview of the package ecosystem. Section 3.3 links to various open microbiome data resources that support this framework. 2.1 Package installation You can install all packages that are required to run every example in this book via the following command: source(&quot;https://raw.githubusercontent.com/microbiome/OMA/master/install_packages.R&quot;) 2.1.1 Installing specific packages You can install R packages of your choice with the following command line procedure. Bioconductor development version requires the installation of the latest R beta version. This is primarily recommended for those who already have experience with R/Bioconductor and need access to the latest updates. BiocManager::install(&quot;microbiome/mia&quot;, version=&quot;devel&quot;) Github development version provides access to the latest but potentially unstable features. This is useful when you want access to all available tools. devtools::install_github(&quot;microbiome/mia&quot;) 2.2 Package ecosystem Methods for (Tree)SummarizedExperiment and MultiAssayExperiment data containers are provided by multiple independent developers through R/Bioconductor packages. Some of these are listed below (tips on new packages are welcome). 2.2.1 mia package family The mia package family provides general methods for microbiome data wrangling, analysis and visualization. mia: Microbiome analysis tools (Felix G. M. Ernst, Shetty, and Lahti 2020) miaViz: Microbiome analysis specific visualization (Felix G. M. Ernst, Borman, and Lahti 2022) miaSim: Microbiome data simulations (Simsek et al. 2021) miaTime: Microbiome time series analysis (L. Lahti 2021) 2.2.2 Differential abundance The following DA methods support (Tree)SummarizedExperiment. ANCOMBC for differential abundance analysis benchdamic for benchmarking differential abundance methods ALDEx2 for differential abundance analysis 2.2.3 Other packages philr (Silverman et al. (2017)) phylogeny-aware phILR transformation MicrobiotaProcess for “tidy” analysis of microbiome and other ecological data Tools for Microbiome Analysis site listed over 130 R packages for microbiome data science in Many of these are not in Bioconductor, or do not directly support the data containers used in this book but can be often used with minor modifications. The phyloseq-based tools can be used by converting the TreeSE data into phyloseq with makePhyloseqFromTreeSummarizedExperiment. 2.2.4 Open microbiome data Hundreds of published microbiome data sets are readily available in these data containers (see 3.3). "],["containers.html", "Chapter 3 Microbiome Data 3.1 Data science framework 3.2 Data containers 3.3 Demonstration data 3.4 Loading experimental microbiome data", " Chapter 3 Microbiome Data .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } 3.1 Data science framework The building blocks of the framework are data container (SummarizedExperiment and its derivatives), packages from various developers using the TreeSE container, open demonstration data sets, in a separate chapter 3.3, and online tutorials including this online book as well as the various package vignettes and other materials. 3.2 Data containers SummarizedExperiment (SE) (Morgan et al. 2020) is a generic and highly optimized container for complex data structures. It has become a common choice for analysing various types of biomedical profiling data, such as RNAseq, ChIp-Seq, microarrays, flow cytometry, proteomics, and single-cell sequencing. [TreeSummarizedExperiment] (TreeSE) (Huang 2020) was developed as an extension to incorporate hierarchical information (such as phylogenetic trees and sample hierarchies) and reference sequences. [MultiAssayExperiment] (MAE) (Ramos et al. 2017) provides an organized way to bind several different data containers together in a single object. For example, we can bind microbiome data (in TreeSE container) with metabolomic profiling data (in SE) container, with (partially) shared sample metadata. This is convenient and robust for instance in subsetting and other data manipulation tasks. Microbiome data can be part of multiomics experiments and analysis strategies. We highlight how the methods used througout in this book relate to this data framework by using the TreeSummarizedExperiment, MultiAssayExperiment, and classes beyond. This section provides an introductions to these data containers. In microbiome data science, these containers link taxonomic abundance tables with rich side information on the features and samples. Taxonomic abundance data can be obtained by 16S rRNA amplicon or metagenomic sequencing, phylogenetic microarrays, or by other means. Many microbiome experiments include multiple versions and types of data generated independently or derived from each other through transformation or agglomeration. We start by providing recommendations on how to represent different varieties of multi-table data within the TreeSummarizedExperiment class. The options and recommendations are summarized in Table 3.1. 3.2.1 Assay data The original count-based taxonomic abundance tables may have different transformations, such as logarithmic, Centered Log-Ratio (CLR), or relative abundance. These are typically stored in assays. Let us load example data and rename it as tse. library(mia) data(&quot;hitchip1006&quot;, package = &quot;miaTime&quot;) tse &lt;- hitchip1006 The assays slot contains the experimental data as multiple count matrices. The result of assays is a list of matrices. assays(tse) ## List of length 1 ## names(1): counts Individual assays can be accessed via assay assay(tse, &quot;counts&quot;)[1:5,1:7] ## Sample-1 Sample-2 Sample-3 Sample-4 Sample-5 ## Actinomycetaceae 0 0 0 0 0 ## Aerococcus 0 0 0 0 0 ## Aeromonas 0 0 0 0 0 ## Akkermansia 21 36 475 61 34 ## Alcaligenes faecalis et rel. 1 1 1 2 1 ## Sample-6 Sample-7 ## Actinomycetaceae 0 0 ## Aerococcus 0 0 ## Aeromonas 0 0 ## Akkermansia 14 27 ## Alcaligenes faecalis et rel. 1 1 To illustrate the use of multiple assays, the relative abundance data can be calculated and stored along the original count data using transformAssay. tse &lt;- transformAssay(tse, assay.type = &quot;counts&quot;, method = &quot;relabundance&quot;) assays(tse) ## List of length 2 ## names(2): counts relabundance Now there are two assays available in the tse object, counts and relabundance. assay(tse, &quot;relabundance&quot;)[1:5,1:7] ## Sample-1 Sample-2 Sample-3 Sample-4 Sample-5 ## Actinomycetaceae 0.0000000 0.000e+00 0.0000000 0.0000000 0.000e+00 ## Aerococcus 0.0000000 0.000e+00 0.0000000 0.0000000 0.000e+00 ## Aeromonas 0.0000000 0.000e+00 0.0000000 0.0000000 0.000e+00 ## Akkermansia 0.0027657 3.547e-03 0.0666106 0.0056195 2.833e-03 ## Alcaligenes faecalis et rel. 0.0001317 9.854e-05 0.0001402 0.0001842 8.333e-05 ## Sample-6 Sample-7 ## Actinomycetaceae 0.0000000 0.0000000 ## Aerococcus 0.0000000 0.0000000 ## Aeromonas 0.0000000 0.0000000 ## Akkermansia 0.0017690 0.0045570 ## Alcaligenes faecalis et rel. 0.0001264 0.0001688 Here the dimension of the count data remains unchanged in transformation. This is in fact, a requirement for the assays. 3.2.2 colData colData contains data on the samples. colData(tse) ## DataFrame with 1151 rows and 10 columns ## age sex nationality DNA_extraction_method project ## &lt;integer&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; ## Sample-1 28 male US NA 1 ## Sample-2 24 female US NA 1 ## Sample-3 52 male US NA 1 ## Sample-4 22 female US NA 1 ## Sample-5 25 female US NA 1 ## ... ... ... ... ... ... ## Sample-1168 50 female Scandinavia r 40 ## Sample-1169 31 female Scandinavia r 40 ## Sample-1170 31 female Scandinavia r 40 ## Sample-1171 52 male Scandinavia r 40 ## Sample-1172 52 male Scandinavia r 40 ## diversity bmi_group subject time sample ## &lt;numeric&gt; &lt;factor&gt; &lt;factor&gt; &lt;numeric&gt; &lt;character&gt; ## Sample-1 5.76 severeobese 1 0 Sample-1 ## Sample-2 6.06 obese 2 0 Sample-2 ## Sample-3 5.50 lean 3 0 Sample-3 ## Sample-4 5.87 underweight 4 0 Sample-4 ## Sample-5 5.89 lean 5 0 Sample-5 ## ... ... ... ... ... ... ## Sample-1168 5.87 severeobese 244 8.1 Sample-1168 ## Sample-1169 5.87 overweight 245 2.3 Sample-1169 ## Sample-1170 5.92 overweight 245 8.2 Sample-1170 ## Sample-1171 6.04 overweight 246 2.1 Sample-1171 ## Sample-1172 5.74 overweight 246 7.9 Sample-1172 3.2.3 rowData rowData contains data on the features of the analyzed samples. Of particular interest to the microbiome field, this is used to store taxonomic information. rowData(tse) ## DataFrame with 130 rows and 3 columns ## Phylum Family ## &lt;character&gt; &lt;character&gt; ## Actinomycetaceae Actinobacteria Actinobacteria ## Aerococcus Firmicutes Bacilli ## Aeromonas Proteobacteria Proteobacteria ## Akkermansia Verrucomicrobia Verrucomicrobia ## Alcaligenes faecalis et rel. Proteobacteria Proteobacteria ## ... ... ... ## Vibrio Proteobacteria Proteobacteria ## Weissella et rel. Firmicutes Bacilli ## Wissella et rel. Firmicutes Bacilli ## Xanthomonadaceae Proteobacteria Proteobacteria ## Yersinia et rel. Proteobacteria Proteobacteria ## Genus ## &lt;character&gt; ## Actinomycetaceae Actinomycetaceae ## Aerococcus Aerococcus ## Aeromonas Aeromonas ## Akkermansia Akkermansia ## Alcaligenes faecalis et rel. Alcaligenes faecalis.. ## ... ... ## Vibrio Vibrio ## Weissella et rel. Weissella et rel. ## Wissella et rel. Wissella et rel. ## Xanthomonadaceae Xanthomonadaceae ## Yersinia et rel. Yersinia et rel. 3.2.4 rowTree Phylogenetic trees also play an important role in the microbiome field. The TreeSummarizedExperiment class can keep track of features and node relations via two functions, rowTree and rowLinks. A tree can be accessed via rowTree as phylo object. rowTree(tse) ## NULL The links to the individual features are available through rowLinks. rowLinks(tse) ## NULL Please note that there can be a 1:1 relationship between tree nodes and features, but this is not a must-have. This means there can be features, which are not linked to nodes, and nodes, which are not linked to features. To change the links in an existing object, the changeTree function is available. 3.2.5 Alternative experiments Alternative experiments complement assays. They can contain complementary data, which is no longer tied to the same dimensions as the assay data. However, the number of samples (columns) must be the same. This can come into play, for instance, when one has taxonomic abundance profiles quantified with different measurement technologies, such as phylogenetic microarrays, amplicon sequencing, or metagenomic sequencing. Another common use case is including abundance tables for different taxonomic ranks. Such alternative experiments concerning the same set of samples can be stored as Separate assays assuming that the taxonomic information can be mapped between features directly 1:1; or Data in the altExp slot of the TreeSummarizedExperiment, if the feature dimensions differ. Each element of the altExp slot is a SummarizedExperiment or an object from a derived class with independent feature data. The following shows how to store taxonomic abundance tables agglomerated at different taxonomic levels. However, the data could as well originate from entirely different measurement sources as long as the samples match. Let us first agglomerate the data to Phylum level. This yields a new TreeSE data object. tse_phylum &lt;- mergeFeaturesByRank(tse, &quot;Phylum&quot;, na.rm=TRUE) # Both have the same number of columns (samples) dim(tse) ## [1] 130 1151 dim(tse_phylum) ## [1] 8 1151 Then we can add the new phylum-level data object as an alternative experiment in the original data. # Add the new data object to the original data object as an alternative experiment with the name &quot;Phylum&quot; altExp(tse, &quot;Phylum&quot;) &lt;- tse_phylum # Check the alternative experiment names available in the data altExpNames(tse) ## [1] &quot;Phylum&quot; We can now subset the data, for instance, and this acts on both altExp and assay data. tse[,1:10] ## class: TreeSummarizedExperiment ## dim: 130 10 ## metadata(0): ## assays(2): counts relabundance ## rownames(130): Actinomycetaceae Aerococcus ... Xanthomonadaceae ## Yersinia et rel. ## rowData names(3): Phylum Family Genus ## colnames(10): Sample-1 Sample-2 ... Sample-9 Sample-10 ## colData names(10): age sex ... time sample ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(1): Phylum ## rowLinks: NULL ## rowTree: NULL ## colLinks: NULL ## colTree: NULL dim(altExp(tse[,1:10],&quot;Phylum&quot;)) ## [1] 8 10 For more details on altExp, you can check the introduction to the SingleCellExperiment package (Lun and Risso 2020). 3.2.6 MultiAssayExperiments Multiple experiments relate to complementary measurement types, such as transcriptomic or metabolomic profiling of the microbiome or the host. Multiple experiments can be represented using the same options as alternative experiments, or by using the MultiAssayExperiment class (Ramos et al. 2017). Depending on how the datasets relate to each other the data can be stored as: Separate altExp if the samples can be matched directly 1:1; or As MultiAssayExperiment objects, in which the connections between samples are defined through a sampleMap. Each element on the experimentsList of an MultiAssayExperiment is matrix or matrix-like objects, including SummarizedExperiment objects, and the number of samples can differ between the elements. For information have a look at the intro vignette of the MultiAssayExperiment package. Table 3.1: Recommended options for storing multiple data tables in microbiome studies The assays are best suited for data transformations (one-to-one match between samples and columns across the assays). The alternative experiments are particularly suitable for alternative versions of the data that are of same type but may have a different number of features (e.g. taxonomic groups); this is for instance the case with taxonomic abundance tables agglomerated at different levels (e.g. genus vs. phyla) or alternative profiling technologies (e.g. amplicon sequencing vs. shallow shotgun metagenomics). For alternative experiments one-to-one match between samples (cols) is libraryd but the alternative experiment tables can have different numbers of features (rows). Finally, elements of the MultiAssayExperiment provide the most flexible way to incorporate multi-omic data tables with flexible numbers of samples and features. We recommend these conventions as the basis for methods development and application in microbiome studies. Option Rows (features) Cols (samples) Recommended assays match match Data transformations altExp free match Alternative experiments MultiAssay free free (mapping) Multi-omic experiments 3.3 Demonstration data Open demonstration data for testing and benchmarking purposes is available from multiple locations. This chapter introduces some options. The other chapters of this book provide ample examples about the use of the data. 3.3.1 Package data The mia R package contains example datasets that are direct conversions from the alternative phyloseq container to the TreeSummarizedExperiment container. List the available datasets in the mia package: library(mia) data(package=&quot;mia&quot;) Load the GlobalPatterns data from the mia package: data(&quot;GlobalPatterns&quot;, package=&quot;mia&quot;) GlobalPatterns ## class: TreeSummarizedExperiment ## dim: 19216 26 ## metadata(0): ## assays(1): counts ## rownames(19216): 549322 522457 ... 200359 271582 ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(26): CL3 CC1 ... Even2 Even3 ## colData names(7): X.SampleID Primer ... SampleType Description ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (19216 rows) ## rowTree: 1 phylo tree(s) (19216 leaves) ## colLinks: NULL ## colTree: NULL 3.3.1.1 Tengeler2020 Tengeler2020 is derived from a randomised blinded study on the effects of gut microbiome on attention-deficit/hyperactivity disorder (ADHD) in humanised mice (C Tengeler et al. 2020). The dataset is briefly presented in these slides. 3.3.1.2 HintikkaXOData HintikkaXOData is derived from a study about the effects of fat diet and prebiotics on the microbiome of rat models (Hintikka et al. 2021). It is available in the MAE data container for R. The dataset is briefly summarized in these slides. 3.3.2 ExperimentHub data ExperimentHub provides a variety of data resources, including the microbiomeDataSets package (Morgan and Shepherd 2021; Leo Lahti, Ernst, and Shetty 2021). A table of the available datasets is available through the availableDataSets function. library(microbiomeDataSets) availableDataSets() ## Dataset ## 1 GrieneisenTSData ## 2 HintikkaXOData ## 3 LahtiMLData ## 4 LahtiMData ## 5 LahtiWAData ## 6 OKeefeDSData ## 7 SilvermanAGutData ## 8 SongQAData ## 9 SprockettTHData All data are downloaded from ExperimentHub and cached for local re-use. Check the man pages of each function for a detailed documentation of the data contents and references. Let us retrieve a MultiAssayExperiment dataset: # mae &lt;- HintikkaXOData() # Since HintikkaXOData is now added to mia, we can load it directly from there # We suggest to check other datasets from microbiomeDataSets data(HintikkaXOData, package = &quot;mia&quot;) mae &lt;- HintikkaXOData Data is available in SummarizedExperiment, r Biocpkg(\"TreeSummarizedExperiment\") and r Biocpkg(\"MultiAssayExperiment\") data containers; see the separate page on alternative containers for more details. 3.3.3 Curated metagenomic data curatedMetagenomicData is a large collection of curated human microbiome datasets, provided as (Tree)SummarizedExperiment objects (Pasolli et al. 2017). The resource provides curated human microbiome data including gene families, marker abundance, marker presence, pathway abundance, pathway coverage, and relative abundance for samples from different body sites. See the package homepage for more details on data availability and access. As one example, let us retrieve the Vatanen (2016) (Vatanen et al. 2016) data set. This is a larger collection with a bit longer download time. library(curatedMetagenomicData) tse &lt;- curatedMetagenomicData(&quot;Vatanen*&quot;, dryrun = FALSE, counts = TRUE) 3.3.4 Human microbiome compendium MicroBioMap dataset includes over 170k samples of publicly available 16S rRNA amplicon sequencing data, all processed using the same pipeline and reference database(Abdill2023?). After installing the MicroBioMap package (see the original website for instructions), you can load the compendium with library(MicroBioMap) cpd &lt;- getCompendium() This returns a TreeSummarizedExperiment object. Currently, the “tree” part of the TreeSummarizedExperiment is not populated, but that is on the roadmap(compendiumpackage?). After loading the compendium, you will have immediate access to nearly 170,000 microbiome samples of publicly available 16S rRNA amplicon sequencing data, all processed using the same pipeline and reference database. For more use examples in R/Bioconductor, see the MicroBioMap vignette. 3.3.5 Other data sources The current collections provide access to vast microbiome data resources. The output has to be converted into TreeSE/MAE separately. MGnifyR provides access to EBI/MGnify qiitr provides access to QIITA 3.4 Loading experimental microbiome data 3.4.1 16S workflow Result of amplicon sequencing is a large number of files that include all the sequences that were read from samples. Those sequences need to be matched with taxa. Additionally, we need to know how many times each taxa were found from each sample. There are several algorithms to do that, and DADA2 is one of the most common. You can find DADA2 pipeline tutorial, for example, here. After the DADA2 portion of the tutorial is completed, the data is stored into phyloseq object (Bonus: Handoff to phyloseq). To store the data to TreeSummarizedExperiment, follow the example below. You can find full workflow script without further explanations and comments from here Load required packages. library(mia) library(ggplot2) library(BiocManager) library(Biostrings) Create arbitrary example sample metadata like it was done in the tutorial. Usually, sample metadata is imported as a file. samples.out &lt;- rownames(seqtab.nochim) subject &lt;- sapply(strsplit(samples.out, &quot;D&quot;), `[`, 1) gender &lt;- substr(subject,1,1) subject &lt;- substr(subject,2,999) day &lt;- as.integer(sapply(strsplit(samples.out, &quot;D&quot;), `[`, 2)) samdf &lt;- data.frame(Subject=subject, Gender=gender, Day=day) samdf$When &lt;- &quot;Early&quot; samdf$When[samdf$Day&gt;100] &lt;- &quot;Late&quot; rownames(samdf) &lt;- samples.out Convert data into right format and create a TreeSE object. # Create a list that contains assays counts &lt;- t(seqtab.nochim) counts &lt;- as.matrix(counts) assays &lt;- SimpleList(counts = counts) # Convert colData and rowData into DataFrame samdf &lt;- DataFrame(samdf) taxa &lt;- DataFrame(taxa) # Create TreeSE tse &lt;- TreeSummarizedExperiment(assays = assays, colData = samdf, rowData = taxa ) # Remove mock sample like it is also done in DADA2 pipeline tutorial tse &lt;- tse[ , colnames(tse) != &quot;mock&quot;] Add sequences into referenceSeq slot and convert rownames into simpler format. # Convert sequences into right format dna &lt;- Biostrings::DNAStringSet( rownames(tse) ) # Add sequences into referenceSeq slot referenceSeq(tse) &lt;- dna # Convert rownames into ASV_number format rownames(tse) &lt;- paste0(&quot;ASV&quot;, seq( nrow(tse) )) tse ## class: TreeSummarizedExperiment ## dim: 232 20 ## metadata(0): ## assays(1): counts ## rownames(232): ASV1 ASV2 ... ASV231 ASV232 ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(20): F3D0 F3D1 ... F3D9 Mock ## colData names(4): Subject Gender Day When ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: NULL ## rowTree: NULL ## colLinks: NULL ## colTree: NULL ## referenceSeq: a DNAStringSet (232 sequences) 3.4.2 Import from external files Microbiome (taxonomic) profiling data is commonly distributed in various file formats. You can import such external data files as a (Tree)SummarizedExperiment object, but the details depend on the file format. Here, we provide examples for common formats. Some datasets and raw files to learn how to import raw data and construct TreeSE/MAE containers are available in the microbiome data repository. 3.4.2.1 CSV import CSV data tables can be imported with the standard R functions, then converted to the desired format. For detailed examples, you can check the Bioconductor course material by Martin Morgan. You can also check the example files and construct your own CSV files accordingly. Recommendations for the CSV files are the following. File names are arbitrary; we refer here to the same names as in the examples: Abundance table (assay_taxa.csv): data matrix (features x samples); first column provides feature IDs, the first row provides sample IDs; other values should be numeric (abundances). Row data (rowdata_taxa.csv): data table (features x info); first column provides feature IDs, the first row provides column headers; this file usually contains the taxonomic mapping between different taxonomic levels. Ideally, the feature IDs (row names) match one-to-one with the abundance table row names. Column data (coldata.csv): data table (samples x info); first column provides sample IDs, the first row provides column headers; this file usually contains the sample metadata/phenodata (such as subject age, health etc). Ideally, the sample IDs match one-to-one with the abundance table column names. After you have set up the CSV files, you can read them in R: count_file &lt;- &quot;data/assay_taxa.csv&quot; tax_file &lt;- &quot;data/rowdata_taxa.csv&quot; sample_file &lt;- &quot;data/coldata.csv&quot; # Load files counts &lt;- read.csv(count_file, row.names=1) # Abundance table (e.g. ASV data; to assay data) tax &lt;- read.csv(tax_file, row.names=1) # Taxonomy table (to rowData) samples &lt;- read.csv(sample_file, row.names=1) # Sample data (to colData) After reading the data in R, ensure the following: abundance table (counts): numeric matrix, with feature IDs as rownames and sample IDs as column names rowdata (tax): DataFrame, with feature IDs as rownames. If this is a data.frame you can use the function DataFrame() to change the format. Column names are free but in microbiome analysis they usually they refer to taxonomic ranks. The rownames in rowdata should match with rownames in abundance table. coldata (samples): DataFrame, with sample IDs as rownames. If this is a data.frame you can use the function DataFrame() to change the format. Column names are free. The rownames in coldata should match with colnames in abundance table. Always ensure that the tables have rownames! The TreeSE constructor compares rownames and ensures that, for example, right samples are linked with right patient. Also ensure that the row and column names match one-to-one between abundance table, rowdata, and coldata: # Match rows and columns counts &lt;- counts[rownames(tax), rownames(samples)] # Let us ensure that the data is in correct (numeric matrix) format: counts &lt;- as.matrix(counts) If you hesitate about the format of the data, you can compare to one of the available demonstration datasets, and make sure that your data components have the same format. There are many different source files and many different ways to read data in R. One can do data manipulation in R as well. Investigate the entries as follows. # coldata rownames match assay colnames all(rownames(samples) == colnames(counts)) # our dataset ## [1] TRUE class(samples) # should be data.frame or DataFrame ## [1] &quot;data.frame&quot; # rowdata rownames match assay rownames all(rownames(tax) == rownames(counts)) # our dataset ## [1] TRUE class(tax) # should be data.frame or DataFrame ## [1] &quot;data.frame&quot; # Counts class(counts) # should be a numeric matrix ## [1] &quot;matrix&quot; &quot;array&quot; 3.4.3 Constructing TreeSummarizedExperiment Now let us create the TreeSE object from the input data tables. Here we also convert the data objects in their preferred formats: counts –&gt; numeric matrix rowData –&gt; DataFrame colData –&gt; DataFrame The SimpleList could be used to include multiple alternative assays, if necessary. # Create a TreeSE tse_taxa &lt;- TreeSummarizedExperiment(assays = SimpleList(counts = counts), colData = DataFrame(samples), rowData = DataFrame(tax)) tse_taxa ## class: TreeSummarizedExperiment ## dim: 12706 40 ## metadata(0): ## assays(1): counts ## rownames(12706): GAYR01026362.62.2014 CVJT01000011.50.2173 ... ## JRJTB:03787:02429 JRJTB:03787:02478 ## rowData names(7): Phylum Class ... Species OTU ## colnames(40): C1 C2 ... C39 C40 ## colData names(6): Sample Rat ... Fat XOS ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: NULL ## rowTree: NULL ## colLinks: NULL ## colTree: NULL Now you should have a ready-made TreeSE data object that can be used in downstream analyses. 3.4.4 Constructing MultiAssayExperiment To construct a MultiAssayExperiment object, just combine multiple TreeSE data containers. Here we import metabolite data from the same study. count_file &lt;- &quot;data/assay_metabolites.csv&quot; sample_file &lt;- &quot;data/coldata.csv&quot; # Load files counts &lt;- read.csv(count_file, row.names=1) samples &lt;- read.csv(sample_file, row.names=1) # Create a TreeSE for the metabolite data tse_metabolite &lt;- TreeSummarizedExperiment(assays = SimpleList(concs = as.matrix(counts)), colData = DataFrame(samples)) tse_metabolite ## class: TreeSummarizedExperiment ## dim: 38 40 ## metadata(0): ## assays(1): concs ## rownames(38): Butyrate Acetate ... Malonate 1,3-dihydroxyacetone ## rowData names(0): ## colnames(40): C1 C2 ... C39 C40 ## colData names(6): Sample Rat ... Fat XOS ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: NULL ## rowTree: NULL ## colLinks: NULL ## colTree: NULL Now we can combine these two experiments into MAE. # Create an ExperimentList that includes experiments experiments &lt;- ExperimentList(microbiome = tse_taxa, metabolite = tse_metabolite) # Create a MAE mae &lt;- MultiAssayExperiment(experiments = experiments) mae ## A MultiAssayExperiment object of 2 listed ## experiments with user-defined names and respective classes. ## Containing an ExperimentList class object of length 2: ## [1] microbiome: TreeSummarizedExperiment with 12706 rows and 40 columns ## [2] metabolite: TreeSummarizedExperiment with 38 rows and 40 columns ## Functionality: ## experiments() - obtain the ExperimentList instance ## colData() - the primary/phenotype DataFrame ## sampleMap() - the sample coordination DataFrame ## `$`, `[`, `[[` - extract colData columns, subset, or experiment ## *Format() - convert into a long or wide DataFrame ## assays() - convert ExperimentList to a SimpleList of matrices ## exportClass() - save data to flat files 3.4.5 Import functions for standard formats Specific import functions are provided for: Biom files (see help(mia::loadFromBiom)) QIIME2 files (see help(mia::loadFromQIIME2)) Mothur files (see help(mia::loadFromMothur)) 3.4.5.1 Biom import Here we show how Biom files are imported into a TreeSE object using as an example Tengeler2020, which is further described in section 3.3.1.1. This dataset consists of 3 files, which can be fetched or downloaded from this repository: biom file: abundance table and taxonomy information csv file: sample metadata tree file: phylogenetic tree To begin with, we store the data in a local directory within the working directory, such as data/, and define the source file paths. biom_file_path &lt;- &quot;data/Aggregated_humanization2.biom&quot; sample_meta_file_path &lt;- &quot;data/Mapping_file_ADHD_aggregated.csv&quot; tree_file_path &lt;- &quot;data/Data_humanization_phylo_aggregation.tre&quot; Now we can read in the biom file and convert it into a TreeSE object. In addition, we retrieve the rank names from the prefixes of the feature names and then remove them with the rankFromPrefix and removeTaxaPrefixes optional arguments. library(mia) # read biom and convert it to TreeSE tse &lt;- loadFromBiom(biom_file_path, rankFromPrefix = TRUE, removeTaxaPrefixes = TRUE) # Check tse ## class: TreeSummarizedExperiment ## dim: 151 27 ## metadata(0): ## assays(1): counts ## rownames(151): 1726470 1726471 ... 17264756 17264757 ## rowData names(6): taxonomy1 Phylum ... Family Genus ## colnames(27): A110 A111 ... A38 A39 ## colData names(0): ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: NULL ## rowTree: NULL ## colLinks: NULL ## colTree: NULL The assays slot includes a list of abundance tables. The imported abundance table is named as “counts”. Let us inspect only the first cols and rows. assay(tse, &quot;counts&quot;)[1:3, 1:3] ## A110 A111 A12 ## 1726470 17722 11630 0 ## 1726471 12052 0 2679 ## 17264731 0 970 0 The rowdata includes taxonomic information from the biom file. The head() command shows just the beginning of the data table for an overview. knitr::kable() helps print the information more nicely. head(rowData(tse)) ## DataFrame with 6 rows and 6 columns ## taxonomy1 Phylum Class Order ## &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;character&gt; ## 1726470 &quot;k__Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 1726471 &quot;k__Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 17264731 &quot;k__Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 17264726 &quot;k__Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 1726472 &quot;k__Bacteria Verrucomicrobia Verrucomicrobiae Verrucomicrobiales ## 17264724 &quot;k__Bacteria Bacteroidetes Bacteroidia Bacteroidales ## Family Genus ## &lt;character&gt; &lt;character&gt; ## 1726470 Bacteroidaceae Bacteroides&quot; ## 1726471 Bacteroidaceae Bacteroides&quot; ## 17264731 Porphyromonadaceae Parabacteroides&quot; ## 17264726 Bacteroidaceae Bacteroides&quot; ## 1726472 Verrucomicrobiaceae Akkermansia&quot; ## 17264724 Bacteroidaceae Bacteroides&quot; We further polish the feature names by removing unnecessary characters and then replace the original rowData with its updated version. # Genus level has additional &#39;\\&quot;&#39;, so let&#39;s delete that also rowdata_modified &lt;- BiocParallel::bplapply(rowData(tse), FUN = stringr::str_remove, pattern = &#39;\\&quot;&#39;) # rowdata_modified is a list, so convert this back to DataFrame format. # and assign the cleaned data back to the TSE rowData rowData(tse) &lt;- DataFrame(rowdata_modified) # Now we have a nicer table head(rowData(tse)) ## DataFrame with 6 rows and 6 columns ## taxonomy1 Phylum Class Order ## &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;character&gt; ## 1726470 k__Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 1726471 k__Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 17264731 k__Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 17264726 k__Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 1726472 k__Bacteria Verrucomicrobia Verrucomicrobiae Verrucomicrobiales ## 17264724 k__Bacteria Bacteroidetes Bacteroidia Bacteroidales ## Family Genus ## &lt;character&gt; &lt;character&gt; ## 1726470 Bacteroidaceae Bacteroides ## 1726471 Bacteroidaceae Bacteroides ## 17264731 Porphyromonadaceae Parabacteroides ## 17264726 Bacteroidaceae Bacteroides ## 1726472 Verrucomicrobiaceae Akkermansia ## 17264724 Bacteroidaceae Bacteroides We notice that the imported biom file did not contain any colData yet, so only an empty dataframe appears in this slot. head(colData(tse)) ## DataFrame with 6 rows and 0 columns Let us add colData from the sample metadata, which is stored in a CSV file. # CSV file with colnames in the first row and rownames in the first column sample_meta &lt;- read.csv(sample_meta_file_path, sep = &quot;,&quot;, row.names = 1) # Add this sample data to colData of the taxonomic data object # Note that the data must be given in a DataFrame format (required for our purposes) colData(tse) &lt;- DataFrame(sample_meta) Now the colData includes the sample metadata. head(colData(tse)) ## DataFrame with 6 rows and 4 columns ## Treatment Cohort TreatmentxCohort Description ## &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;character&gt; ## A110 ADHD Cohort_1 ADHD_Cohort_1 A110 ## A12 ADHD Cohort_1 ADHD_Cohort_1 A12 ## A15 ADHD Cohort_1 ADHD_Cohort_1 A15 ## A19 ADHD Cohort_1 ADHD_Cohort_1 A19 ## A21 ADHD Cohort_2 ADHD_Cohort_2 A21 ## A23 ADHD Cohort_2 ADHD_Cohort_2 A23 Finally, we add a phylogenetic tree to the rowData slot. Such feature is available only in TreeSE objects. Similarly, Trees specifying the sample hierarchy can be stored in the colTree slot. Here, we read in the file containing the phylogenetic tree and insert it in corresponding slot of the TreeSE object. # Reads the tree file tree &lt;- ape::read.tree(tree_file_path) # Add tree to rowTree rowTree(tse) &lt;- tree # Check tse ## class: TreeSummarizedExperiment ## dim: 151 27 ## metadata(0): ## assays(1): counts ## rownames(151): 1726470 1726471 ... 17264756 17264757 ## rowData names(6): taxonomy1 Phylum ... Family Genus ## colnames(27): A110 A12 ... A35 A38 ## colData names(4): Treatment Cohort TreatmentxCohort Description ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (151 rows) ## rowTree: 1 phylo tree(s) (151 leaves) ## colLinks: NULL ## colTree: NULL Now the rowTree slot contains the phylogenetic tree: head(rowTree(tse)) 3.4.6 Conversions between data formats in R If the data has already been imported in R in another format, it can be readily converted into TreeSummarizedExperiment, as shown in our next example. Note that similar conversion functions to TreeSummarizedExperiment are available for multiple data formats via the mia package (see makeTreeSummarizedExperimentFrom* for phyloseq, Biom, and DADA2). library(mia) # phyloseq example data data(GlobalPatterns, package=&quot;phyloseq&quot;) GlobalPatterns_phyloseq &lt;- GlobalPatterns GlobalPatterns_phyloseq ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 19216 taxa and 26 samples ] ## sample_data() Sample Data: [ 26 samples by 7 sample variables ] ## tax_table() Taxonomy Table: [ 19216 taxa by 7 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 19216 tips and 19215 internal nodes ] # convert phyloseq to TSE GlobalPatterns_TSE &lt;- makeTreeSummarizedExperimentFromPhyloseq(GlobalPatterns_phyloseq) GlobalPatterns_TSE ## class: TreeSummarizedExperiment ## dim: 19216 26 ## metadata(0): ## assays(1): counts ## rownames(19216): 549322 522457 ... 200359 271582 ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(26): CL3 CC1 ... Even2 Even3 ## colData names(7): X.SampleID Primer ... SampleType Description ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (19216 rows) ## rowTree: 1 phylo tree(s) (19216 leaves) ## colLinks: NULL ## colTree: NULL We can also convert TreeSummarizedExperiment objects into phyloseq with respect to the shared components that are supported by both formats (i.e. taxonomic abundance table, sample metadata, taxonomic table, phylogenetic tree, sequence information). This is useful for instance when additional methods are available for phyloseq. # convert TSE to phyloseq GlobalPatterns_phyloseq2 &lt;- makePhyloseqFromTreeSummarizedExperiment(GlobalPatterns_TSE) GlobalPatterns_phyloseq2 ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 19216 taxa and 26 samples ] ## sample_data() Sample Data: [ 26 samples by 7 sample variables ] ## tax_table() Taxonomy Table: [ 19216 taxa by 7 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 19216 tips and 19215 internal nodes ] Conversion is possible between other data formats. Interested readers can refer to the following functions: * makeTreeSummarizedExperimentFromDADA2 * makeSummarizedExperimentFromBiom * loadFromMetaphlan * readQZA "],["datamanipulation.html", "Chapter 4 Data Manipulation 4.1 Tidying and subsetting 4.2 Add or modify data 4.3 Merge data", " Chapter 4 Data Manipulation .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } 4.1 Tidying and subsetting 4.1.1 Tidy data For several custom analysis and visualization packages, such as those from tidyverse, the SE data can be converted to a long data.frame format with meltAssay. library(mia) data(GlobalPatterns, package=&quot;mia&quot;) tse &lt;- GlobalPatterns tse &lt;- transformAssay(tse, MARGIN = &quot;samples&quot;, method=&quot;relabundance&quot;) molten_tse &lt;- mia::meltAssay(tse, add_row_data = TRUE, add_col_data = TRUE, assay.type = &quot;relabundance&quot;) molten_tse ## # A tibble: 499,616 × 17 ## FeatureID SampleID relabundance Kingdom Phylum Class Order Family Genus ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 549322 CL3 0 Archaea Crenarchaeo… Ther… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 549322 CC1 0 Archaea Crenarchaeo… Ther… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 549322 SV1 0 Archaea Crenarchaeo… Ther… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 549322 M31Fcsw 0 Archaea Crenarchaeo… Ther… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 549322 M11Fcsw 0 Archaea Crenarchaeo… Ther… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 549322 M31Plmr 0 Archaea Crenarchaeo… Ther… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 549322 M11Plmr 0 Archaea Crenarchaeo… Ther… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 549322 F21Plmr 0 Archaea Crenarchaeo… Ther… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 549322 M31Tong 0 Archaea Crenarchaeo… Ther… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 549322 M11Tong 0 Archaea Crenarchaeo… Ther… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## # ℹ 499,606 more rows ## # ℹ 8 more variables: Species &lt;chr&gt;, X.SampleID &lt;fct&gt;, Primer &lt;fct&gt;, ## # Final_Barcode &lt;fct&gt;, Barcode_truncated_plus_T &lt;fct&gt;, ## # Barcode_full_length &lt;fct&gt;, SampleType &lt;fct&gt;, Description &lt;fct&gt; 4.1.2 Subsetting Subsetting data helps to draw the focus of analysis on particular sets of samples and / or features. When dealing with large datasets, the subset of interest can be extracted and investigated separately. This might improve performance and reduce the computational load. Load: mia dplyr knitr data GlobalPatterns Let us store GlobalPatterns into tse and check its original number of features (rows) and samples (columns). Note: when subsetting by sample, expect the number of columns to decrease; when subsetting by feature, expect the number of rows to decrease. # Store data into se and check dimensions data(&quot;GlobalPatterns&quot;, package=&quot;mia&quot;) tse &lt;- GlobalPatterns # Show dimensions (features x samples) dim(tse) ## [1] 19216 26 4.1.2.1 Subset by sample (column-wise) For the sake of demonstration, here we will extract a subset containing only the samples of human origin (feces, skin or tongue), stored as SampleType within colData(tse) and also in tse. First, we would like to see all the possible values that SampleType can take on and how frequent those are: # Inspect possible values for SampleType unique(tse$SampleType) ## [1] Soil Feces Skin Tongue ## [5] Freshwater Freshwater (creek) Ocean Sediment (estuary) ## [9] Mock ## 9 Levels: Feces Freshwater Freshwater (creek) Mock ... Tongue # Show the frequency of each value tse$SampleType %&gt;% table() . Freq Feces 4 Freshwater 2 Freshwater (creek) 3 Mock 3 Ocean 3 Sediment (estuary) 3 Skin 3 Soil 3 Tongue 2 Note: after subsetting, expect the number of columns to equal the sum of the frequencies of the samples that you are interested in. For instance, ncols = Feces + Skin + Tongue = 4 + 3 + 2 = 9. Next, we logical index across the columns of tse (make sure to leave the first index empty to select all rows) and filter for the samples of human origin. For this, we use the information on the samples from the meta data colData(tse). # Subset by sample tse_subset_by_sample &lt;- tse[ , tse$SampleType %in% c(&quot;Feces&quot;, &quot;Skin&quot;, &quot;Tongue&quot;)] # Show dimensions dim(tse_subset_by_sample) ## [1] 19216 9 As a sanity check, the new object tse_subset_by_sample should have the original number of features (rows) and a number of samples (columns) equal to the sum of the samples of interest (in this case 9). Several characteristics can be used to subset by sample: origin sampling time sequencing method DNA / RNA barcode cohort 4.1.2.2 Subset by feature (row-wise) Similarly, here we will extract a subset containing only the features that belong to the phyla Actinobacteria and Chlamydiae, stored as Phylum within rowData(tse). However, subsetting by feature implies a few more obstacles, such as the presence of NA elements and the possible need for agglomeration. As previously, we would first like to see all the possible values that Phylum can take on and how frequent those are: # Inspect possible values for phylum unique(rowData(tse)$Phylum) ## [1] &quot;Crenarchaeota&quot; &quot;Euryarchaeota&quot; &quot;Actinobacteria&quot; &quot;Spirochaetes&quot; ## [5] &quot;MVP-15&quot; &quot;Proteobacteria&quot; &quot;SBR1093&quot; &quot;Fusobacteria&quot; ## [9] &quot;Tenericutes&quot; &quot;ZB3&quot; &quot;Cyanobacteria&quot; &quot;GOUTA4&quot; ## [13] &quot;TG3&quot; &quot;Chlorobi&quot; &quot;Bacteroidetes&quot; &quot;Caldithrix&quot; ## [17] &quot;KSB1&quot; &quot;SAR406&quot; &quot;LCP-89&quot; &quot;Thermi&quot; ## [21] &quot;Gemmatimonadetes&quot; &quot;Fibrobacteres&quot; &quot;GN06&quot; &quot;AC1&quot; ## [25] &quot;TM6&quot; &quot;OP8&quot; &quot;Elusimicrobia&quot; &quot;NC10&quot; ## [29] &quot;SPAM&quot; NA &quot;Acidobacteria&quot; &quot;CCM11b&quot; ## [33] &quot;Nitrospirae&quot; &quot;NKB19&quot; &quot;BRC1&quot; &quot;Hyd24-12&quot; ## [37] &quot;WS3&quot; &quot;PAUC34f&quot; &quot;GN04&quot; &quot;GN12&quot; ## [41] &quot;Verrucomicrobia&quot; &quot;Lentisphaerae&quot; &quot;LD1&quot; &quot;Chlamydiae&quot; ## [45] &quot;OP3&quot; &quot;Planctomycetes&quot; &quot;Firmicutes&quot; &quot;OP9&quot; ## [49] &quot;WPS-2&quot; &quot;Armatimonadetes&quot; &quot;SC3&quot; &quot;TM7&quot; ## [53] &quot;GN02&quot; &quot;SM2F11&quot; &quot;ABY1_OD1&quot; &quot;ZB2&quot; ## [57] &quot;OP11&quot; &quot;Chloroflexi&quot; &quot;SC4&quot; &quot;WS1&quot; ## [61] &quot;GAL15&quot; &quot;AD3&quot; &quot;WS2&quot; &quot;Caldiserica&quot; ## [65] &quot;Thermotogae&quot; &quot;Synergistetes&quot; &quot;SR1&quot; # Show the frequency of each value rowData(tse)$Phylum %&gt;% table() . Freq ABY1_OD1 7 AC1 1 Acidobacteria 1021 Actinobacteria 1631 AD3 9 Armatimonadetes 61 Bacteroidetes 2382 BRC1 13 Caldiserica 3 Caldithrix 10 CCM11b 2 Chlamydiae 21 Chlorobi 64 Chloroflexi 437 Crenarchaeota 106 Cyanobacteria 393 Elusimicrobia 31 Euryarchaeota 102 Fibrobacteres 7 Firmicutes 4356 Fusobacteria 37 GAL15 2 Gemmatimonadetes 191 GN02 8 GN04 7 GN06 2 GN12 1 GOUTA4 11 Hyd24-12 4 KSB1 6 LCP-89 2 LD1 2 Lentisphaerae 21 MVP-15 5 NC10 9 Nitrospirae 74 NKB19 16 OP11 6 OP3 30 OP8 27 OP9 4 PAUC34f 3 Planctomycetes 638 Proteobacteria 6416 SAR406 21 SBR1093 9 SC3 8 SC4 8 SM2F11 5 SPAM 22 Spirochaetes 124 SR1 5 Synergistetes 7 Tenericutes 143 TG3 5 Thermi 46 Thermotogae 1 TM6 27 TM7 32 Verrucomicrobia 470 WPS-2 20 WS1 5 WS2 2 WS3 70 ZB2 2 ZB3 2 Note: after subsetting, expect the number of columns to equal the sum of the frequencies of the feature(s) that you are interested in. For instance, nrows = Actinobacteria + Chlamydiae = 1631 + 21 = 1652. Depending on your research question, you might or might not need to agglomerate the data in the first place: if you want to find the abundance of each and every feature that belongs to Actinobacteria and Chlamydiae, agglomeration is not needed; if you want to find the total abundance of all features that belong to Actinobacteria or Chlamydiae, agglomeration is recommended. 4.1.2.2.1 Non-agglomerated data Next, we logical index across the rows of tse (make sure to leave the second index empty to select all columns) and filter for the features that fall in either Actinobacteria or Chlamydiae group. For this, we use the information on the samples from the metadata rowData(tse). The first term with the %in% operator includes all the features of interest, whereas the second term after the AND operator &amp; filters out all features that have an NA in place of the phylum variable. # Subset by feature tse_subset_by_feature &lt;- tse[rowData(tse)$Phylum %in% c(&quot;Actinobacteria&quot;, &quot;Chlamydiae&quot;) &amp; !is.na(rowData(tse)$Phylum), ] # Show dimensions dim(tse_subset_by_feature) ## [1] 1652 26 As a sanity check, the new object, tse_subset_by_feature, should have the original number of samples (columns) and a number of features (rows) equal to the sum of the features of interest (in this case, 1652). 4.1.2.2.2 Agglomerated data When total abundances of certain phyla are of relevance, the data is initially agglomerated by Phylum. Then, similar steps as in the case of non-agglomerated data are followed. # Agglomerate by phylum tse_phylum &lt;- tse %&gt;% mergeFeaturesByRank(rank = &quot;Phylum&quot;) # Subset by feature and remove NAs tse_phylum_subset_by_feature &lt;- tse_phylum[rowData(tse_phylum)$Phylum %in% c(&quot;Actinobacteria&quot;, &quot;Chlamydiae&quot;) &amp; !is.na(rowData(tse_phylum)$Phylum), ] # Show dimensions dim(tse_phylum_subset_by_feature) ## [1] 2 26 Note: as data was agglomerated, the number of rows should equal the number of phyla used to index (in this case, just 2). Alternatively: # Store features of interest into phyla phyla &lt;- c(&quot;Phylum:Actinobacteria&quot;, &quot;Phylum:Chlamydiae&quot;) # subset by feature tse_phylum_subset_by_feature &lt;- tse_phylum[phyla, ] # Show dimensions dim(tse_subset_by_feature) ## [1] 1652 26 The code above returns the non-agglomerated version of the data. Fewer characteristics can be used to subset by feature: Taxonomic rank Meta-taxonomic group For subsetting by kingdom, agglomeration does not apply, whereas for the other ranks it can be applied if necessary. 4.1.2.3 Subset by sample and feature Finally, we can subset data by sample and feature at once. The resulting subset contains all the samples of human origin and all the features of phyla Actinobacteria or Chlamydiae. # Subset by sample and feature and remove NAs tse_subset_by_sample_feature &lt;- tse[rowData(tse)$Phylum %in% c(&quot;Actinobacteria&quot;, &quot;Chlamydiae&quot;) &amp; !is.na(rowData(tse)$Phylum), tse$SampleType %in% c(&quot;Feces&quot;, &quot;Skin&quot;, &quot;Tongue&quot;)] # Show dimensions dim(tse_subset_by_sample_feature) ## [1] 1652 9 Note: the dimensions of tse_subset_by_sample_feature agree with those of the previous subsets (9 columns filtered by sample and 1652 rows filtered by feature). If a study was to consider and quantify the presence of Actinobacteria as well as Chlamydiae in different sites of the human body, tse_subset_by_sample_feature might be a suitable subset to start with. 4.1.2.4 Remove empty columns and rows Sometimes data might contain, e.g., features that are not present in any of the samples. This can occur, for example, after the data subsetting. In certain analyses, we might want to remove those instances. # Agglomerate data at Genus level tse_genus &lt;- mergeFeaturesByRank(tse, rank = &quot;Genus&quot;) # List bacteria that we want to include genera &lt;- c(&quot;Class:Thermoprotei&quot;, &quot;Genus:Sulfolobus&quot;, &quot;Genus:Sediminicola&quot;) # Subset data tse_genus_sub &lt;- tse_genus[genera, ] tse_genus_sub ## class: TreeSummarizedExperiment ## dim: 3 26 ## metadata(1): agglomerated_by_rank ## assays(1): counts ## rownames(3): Class:Thermoprotei Genus:Sulfolobus Genus:Sediminicola ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(26): CL3 CC1 ... Even2 Even3 ## colData names(7): X.SampleID Primer ... SampleType Description ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (3 rows) ## rowTree: 1 phylo tree(s) (19216 leaves) ## colLinks: NULL ## colTree: NULL # List total counts of each sample colSums(assay(tse_genus_sub, &quot;counts&quot;)) ## CL3 CC1 SV1 M31Fcsw M11Fcsw M31Plmr M11Plmr F21Plmr ## 1 0 0 1 1 0 4 1 ## M31Tong M11Tong LMEpi24M SLEpi20M AQC1cm AQC4cm AQC7cm NP2 ## 7 3 0 2 64 105 136 222 ## NP3 NP5 TRRsed1 TRRsed2 TRRsed3 TS28 TS29 Even1 ## 6433 1154 2 2 2 0 0 0 ## Even2 Even3 ## 2 0 Now we can see that certain samples do not include any bacteria. We can remove those. # Remove samples that do not contain any bacteria tse_genus_sub &lt;- tse_genus_sub[ , colSums(assay(tse_genus_sub, &quot;counts&quot;)) != 0 ] tse_genus_sub ## class: TreeSummarizedExperiment ## dim: 3 18 ## metadata(1): agglomerated_by_rank ## assays(1): counts ## rownames(3): Class:Thermoprotei Genus:Sulfolobus Genus:Sediminicola ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(18): CL3 M31Fcsw ... TRRsed3 Even2 ## colData names(7): X.SampleID Primer ... SampleType Description ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (3 rows) ## rowTree: 1 phylo tree(s) (19216 leaves) ## colLinks: NULL ## colTree: NULL The same action can also be applied to the features. # Take only those samples that are collected from feces, skin, or tongue tse_genus_sub &lt;- tse_genus[ , tse_genus$SampleType %in% c(&quot;Feces&quot;, &quot;Skin&quot;, &quot;Tongue&quot;)] tse_genus_sub ## class: TreeSummarizedExperiment ## dim: 1516 9 ## metadata(1): agglomerated_by_rank ## assays(1): counts ## rownames(1516): Class:Thermoprotei Genus:Sulfolobus ... ## Genus:Coprothermobacter Phylum:SR1 ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(9): M31Fcsw M11Fcsw ... TS28 TS29 ## colData names(7): X.SampleID Primer ... SampleType Description ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (1516 rows) ## rowTree: 1 phylo tree(s) (19216 leaves) ## colLinks: NULL ## colTree: NULL # What is the number of bacteria that are not present? sum(rowSums(assay(tse_genus_sub, &quot;counts&quot;)) == 0) ## [1] 435 We can see that there are bacteria that are not present in these samples we chose. We can remove those bacteria from the data. # Take only those bacteria that are present tse_genus_sub &lt;- tse_genus_sub[rowSums(assay(tse_genus_sub, &quot;counts&quot;)) &gt; 0, ] tse_genus_sub ## class: TreeSummarizedExperiment ## dim: 1081 9 ## metadata(1): agglomerated_by_rank ## assays(1): counts ## rownames(1081): Genus:Sulfolobus Order:NRP-J ... ## Genus:Coprothermobacter Phylum:SR1 ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(9): M31Fcsw M11Fcsw ... TS28 TS29 ## colData names(7): X.SampleID Primer ... SampleType Description ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (1081 rows) ## rowTree: 1 phylo tree(s) (19216 leaves) ## colLinks: NULL ## colTree: NULL 4.1.3 Splitting You can split the data based on variables by using the functions splitByRanks and splitOn. splitByRanks splits the data based on taxonomic ranks. Since the elements of the output list share columns, they can be stored into altExp. altExps(tse) &lt;- splitByRanks(tse) altExps(tse) ## List of length 7 ## names(7): Kingdom Phylum Class Order Family Genus Species If you want to split the data based on another variable than taxonomic rank, use splitOn. It works for row-wise and column-wise splitting. splitOn(tse, &quot;SampleType&quot;) ## List of length 9 ## names(9): Soil Feces Skin Tongue ... Ocean Sediment (estuary) Mock 4.2 Add or modify data The information contained by the colData of a TreeSE can be modified by accessing the desired variables. # modify the Description entries colData(tse)$Description &lt;- paste(colData(tse)$Description, &quot;modified description&quot;) # view modified variable head(tse$Description) ## [1] &quot;Calhoun South Carolina Pine soil, pH 4.9 modified description&quot; ## [2] &quot;Cedar Creek Minnesota, grassland, pH 6.1 modified description&quot; ## [3] &quot;Sevilleta new Mexico, desert scrub, pH 8.3 modified description&quot; ## [4] &quot;M3, Day 1, fecal swab, whole body study modified description&quot; ## [5] &quot;M1, Day 1, fecal swab, whole body study modified description&quot; ## [6] &quot;M3, Day 1, right palm, whole body study modified description&quot; New information can also be added to the experiment by creating a new variable. # simulate new data new_data &lt;- runif(ncol(tse)) # store new data as new variable in colData colData(tse)$NewVariable &lt;- new_data # view new variable head(tse$NewVariable) ## [1] 0.7482 0.4139 0.3670 0.5725 0.1988 0.3611 4.3 Merge data mia package has mergeSEs function that merges multiple SummarizedExperiment objects. For example, it is possible to combine multiple TreeSE objects which each includes one sample. mergeSEs works like dplyr joining functions. In fact, there are available dplyr-like aliases of mergeSEs, such as full_join. # Take subsets for demonstration purposes tse1 &lt;- tse[, 1] tse2 &lt;- tse[, 2] tse3 &lt;- tse[, 3] tse4 &lt;- tse[1:100, 4] # With inner join, we want to include all shared rows. When using mergeSEs function # all samples are always preserved. tse &lt;- mergeSEs(list(tse1, tse2, tse3, tse4), join = &quot;inner&quot;) tse ## class: TreeSummarizedExperiment ## dim: 100 4 ## metadata(0): ## assays(1): counts ## rownames(100): 239672 243675 ... 549322 951 ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(4): CC1 CL3 M31Fcsw SV1 ## colData names(8): X.SampleID Primer ... Description NewVariable ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (100 rows) ## rowTree: 1 phylo tree(s) (19216 leaves) ## colLinks: NULL ## colTree: NULL # Left join preserves all rows of the 1st object tse &lt;- mia::left_join(tse1, tse4, missing_values = 0) tse ## class: TreeSummarizedExperiment ## dim: 19216 2 ## metadata(0): ## assays(1): counts ## rownames(19216): 239672 243675 ... 239967 254851 ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(2): CL3 M31Fcsw ## colData names(8): X.SampleID Primer ... Description NewVariable ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (19216 rows) ## rowTree: 1 phylo tree(s) (19216 leaves) ## colLinks: NULL ## colTree: NULL 4.3.1 Additional functions mapTaxonomy mergeFeatures/mergeSamples "],["quality-control.html", "Chapter 5 Exploration and Quality Control 5.1 Abundance 5.2 Prevalence 5.3 Quality control", " Chapter 5 Exploration and Quality Control .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } This chapter focuses on the quality control and exploration of microbiome data and establishes commonly used descriptive summaries. Familiarizing with the peculiarities of a given dataset is the essential basis for any data analysis and model building. The dataset should not suffer from severe technical biases, and you should at least be aware of potential challenges, such as outliers, biases, unexpected patterns and so forth. Standard summaries and visualizations can help, and the rest comes with experience. The exploration and quality control can be iterative processes. library(mia) 5.1 Abundance Abundance visualization is an important data exploration approach. miaViz offers the function plotAbundanceDensity to plot the most abundant taxa with several options. Next, a few demonstrations are shown, using the (L. Lahti et al. 2014) dataset. A Jitter plot based on relative abundance data, similar to the one presented at (Salosensaari et al. 2021) supplementary figure 1, could be visualized as follows: # Load example data library(miaTime) library(miaViz) data(hitchip1006) tse &lt;- hitchip1006 # Add relative abundances tse &lt;- transformAssay(tse, MARGIN = &quot;samples&quot;, method = &quot;relabundance&quot;) # Use argument names # assay.type / assay.type / assay.type # depending on the mia package version plotAbundanceDensity(tse, layout = &quot;jitter&quot;, assay.type = &quot;relabundance&quot;, n = 40, point_size=1, point_shape=19, point_alpha=0.1) + scale_x_log10(label=scales::percent) The relative abundance values for the top-5 taxonomic features can be visualized as a density plot over a log scaled axis, with “nationality” indicated by colors: plotAbundanceDensity(tse, layout = &quot;density&quot;, assay.type = &quot;relabundance&quot;, n = 5, colour_by=&quot;nationality&quot;, point_alpha=1/10) + scale_x_log10() 5.2 Prevalence Prevalence quantifies the frequency of samples where certain microbes were detected (above a given detection threshold). The prevalence can be given as sample size (N) or percentage (unit interval). Investigating prevalence allows you either to focus on changes which pertain to the majority of the samples, or identify rare microbes, which may be conditionally abundant in a small number of samples. The population prevalence (frequency) at a 1% relative abundance threshold (detection = 1/100 and as_relative = TRUE), can look like this. head(getPrevalence(tse, detection = 1/100, sort = TRUE, as_relative = TRUE)) ## Faecalibacterium prausnitzii et rel. Ruminococcus obeum et rel. ## 0.9522 0.9140 ## Oscillospira guillermondii et rel. Clostridium symbiosum et rel. ## 0.8801 0.8714 ## Subdoligranulum variable at rel. Clostridium orbiscindens et rel. ## 0.8358 0.8315 The function arguments detection and as_relative can also be used to access, how many samples do pass a threshold for raw counts. Here, the population prevalence (frequency) at the absolute abundance threshold (as_relative = FALSE) at read count 1 (detection = 1) is accessed. head(getPrevalence(tse, detection = 1, sort = TRUE, assay.type = &quot;counts&quot;, as_relative = FALSE)) ## Uncultured Mollicutes Uncultured Clostridiales II ## 1 1 ## Uncultured Clostridiales I Tannerella et rel. ## 1 1 ## Sutterella wadsworthia et rel. Subdoligranulum variable at rel. ## 1 1 If the output should be used for subsetting or storing the data in the rowData, set sort = FALSE. 5.2.1 Prevalence analysis To investigate microbiome prevalence at a selected taxonomic level, two approaches are available. First the data can be agglomerated to the taxonomic level and getPrevalence applied on the resulting object. # Agglomerate taxa abundances to Phylum level, and add the new table # to the altExp slot altExp(tse,&quot;Phylum&quot;) &lt;- mergeFeaturesByRank(tse, &quot;Phylum&quot;) # Check prevalence for the Phylum abundance table from the altExp slot head(getPrevalence(altExp(tse,&quot;Phylum&quot;), detection = 1/100, sort = TRUE, assay.type = &quot;counts&quot;, as_relative = TRUE)) ## Firmicutes Bacteroidetes Actinobacteria Proteobacteria Verrucomicrobia ## 1.0000000 0.9852302 0.4821894 0.2988705 0.1277150 ## Cyanobacteria ## 0.0008688 Alternatively, the rank argument could be set to perform the agglomeration on the fly. head(getPrevalence(tse, rank = &quot;Phylum&quot;, detection = 1/100, sort = TRUE, assay.type = &quot;counts&quot;, as_relative = TRUE)) ## Firmicutes Bacteroidetes Actinobacteria Proteobacteria Verrucomicrobia ## 1.0000000 0.9852302 0.4821894 0.2988705 0.1277150 ## Cyanobacteria ## 0.0008688 Note that, by default, na.rm = TRUE is used for agglomeration in getPrevalence, whereas the default for mergeFeaturesByRank is FALSE to prevent accidental data loss. If you only need the names of the prevalent taxa, getPrevalentFeatures is available. This returns the taxa that exceed the given prevalence and detection thresholds. getPrevalentFeatures(tse, detection = 0, prevalence = 50/100) prev &lt;- getPrevalentFeatures(tse, detection = 0, prevalence = 50/100, rank = &quot;Phylum&quot;, sort = TRUE) prev Note that the detection and prevalence thresholds are not the same, since detection can be applied to relative counts or absolute counts depending on whether as_relative is set TRUE or FALSE The function ‘getPrevalentAbundance’ can be used to check the total relative abundance of the prevalent taxa (between 0 and 1). 5.2.2 Rare taxa Related functions are available for the analysis of rare taxa (rareMembers; rareAbundance; lowAbundance, getRareFeatures, subsetByRareFeatures). 5.2.3 Plotting prevalence To plot the prevalence, add the prevalence of each taxon to rowData. Here, we are analysing the Phylum level abundances, which are stored in the altExp slot. rowData(altExp(tse,&quot;Phylum&quot;))$prevalence &lt;- getPrevalence(altExp(tse,&quot;Phylum&quot;), detection = 1/100, sort = FALSE, assay.type = &quot;counts&quot;, as_relative = TRUE) The prevalences can then be plotted using the plotting functions from the scater package. library(scater) plotRowData(altExp(tse,&quot;Phylum&quot;), &quot;prevalence&quot;, colour_by = &quot;Phylum&quot;) The prevalence can also be visualized on the taxonomic tree with the miaViz package. altExps(tse) &lt;- splitByRanks(tse) altExps(tse) &lt;- lapply(altExps(tse), function(y){ rowData(y)$prevalence &lt;- getPrevalence(y, detection = 1/100, sort = FALSE, assay.type = &quot;counts&quot;, as_relative = TRUE) y }) top_phyla &lt;- getTopFeatures(altExp(tse,&quot;Phylum&quot;), method=&quot;prevalence&quot;, top=5L, assay.type=&quot;counts&quot;) top_phyla_mean &lt;- getTopFeatures(altExp(tse,&quot;Phylum&quot;), method=&quot;mean&quot;, top=5L, assay.type=&quot;counts&quot;) x &lt;- unsplitByRanks(tse, ranks = taxonomyRanks(tse)[1:6]) x &lt;- addTaxonomyTree(x) After some preparation, the data is assembled and can be plotted with plotRowTree. library(miaViz) plotRowTree(x[rowData(x)$Phylum %in% top_phyla,], edge_colour_by = &quot;Phylum&quot;, tip_colour_by = &quot;prevalence&quot;, node_colour_by = &quot;prevalence&quot;) Figure 5.1: Prevalence of top phyla as judged by prevalence plotRowTree(x[rowData(x)$Phylum %in% top_phyla_mean,], edge_colour_by = &quot;Phylum&quot;, tip_colour_by = &quot;prevalence&quot;, node_colour_by = &quot;prevalence&quot;) Figure 5.2: Prevalence of top phyla as judged by mean abundance 5.3 Quality control Next, let us load the GlobalPatterns dataset to illustrate standard microbiome data summaries. library(mia) data(&quot;GlobalPatterns&quot;, package=&quot;mia&quot;) tse &lt;- GlobalPatterns 5.3.1 Top taxa The getTopFeatures identifies top taxa in the data. # Pick the top taxa top_features &lt;- getTopFeatures(tse, method=&quot;median&quot;, top=10) # Check the information for these rowData(tse)[top_features, taxonomyRanks(tse)] ## DataFrame with 10 rows and 7 columns ## Kingdom Phylum Class Order ## &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;character&gt; ## 549656 Bacteria Cyanobacteria Chloroplast Stramenopiles ## 331820 Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 317182 Bacteria Cyanobacteria Chloroplast Stramenopiles ## 94166 Bacteria Proteobacteria Gammaproteobacteria Pasteurellales ## 279599 Bacteria Cyanobacteria Nostocophycideae Nostocales ## 158660 Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 329744 Bacteria Actinobacteria Actinobacteria Actinomycetales ## 326977 Bacteria Actinobacteria Actinobacteria Bifidobacteriales ## 248140 Bacteria Bacteroidetes Bacteroidia Bacteroidales ## 550960 Bacteria Proteobacteria Gammaproteobacteria Enterobacteriales ## Family Genus Species ## &lt;character&gt; &lt;character&gt; &lt;character&gt; ## 549656 NA NA NA ## 331820 Bacteroidaceae Bacteroides NA ## 317182 NA NA NA ## 94166 Pasteurellaceae Haemophilus Haemophilusparainflu.. ## 279599 Nostocaceae Dolichospermum NA ## 158660 Bacteroidaceae Bacteroides NA ## 329744 ACK-M1 NA NA ## 326977 Bifidobacteriaceae Bifidobacterium Bifidobacteriumadole.. ## 248140 Bacteroidaceae Bacteroides Bacteroidescaccae ## 550960 Enterobacteriaceae Providencia NA 5.3.2 Library size / read count The total counts/sample can be calculated using perCellQCMetrics/addPerCellQC from the scater package. The former one just calculates the values, whereas the latter one directly adds them to colData. library(scater) perCellQCMetrics(tse) ## DataFrame with 26 rows and 3 columns ## sum detected total ## &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; ## CL3 864077 6964 864077 ## CC1 1135457 7679 1135457 ## SV1 697509 5729 697509 ## M31Fcsw 1543451 2667 1543451 ## M11Fcsw 2076476 2574 2076476 ## ... ... ... ... ## TS28 937466 2679 937466 ## TS29 1211071 2629 1211071 ## Even1 1216137 4213 1216137 ## Even2 971073 3130 971073 ## Even3 1078241 2776 1078241 tse &lt;- addPerCellQC(tse) colData(tse) ## DataFrame with 26 rows and 10 columns ## X.SampleID Primer Final_Barcode Barcode_truncated_plus_T ## &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; ## CL3 CL3 ILBC_01 AACGCA TGCGTT ## CC1 CC1 ILBC_02 AACTCG CGAGTT ## SV1 SV1 ILBC_03 AACTGT ACAGTT ## M31Fcsw M31Fcsw ILBC_04 AAGAGA TCTCTT ## M11Fcsw M11Fcsw ILBC_05 AAGCTG CAGCTT ## ... ... ... ... ... ## TS28 TS28 ILBC_25 ACCAGA TCTGGT ## TS29 TS29 ILBC_26 ACCAGC GCTGGT ## Even1 Even1 ILBC_27 ACCGCA TGCGGT ## Even2 Even2 ILBC_28 ACCTCG CGAGGT ## Even3 Even3 ILBC_29 ACCTGT ACAGGT ## Barcode_full_length SampleType ## &lt;factor&gt; &lt;factor&gt; ## CL3 CTAGCGTGCGT Soil ## CC1 CATCGACGAGT Soil ## SV1 GTACGCACAGT Soil ## M31Fcsw TCGACATCTCT Feces ## M11Fcsw CGACTGCAGCT Feces ## ... ... ... ## TS28 GCATCGTCTGG Feces ## TS29 CTAGTCGCTGG Feces ## Even1 TGACTCTGCGG Mock ## Even2 TCTGATCGAGG Mock ## Even3 AGAGAGACAGG Mock ## Description sum detected ## &lt;factor&gt; &lt;numeric&gt; &lt;numeric&gt; ## CL3 Calhoun South Carolina Pine soil, pH 4.9 864077 6964 ## CC1 Cedar Creek Minnesota, grassland, pH 6.1 1135457 7679 ## SV1 Sevilleta new Mexico, desert scrub, pH 8.3 697509 5729 ## M31Fcsw M3, Day 1, fecal swab, whole body study 1543451 2667 ## M11Fcsw M1, Day 1, fecal swab, whole body study 2076476 2574 ## ... ... ... ... ## TS28 Twin #1 937466 2679 ## TS29 Twin #2 1211071 2629 ## Even1 Even1 1216137 4213 ## Even2 Even2 971073 3130 ## Even3 Even3 1078241 2776 ## total ## &lt;numeric&gt; ## CL3 864077 ## CC1 1135457 ## SV1 697509 ## M31Fcsw 1543451 ## M11Fcsw 2076476 ## ... ... ## TS28 937466 ## TS29 1211071 ## Even1 1216137 ## Even2 971073 ## Even3 1078241 The distribution of calculated library sizes can be visualized as a histogram (left), or by sorting the samples by library size (right). library(ggplot2) p1 &lt;- ggplot(as.data.frame(colData(tse))) + geom_histogram(aes(x = sum), color = &quot;black&quot;, fill = &quot;gray&quot;, bins = 30) + labs(x = &quot;Library size&quot;, y = &quot;Frequency (n)&quot;) + # scale_x_log10(breaks = scales::trans_breaks(&quot;log10&quot;, function(x) 10^x), # labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x))) + theme_bw() + theme(panel.grid.major = element_blank(), # Removes the grid panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = &quot;black&quot;)) # Adds y-axis library(dplyr) df &lt;- as.data.frame(colData(tse)) %&gt;% arrange(sum) %&gt;% mutate(index = 1:n()) p2 &lt;- ggplot(df, aes(y = index, x = sum/1e6)) + geom_point() + labs(x = &quot;Library size (million reads)&quot;, y = &quot;Sample index&quot;) + theme_bw() + theme(panel.grid.major = element_blank(), # Removes the grid panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = &quot;black&quot;)) # Adds y-axis library(patchwork) p1 + p2 Figure 5.3: Library size distribution. Library sizes other variables from colData can be visualized by using specified function called plotColData. library(ggplot2) # Sort samples by read count, order the factor levels, and store back to tse as DataFrame # TODO: plotColData could include an option for sorting samples based on colData variables colData(tse) &lt;- as.data.frame(colData(tse)) %&gt;% arrange(X.SampleID) %&gt;% mutate(X.SampleID = factor(X.SampleID, levels=X.SampleID)) %&gt;% DataFrame plotColData(tse,&quot;sum&quot;,&quot;X.SampleID&quot;, colour_by = &quot;SampleType&quot;) + theme(axis.text.x = element_text(angle = 45, hjust=1)) + labs(y = &quot;Library size (N)&quot;, x = &quot;Sample ID&quot;) Figure 5.4: Library sizes per sample. plotColData(tse,&quot;sum&quot;,&quot;SampleType&quot;, colour_by = &quot;SampleType&quot;) + theme(axis.text.x = element_text(angle = 45, hjust=1)) Figure 5.5: Library sizes per sample type. In addition, data can be rarefied with subsampleCounts, which normalises the samples to an equal number of reads. However, this practice has been discouraged for the analysis of differentially abundant microorganisms (see (P. J. McMurdie and Holmes 2014)). 5.3.3 Contaminant sequences Samples might be contaminated with exogenous sequences. The impact of each contaminant can be estimated based on their frequencies and concentrations across the samples. The following decontam functions are based on the (Davis et al. 2018) and support such functionality: isContaminant, isNotContaminant addContaminantQC, addNotContaminantQC "],["taxonomic-information.html", "Chapter 6 Taxonomic Information 6.1 Assigning taxonomic information. 6.2 Functions to access taxonomic information 6.3 Data agglomeration 6.4 Data transformation", " Chapter 6 Taxonomic Information .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } library(mia) data(&quot;GlobalPatterns&quot;, package = &quot;mia&quot;) tse &lt;- GlobalPatterns Taxonomic information is a key part of analyzing microbiome data and without it, any type of data analysis probably will not make much sense. However, the degree of detail of taxonomic information differs depending on the dataset and annotation data used. Therefore, the mia package expects a loose assembly of taxonomic information and assumes certain key aspects: Taxonomic information is given as character vectors or factors in the rowData of a SummarizedExperiment object. The columns containing the taxonomic information must be named domain, kingdom, phylum, class, order, family, genus, species or with a capital first letter. the columns must be given in the order shown above column can be omited, but the order must remain In this chapter, we will refer to co-abundant groups as CAGs, which are clusters of taxa that co-vary across samples. 6.1 Assigning taxonomic information. There are a number of methods to assign taxonomic information. We like to give a short introduction about the methods available without ranking one over the other. This has to be your choice based on the result for the individual dataset. 6.1.1 dada2 The dada2 package (Benjamin J. Callahan et al. 2016) implements the assignTaxonomy function, which takes as input the ASV sequences associated with each row of data and a training dataset. For more information visit the dada2 homepage. 6.1.2 DECIPHER The DECIPHER package (Wright 2020) implements the IDTAXA algorithm to assign either taxonomic information or function information. For mia only the first option is of interest for now and more information can be found on the DECIPHER website. 6.2 Functions to access taxonomic information checkTaxonomy checks whether the taxonomic information is usable for mia checkTaxonomy(tse) ## [1] TRUE Since the rowData can contain other data, taxonomyRanks will return the columns mia assumes to contain the taxonomic information. taxonomyRanks(tse) ## [1] &quot;Kingdom&quot; &quot;Phylum&quot; &quot;Class&quot; &quot;Order&quot; &quot;Family&quot; &quot;Genus&quot; &quot;Species&quot; This can then be used to subset the rowData to columns needed. rowData(tse)[, taxonomyRanks(tse)] ## DataFrame with 19216 rows and 7 columns ## Kingdom Phylum Class Order Family ## &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;character&gt; ## 549322 Archaea Crenarchaeota Thermoprotei NA NA ## 522457 Archaea Crenarchaeota Thermoprotei NA NA ## 951 Archaea Crenarchaeota Thermoprotei Sulfolobales Sulfolobaceae ## 244423 Archaea Crenarchaeota Sd-NA NA NA ## 586076 Archaea Crenarchaeota Sd-NA NA NA ## ... ... ... ... ... ... ## 278222 Bacteria SR1 NA NA NA ## 463590 Bacteria SR1 NA NA NA ## 535321 Bacteria SR1 NA NA NA ## 200359 Bacteria SR1 NA NA NA ## 271582 Bacteria SR1 NA NA NA ## Genus Species ## &lt;character&gt; &lt;character&gt; ## 549322 NA NA ## 522457 NA NA ## 951 Sulfolobus Sulfolobusacidocalda.. ## 244423 NA NA ## 586076 NA NA ## ... ... ... ## 278222 NA NA ## 463590 NA NA ## 535321 NA NA ## 200359 NA NA ## 271582 NA NA taxonomyRankEmpty checks for empty values in the given rank and returns a logical vector of length(x). all(!taxonomyRankEmpty(tse, rank = &quot;Kingdom&quot;)) ## [1] TRUE table(taxonomyRankEmpty(tse, rank = &quot;Genus&quot;)) ## ## FALSE TRUE ## 8008 11208 table(taxonomyRankEmpty(tse, rank = &quot;Species&quot;)) ## ## FALSE TRUE ## 1413 17803 getTaxonomyLabels is a multi-purpose function, which turns taxonomic information into a character vector of length(x) head(getTaxonomyLabels(tse)) ## [1] &quot;Class:Thermoprotei&quot; &quot;Class:Thermoprotei_1&quot; ## [3] &quot;Species:Sulfolobusacidocaldarius&quot; &quot;Class:Sd-NA&quot; ## [5] &quot;Class:Sd-NA_1&quot; &quot;Class:Sd-NA_2&quot; By default, this will use the lowest non-empty information to construct a string with the following scheme level:value. If all levels are the same, this part is omitted, but can be added by setting with_rank = TRUE. phylum &lt;- !is.na(rowData(tse)$Phylum) &amp; vapply(data.frame(apply(rowData(tse)[, taxonomyRanks(tse)[3:7]], 1L, is.na)), all, logical(1)) head(getTaxonomyLabels(tse[phylum,])) ## [1] &quot;Crenarchaeota&quot; &quot;Crenarchaeota_1&quot; &quot;Crenarchaeota_2&quot; &quot;Actinobacteria&quot; ## [5] &quot;Actinobacteria_1&quot; &quot;Spirochaetes&quot; head(getTaxonomyLabels(tse[phylum,], with_rank = TRUE)) ## [1] &quot;Phylum:Crenarchaeota&quot; &quot;Phylum:Crenarchaeota_1&quot; ## [3] &quot;Phylum:Crenarchaeota_2&quot; &quot;Phylum:Actinobacteria&quot; ## [5] &quot;Phylum:Actinobacteria_1&quot; &quot;Phylum:Spirochaetes&quot; By default the return value of getTaxonomyLabels contains only unique elements by passing it through make.unique. This step can be omitted by setting make_unique = FALSE. head(getTaxonomyLabels(tse[phylum,], with_rank = TRUE, make_unique = FALSE)) ## [1] &quot;Phylum:Crenarchaeota&quot; &quot;Phylum:Crenarchaeota&quot; &quot;Phylum:Crenarchaeota&quot; ## [4] &quot;Phylum:Actinobacteria&quot; &quot;Phylum:Actinobacteria&quot; &quot;Phylum:Spirochaetes&quot; To apply the loop resolving function resolveLoop from the TreeSummarizedExperiment package (Huang 2020) within getTaxonomyLabels, set resolve_loops = TRUE. The function getUniqueFeatures gives a list of unique taxa for the specified taxonomic rank. head(getUniqueFeatures(tse, rank = &quot;Phylum&quot;)) ## [1] &quot;Crenarchaeota&quot; &quot;Euryarchaeota&quot; &quot;Actinobacteria&quot; &quot;Spirochaetes&quot; ## [5] &quot;MVP-15&quot; &quot;Proteobacteria&quot; 6.2.1 Generate a taxonomic tree on the fly To create a taxonomic tree, taxonomyTree used the information and returns a phylo object. Duplicate information from the rowData is removed. taxonomyTree(tse) ## ## Phylogenetic tree with 1645 tips and 1089 internal nodes. ## ## Tip labels: ## Species:Cenarchaeumsymbiosum, Species:pIVWA5, Species:CandidatusNitrososphaeragargensis, Species:SCA1145, Species:SCA1170, Species:Sulfolobusacidocaldarius, ... ## Node labels: ## root:ALL, Kingdom:Archaea, Phylum:Crenarchaeota, Class:C2, Class:Sd-NA, Class:Thaumarchaeota, ... ## ## Rooted; includes branch lengths. tse &lt;- addTaxonomyTree(tse) tse ## class: TreeSummarizedExperiment ## dim: 19216 26 ## metadata(0): ## assays(1): counts ## rownames(19216): Class:Thermoprotei Class:Thermoprotei ... Phylum:SR1 ## Phylum:SR1 ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(26): CL3 CC1 ... Even2 Even3 ## colData names(7): X.SampleID Primer ... SampleType Description ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (19216 rows) ## rowTree: 1 phylo tree(s) (1645 leaves) ## colLinks: NULL ## colTree: NULL The implementation is based on the toTree function from the TreeSummarizedExperiment package (Huang 2020). 6.3 Data agglomeration One of the main applications of taxonomic information in regards to count data is to agglomerate count data on taxonomic levels and track the influence of changing conditions through these levels. For this mia contains the mergeFeaturesByRank function. The ideal location to store the agglomerated data is as an alternative experiment. tse &lt;- transformAssay(tse, assay.type = &quot;counts&quot;, method = &quot;relabundance&quot;) altExp(tse, &quot;Family&quot;) &lt;- mergeFeaturesByRank(tse, rank = &quot;Family&quot;, agglomerateTree = TRUE) altExp(tse, &quot;Family&quot;) ## class: TreeSummarizedExperiment ## dim: 603 26 ## metadata(1): agglomerated_by_rank ## assays(2): counts relabundance ## rownames(603): Class:Thermoprotei Family:Sulfolobaceae ... ## Family:Thermodesulfobiaceae Phylum:SR1 ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(26): CL3 CC1 ... Even2 Even3 ## colData names(7): X.SampleID Primer ... SampleType Description ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (603 rows) ## rowTree: 1 phylo tree(s) (496 leaves) ## colLinks: NULL ## colTree: NULL If multiple assays (counts and relabundance) exist, both will be agglomerated. assayNames(tse) ## [1] &quot;counts&quot; &quot;relabundance&quot; assayNames(altExp(tse, &quot;Family&quot;)) ## [1] &quot;counts&quot; &quot;relabundance&quot; assay(altExp(tse, &quot;Family&quot;), &quot;relabundance&quot;)[1:5, 1:7] ## CL3 CC1 SV1 M31Fcsw M11Fcsw M31Plmr M11Plmr ## Class:Thermoprotei 0.0000000 0.000e+00 0 0 0 0 0.000e+00 ## Family:Sulfolobaceae 0.0000000 0.000e+00 0 0 0 0 2.305e-06 ## Class:Sd-NA 0.0000000 0.000e+00 0 0 0 0 0.000e+00 ## Order:NRP-J 0.0001991 2.070e-04 0 0 0 0 6.914e-06 ## Family:SAGMA-X 0.0000000 6.165e-06 0 0 0 0 0.000e+00 assay(altExp(tse, &quot;Family&quot;), &quot;counts&quot;)[1:5, 1:7] ## CL3 CC1 SV1 M31Fcsw M11Fcsw M31Plmr M11Plmr ## Class:Thermoprotei 0 0 0 0 0 0 0 ## Family:Sulfolobaceae 0 0 0 0 0 0 1 ## Class:Sd-NA 0 0 0 0 0 0 0 ## Order:NRP-J 172 235 0 0 0 0 3 ## Family:SAGMA-X 0 7 0 0 0 0 0 altExpNames now consists of Family level data. This can be extended to use any taxonomic level listed in mia::taxonomyRanks(tse). Rare taxa can also be aggregated into a single group “Other” instead of filtering them out. A suitable function for this is mergeFeaturesByPrevalence. The number of rare taxa is higher on the species level, which causes the need for data agglomeration by prevalence. altExp(tse, &quot;Species_byPrevalence&quot;) &lt;- mergeFeaturesByPrevalence(tse, rank = &quot;Species&quot;, other_label = &quot;Other&quot;, prevalence = 5 / 100, detection = 1 / 100, as_relative = T) altExp(tse, &quot;Species_byPrevalence&quot;) ## class: TreeSummarizedExperiment ## dim: 113 26 ## metadata(2): agglomerated_by_rank agglomerated_by_rank ## assays(2): counts relabundance ## rownames(113): Family:MarinegroupII Order:Actinomycetales ... ## Species:Veillonellaparvula Other ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(26): CL3 CC1 ... Even2 Even3 ## colData names(7): X.SampleID Primer ... SampleType Description ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: NULL ## rowTree: NULL ## colLinks: NULL ## colTree: NULL assay(altExp(tse, &quot;Species_byPrevalence&quot;), &quot;relabundance&quot;)[88:92, 1:7] ## CL3 CC1 SV1 M31Fcsw M11Fcsw M31Plmr ## Genus:Luteolibacter 9.837e-05 8.076e-04 0.0001233 3.239e-06 5.297e-06 4.034e-05 ## Genus:MC18 5.664e-02 2.110e-01 0.0289530 4.471e-05 2.649e-05 1.071e-04 ## Class:Phycisphaerae 6.958e-03 1.816e-02 0.0585985 5.831e-06 4.816e-06 4.729e-05 ## Order:envOPS12 2.315e-05 7.046e-06 0.0000000 1.296e-06 4.816e-07 1.391e-06 ## Order:Clostridiales 1.017e-03 1.629e-04 0.0003857 1.473e-02 2.005e-02 1.647e-03 ## M11Plmr ## Genus:Luteolibacter 1.291e-04 ## Genus:MC18 7.882e-04 ## Class:Phycisphaerae 4.033e-04 ## Order:envOPS12 4.609e-06 ## Order:Clostridiales 1.719e-03 # Saving the tse for later tseGlobalPatterns &lt;- tse 6.3.1 Taxa clustering Another way to agglomerate the data is to cluster the taxa. To do so, we usually start by doing a compositionality aware transformation such as CLR, followed by the application of a standard clustering method. Here is an example that does a CLR transformation followed by the hierarchical clustering algorithm. First, we import the library bluster that simplifies the clustering. library(bluster) Then we do the CLR transform followed by the clustering. We will cluster with two different distances: the euclidean distance and the kendall distance. # Get the data data(&quot;peerj13075&quot;, package = &quot;mia&quot;) tse &lt;- peerj13075 # The result of the CLR transform is stored in the assay clr tse &lt;- transformAssay(tse, method = &quot;clr&quot;, pseudocount = 1) tse &lt;- transformAssay(tse, assay.type = &quot;clr&quot;, method = &quot;z&quot;, MARGIN = &quot;features&quot;) # Cluster (with euclidean distance) on the features of the z assay tse &lt;- cluster(tse, assay.type = &quot;z&quot;, clust.col = &quot;hclustEuclidean&quot;, MARGIN = &quot;features&quot;, HclustParam(dist.fun = stats::dist, method = &quot;ward.D2&quot;)) # Declare the Kendall dissimilarity computation function kendall_dissimilarity &lt;- function(x) { as.dist(1 - cor(t(x), method = &quot;kendall&quot;)) } # Cluster (with Kendall dissimilarity) on the features of the z assay tse &lt;- cluster(tse, assay.type = &quot;z&quot;, clust.col = &quot;hclustKendall&quot;, MARGIN = &quot;features&quot;, HclustParam(dist.fun = kendall_dissimilarity, method = &quot;ward.D2&quot;)) Let us store the resulting cluster indices in the rowData column specified with the clust.col parameter. # Checking the clusters clusters_euclidean &lt;- rowData(tse)$hclustEuclidean head(clusters_euclidean, 10) ## OTU1 OTU2 OTU7 OTU9 OTU10 OTU12 OTU14 OTU15 OTU18 OTU19 ## 1 2 1 1 1 1 3 4 3 2 ## Levels: 1 2 3 4 5 clusters_kendall &lt;- rowData(tse)$hclustKendall head(clusters_kendall, 10) ## OTU1 OTU2 OTU7 OTU9 OTU10 OTU12 OTU14 OTU15 OTU18 OTU19 ## 1 2 1 3 3 1 3 1 1 3 ## Levels: 1 2 3 4 To better visualize the results and the distribution of the clusters, we can plot the histogram of the clusters. library(ggplot2) library(patchwork) # TO arrange several plots as a grid plot1 &lt;- ggplot(as.data.frame(rowData(tse)), aes(x = clusters_euclidean)) + geom_bar() + labs(title = &quot;CAG size distribution (Euclidean distance)&quot;, x = &quot;Clusters&quot;, y = &quot;Feature count (n)&quot;) plot2 &lt;- ggplot(as.data.frame(rowData(tse)), aes(x = clusters_kendall)) + geom_bar() + labs(title = &quot;CAG size distribution (1 - tau)&quot;, x = &quot;Clusters&quot;, y = &quot;Feature count (n)&quot;) plot1 + plot2 + plot_layout(ncol = 2) It’s also possible to merge the rows by cluster. # Aggregate clusters as a sum of each cluster values tse_merged &lt;- mergeFeatures(tse, clusters_euclidean) tse_merged ## class: TreeSummarizedExperiment ## dim: 5 58 ## metadata(0): ## assays(3): counts clr z ## rownames(5): 1 2 3 4 5 ## rowData names(8): kingdom phylum ... hclustEuclidean hclustKendall ## colnames(58): ID1 ID2 ... ID57 ID58 ## colData names(5): Sample Geographical_location Gender Age Diet ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: NULL ## rowTree: NULL ## colLinks: NULL ## colTree: NULL We can note that it worked as planned since there were 5 clusters and there are now 5 rows. 6.4 Data transformation Data transformations are common in microbiome analysis. Examples include the logarithmic transformation, calculation of relative abundances (percentages), and compositionality-aware transformations such as the centered log-ratio transformation (clr). In mia package, transformations are applied to abundance data. The transformed abundance table is stored back to ‘assays’. mia includes transformation function (‘transformAssay()’) which applies sample-wise or column-wise transformation when MARGIN = ‘samples’, feature-wise or row-wise transformation when MARGIN = ‘features’. For a complete list of available transformations and parameters, see function help. tse &lt;- tseGlobalPatterns tse &lt;- transformAssay(tse, assay.type = &quot;counts&quot;, method = &quot;relabundance&quot;, pseudocount = 1) tse &lt;- transformAssay(x = tse, assay.type = &quot;relabundance&quot;, method = &quot;clr&quot;, pseudocount = 1, name = &quot;clr&quot;) head(assay(tse, &quot;clr&quot;)) ## CL3 CC1 SV1 M31Fcsw ## Class:Thermoprotei -5.078e-05 -5.105e-05 -5.055e-05 -4.975e-05 ## Class:Thermoprotei -5.078e-05 -5.105e-05 -5.055e-05 -4.975e-05 ## Species:Sulfolobusacidocaldarius -5.078e-05 -5.105e-05 -5.055e-05 -4.975e-05 ## Class:Sd-NA -5.078e-05 -5.105e-05 -5.055e-05 -4.975e-05 ## Class:Sd-NA -5.078e-05 -5.105e-05 -5.055e-05 -4.975e-05 ## Class:Sd-NA -5.078e-05 -5.105e-05 -5.055e-05 -4.975e-05 ## M11Fcsw M31Plmr M11Plmr F21Plmr ## Class:Thermoprotei -4.947e-05 -4.931e-05 -4.879e-05 -4.671e-05 ## Class:Thermoprotei -4.947e-05 -4.931e-05 -4.879e-05 -4.671e-05 ## Species:Sulfolobusacidocaldarius -4.947e-05 -4.931e-05 -4.658e-05 -4.671e-05 ## Class:Sd-NA -4.947e-05 -4.931e-05 -4.879e-05 -4.671e-05 ## Class:Sd-NA -4.947e-05 -4.931e-05 -4.879e-05 -4.671e-05 ## Class:Sd-NA -4.947e-05 -4.931e-05 -4.879e-05 -4.671e-05 ## M31Tong M11Tong LMEpi24M SLEpi20M ## Class:Thermoprotei -4.846e-05 -4.257e-05 -4.756e-05 -4.837e-05 ## Class:Thermoprotei -4.846e-05 -4.257e-05 -4.756e-05 -4.918e-05 ## Species:Sulfolobusacidocaldarius -4.846e-05 -4.257e-05 -4.756e-05 -4.918e-05 ## Class:Sd-NA -4.846e-05 -4.257e-05 -4.756e-05 -4.918e-05 ## Class:Sd-NA -4.846e-05 -4.257e-05 -4.756e-05 -4.918e-05 ## Class:Sd-NA -4.846e-05 -4.257e-05 -4.756e-05 -4.918e-05 ## AQC1cm AQC4cm AQC7cm NP2 ## Class:Thermoprotei -2.385e-05 -4.438e-06 2.787e-05 -4.731e-05 ## Class:Thermoprotei -4.660e-05 -4.568e-05 -4.428e-05 -4.915e-05 ## Species:Sulfolobusacidocaldarius -4.660e-05 -4.652e-05 -4.777e-05 -4.915e-05 ## Class:Sd-NA -4.660e-05 -3.726e-05 -3.090e-05 -4.915e-05 ## Class:Sd-NA -4.660e-05 -4.568e-05 -4.719e-05 -4.915e-05 ## Class:Sd-NA -4.660e-05 -4.610e-05 -4.603e-05 -4.915e-05 ## NP3 NP5 TRRsed1 TRRsed2 ## Class:Thermoprotei -5.068e-05 -5.083e-05 -3.909e-05 -4.927e-05 ## Class:Thermoprotei -5.068e-05 -5.083e-05 -3.909e-05 -4.927e-05 ## Species:Sulfolobusacidocaldarius -5.068e-05 -5.083e-05 -3.909e-05 -4.927e-05 ## Class:Sd-NA -5.068e-05 -5.083e-05 -3.909e-05 -4.927e-05 ## Class:Sd-NA -5.068e-05 -5.083e-05 -3.909e-05 -4.927e-05 ## Class:Sd-NA -5.068e-05 -5.083e-05 -3.909e-05 -4.927e-05 ## TRRsed3 TS28 TS29 Even1 ## Class:Thermoprotei -4.829e-05 -5.016e-05 -4.934e-05 -5.046e-05 ## Class:Thermoprotei -4.829e-05 -5.016e-05 -4.934e-05 -5.046e-05 ## Species:Sulfolobusacidocaldarius -4.829e-05 -5.016e-05 -4.934e-05 -5.046e-05 ## Class:Sd-NA -4.829e-05 -5.016e-05 -4.934e-05 -5.046e-05 ## Class:Sd-NA -4.829e-05 -5.016e-05 -4.934e-05 -5.046e-05 ## Class:Sd-NA -4.829e-05 -5.016e-05 -4.934e-05 -5.046e-05 ## Even2 Even3 ## Class:Thermoprotei -5.017e-05 -5.034e-05 ## Class:Thermoprotei -5.017e-05 -5.034e-05 ## Species:Sulfolobusacidocaldarius -5.017e-05 -5.034e-05 ## Class:Sd-NA -5.017e-05 -5.034e-05 ## Class:Sd-NA -5.017e-05 -5.034e-05 ## Class:Sd-NA -5.017e-05 -5.034e-05 In ‘pa’ transformation, abundance table is converted to present/absent table. tse &lt;- transformAssay(tse, method = &quot;pa&quot;) head(assay(tse, &quot;pa&quot;)) ## CL3 CC1 SV1 M31Fcsw M11Fcsw M31Plmr M11Plmr ## Class:Thermoprotei 0 0 0 0 0 0 0 ## Class:Thermoprotei 0 0 0 0 0 0 0 ## Species:Sulfolobusacidocaldarius 0 0 0 0 0 0 1 ## Class:Sd-NA 0 0 0 0 0 0 0 ## Class:Sd-NA 0 0 0 0 0 0 0 ## Class:Sd-NA 0 0 0 0 0 0 0 ## F21Plmr M31Tong M11Tong LMEpi24M SLEpi20M ## Class:Thermoprotei 0 0 0 0 1 ## Class:Thermoprotei 0 0 0 0 0 ## Species:Sulfolobusacidocaldarius 0 0 0 0 0 ## Class:Sd-NA 0 0 0 0 0 ## Class:Sd-NA 0 0 0 0 0 ## Class:Sd-NA 0 0 0 0 0 ## AQC1cm AQC4cm AQC7cm NP2 NP3 NP5 TRRsed1 ## Class:Thermoprotei 1 1 1 1 0 0 0 ## Class:Thermoprotei 0 1 1 0 0 0 0 ## Species:Sulfolobusacidocaldarius 0 0 0 0 0 0 0 ## Class:Sd-NA 0 1 1 0 0 0 0 ## Class:Sd-NA 0 1 1 0 0 0 0 ## Class:Sd-NA 0 1 1 0 0 0 0 ## TRRsed2 TRRsed3 TS28 TS29 Even1 Even2 Even3 ## Class:Thermoprotei 0 0 0 0 0 0 0 ## Class:Thermoprotei 0 0 0 0 0 0 0 ## Species:Sulfolobusacidocaldarius 0 0 0 0 0 0 0 ## Class:Sd-NA 0 0 0 0 0 0 0 ## Class:Sd-NA 0 0 0 0 0 0 0 ## Class:Sd-NA 0 0 0 0 0 0 0 # list of abundance tables that assays slot contains assays(tse) ## List of length 4 ## names(4): counts relabundance clr pa "],["community-diversity.html", "Chapter 7 Community Diversity 7.1 Estimation 7.2 Visualization", " Chapter 7 Community Diversity .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } Diversity estimates are a central topic in microbiome data analysis. There are three commonly employed levels of diversity measurements, which are trying to put a number on different aspects of the questions associated with diversity (Whittaker 1960). Many different ways for estimating such diversity measurements have been described in the literature. Which measurement is best or applicable for your samples, is not the aim of the following sections. library(mia) data(&quot;GlobalPatterns&quot;, package=&quot;mia&quot;) tse &lt;- GlobalPatterns Alpha diversity, also sometimes interchangeably used with the term species diversity, summarizes the distribution of species abundances in a given sample into a single number that depends on species richness and evenness. Diversity indices measure the overall community heterogeneity. A number of ecological diversity measures are available. The Hill coefficient combines many standard indices into a single equation that provides observed richness, inverse Simpson, and Shannon diversity, and generalized diversity as special cases. In general, diversity increases together with increasing richness and evenness. Sometimes richness, phylogenetic diversity, evenness, dominance, and rarity are considered to be variants of alpha diversity. Richness refers to the total number of species in a community (sample). The simplest richness index is the number of observed species (observed richness). Assuming limited sampling from the community, however, this may underestimate the true species richness. Several estimators are available, including for instance ACE (A and SM 1992) and Chao1 (A 1984). Richness estimates are unaffected by species abundances. Phylogenetic diversity was first proposed by (Faith 1992). Unlike the diversity measures mentioned above, Phylogenetic diversity (PD) measure incorporates information from phylogenetic relationships stored in phylo tree between species in a community (sample). The Faith’s PD is calculated as the sum of branch length of all species in a community (sample). Evenness focuses on species abundances, and can thus complement the number of species. A typical evenness index is the Pielou’s evenness, which is Shannon diversity normalized by the observed richness. Dominance indices are in general negatively correlated with diversity, and sometimes used in ecological literature. High dominance is obtained when one or few species have a high share of the total species abundance in the community. Rarity indices characterize the concentration of taxa at low abundance. Prevalence and detection thresholds determine rare taxa whose total concentration is represented as a rarity index. 7.1 Estimation Alpha diversity can be estimated with wrapper functions that interact with other packages implementing the calculation, such as vegan (Oksanen et al. 2020). 7.1.1 Richness Richness gives the number of features present within a community and can be calculated with estimateRichness. Each of the estimate diversity/richness/evenness/dominance functions adds the calculated measure(s) to the colData of the SummarizedExperiment under the given column name. Here, we calculate observed features as a measure of richness. tse &lt;- mia::estimateRichness(tse, assay.type = &quot;counts&quot;, index = &quot;observed&quot;, name=&quot;observed&quot;) head(tse$observed) ## CL3 CC1 SV1 M31Fcsw M11Fcsw M31Plmr ## 6964 7679 5729 2667 2574 3214 This allows access to the values to be analyzed directly from the colData, for example by plotting them using plotColData from the scater package (McCarthy et al. 2020). library(scater) plotColData(tse, &quot;observed&quot;, &quot;SampleType&quot;, colour_by = &quot;Final_Barcode&quot;) + theme(axis.text.x = element_text(angle=45,hjust=1)) + ylab(expression(Richness[Observed])) Figure 7.1: Shannon diversity estimates plotted grouped by sample type with colour-labeled barcode. 7.1.2 Diversity The main function, estimateDiversity, calculates the selected diversity index based on the selected assay data. tse &lt;- mia::estimateDiversity(tse, assay.type = &quot;counts&quot;, index = &quot;shannon&quot;, name = &quot;shannon&quot;) head(tse$shannon) ## CL3 CC1 SV1 M31Fcsw M11Fcsw M31Plmr ## 6.577 6.777 6.498 3.828 3.288 4.289 Alpha diversities can be visualized with boxplot. Here, Shannon index is compared between different sample type groups. Individual data points are visualized by plotting them as points with geom_jitter. geom_signif is used to test whether these differences are statistically significant. It adds p-values to plot. library(ggsignif) library(ggplot2) library(patchwork) library(ggsignif) # Subsets the data. Takes only those samples that are from feces, skin, or tongue, # and creates data frame from the collected data df &lt;- as.data.frame(colData(tse)[tse$SampleType %in% c(&quot;Feces&quot;, &quot;Skin&quot;, &quot;Tongue&quot;), ]) # Changes old levels with new levels df$SampleType &lt;- factor(df$SampleType) # For significance testing, all different combinations are determined comb &lt;- split(t(combn(levels(df$SampleType), 2)), seq(nrow(t(combn(levels(df$SampleType), 2))))) ggplot(df, aes(x = SampleType, y = shannon)) + # Outliers are removed, because otherwise each data point would be plotted twice; # as an outlier of boxplot and as a point of dotplot. geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2) + geom_signif(comparisons = comb, map_signif_level = FALSE) + theme(text = element_text(size = 10)) 7.1.3 Faith phylogenetic diversity The Faith index is returned by the function estimateFaith. tse &lt;- mia::estimateFaith(tse, assay.type = &quot;counts&quot;) head(tse$faith) ## [1] 0 0 0 0 0 0 Note: because tse is a TreeSummarizedExperiment object, its phylogenetic tree is used by default. However, the optional argument tree must be provided if tse does not contain one. Below a visual comparison between shannon and faith indices is shown with a violin plot. plots &lt;- lapply(c(&quot;shannon&quot;, &quot;faith&quot;), plotColData, object = tse, colour_by = &quot;SampleType&quot;) plots[[1]] + plots[[2]] + plot_layout(guides = &quot;collect&quot;) Alternatively, the phylogenetic diversity can be calculated by mia::estimateDiversity. This is a faster re-implementation of the widely used function in picante W et al. (2010). Load picante R package and get the phylo stored in rowTree. tse &lt;- mia::estimateDiversity(tse, assay.type = &quot;counts&quot;, index = &quot;faith&quot;, name = &quot;faith&quot;) 7.1.4 Evenness Evenness can be calculated with estimateEvenness. tse &lt;- estimateEvenness(tse, assay.type = &quot;counts&quot;, index=&quot;simpson&quot;) head(tse$simpson) ## [1] 0.026871 0.027197 0.047049 0.005179 0.004304 0.005011 7.1.5 Dominance Dominance can be calculated with estimateDominance. Here, the Relative index is calculated which is the relative abundance of the most dominant species in the sample. tse &lt;- estimateDominance(tse, assay.type = &quot;counts&quot;, index=&quot;relative&quot;) head(tse$relative) ## CL3 CC1 SV1 M31Fcsw M11Fcsw M31Plmr ## 0.03910 0.03226 0.01690 0.22981 0.21778 0.22329 7.1.6 Rarity mia package provides one rarity index called log-modulo skewness. It can be calculated with estimateDiversity. tse &lt;- mia::estimateDiversity(tse, assay.type = &quot;counts&quot;, index = &quot;log_modulo_skewness&quot;) head(tse$log_modulo_skewness) ## [1] 2.061 2.061 2.061 2.061 2.061 2.061 7.1.7 Divergence Divergence can be evaluated with estimateDivergence. Reference and algorithm for the calculation of divergence can be specified as reference and FUN, respectively. tse &lt;- mia::estimateDivergence(tse, assay.type = &quot;counts&quot;, reference = &quot;median&quot;, FUN = vegan::vegdist) 7.2 Visualization A plot comparing all the diversity measures calculated above and stored in colData can then be constructed directly. plots &lt;- lapply(c(&quot;observed&quot;, &quot;shannon&quot;, &quot;simpson&quot;, &quot;relative&quot;, &quot;faith&quot;, &quot;log_modulo_skewness&quot;), plotColData, object = tse, x = &quot;SampleType&quot;, colour_by = &quot;SampleType&quot;) plots &lt;- lapply(plots, &quot;+&quot;, theme(axis.text.x = element_blank(), axis.title.x = element_blank(), axis.ticks.x = element_blank())) ((plots[[1]] | plots[[2]] | plots[[3]]) / (plots[[4]] | plots[[5]] | plots[[6]])) + plot_layout(guides = &quot;collect&quot;) .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } "],["community-similarity.html", "Chapter 8 Community Similarity 8.1 Unsupervised ordination 8.2 Supervised ordination 8.3 Case studies 8.4 Summary", " Chapter 8 Community Similarity Beta diversity quantifies the dissimilarity between communities (multiple samples), as opposed to alpha diversity which focuses on variation within a community (one sample). In microbiome research, commonly used metrics of beta diversity include the Bray-Curtis index (for compositional data), Jaccard index (for presence/absence data, ignoring abundance information), Aitchison distance (Euclidean distance for clr transformed abundances, aiming to avoid the compositionality bias), and the Unifrac distance (that takes into account the phylogenetic tree information). Notably, only some of these measures are actual distances, as this is a mathematical concept whose definition is not satisfied by certain ecological measure, such as the Bray-Curtis index. Therefore, the terms dissimilarity and beta diversity are preferred. Method description Assay type Beta diversity metric Quantitative profiling Absolute counts Bray-Curtis Relative profiling Relative abundances Bray-Curtis Aitchison distance Absolute counts Aitchison Aitchison distance clr Euclidean Robust Aitchison distance rclr Euclidean Presence/Absence similarity Relative abundances Jaccard Presence/Absence similarity Absolute counts Jaccard Phylogenetic distance Rarefied counts Unifrac In practice, beta diversity is usually represented as a dist object, a triangular matrix where the distance between each pair of samples is encoded by a specific cell. This distance matrix can then undergo ordination, which is an important ecological tool to reduce the dimensionality of data for a more efficient analysis and visualization. Ordination techniques aim to capture as much essential information from the data as possible and turn it into a lower dimensional representation. Dimension reduction is bound to lose information but commonly used ordination techniques can preserve relevant information of sample similarities in an optimal way, which is defined in different ways by different methods. Based on the type of algorithm, ordination methods in microbiome research can be generally divided in two categories: unsupervised and supervised ordination. The former includes Principal Coordinate Analysis (PCoA), Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP), whereas the latter is mainly represented by distance-based Redundancy Analysis (dbRDA). We will first discuss unsupervised ordination methods and then proceed to supervised ones. 8.1 Unsupervised ordination Unsupervised ordination methods variation in the data without additional information on covariates or other supervision of the model. Among the different approaches, Multi-Dimensional Scaling (MDS) and non-metric MDS (NMDS) can be regarded as the standard. They are jointly referred to as PCoA. For this demonstration we will analyse beta diversity in GlobalPatterns, and observe the variation between stool samples and those with a different origin. # Load mia and import sample dataset library(mia) data(&quot;GlobalPatterns&quot;, package = &quot;mia&quot;) tse &lt;- GlobalPatterns # Beta diversity metrics like Bray-Curtis are often applied to relabundances tse &lt;- transformAssay(tse, assay.type = &quot;counts&quot;, method = &quot;relabundance&quot;) # Other metrics like Aitchison to clr-transformed data tse &lt;- transformAssay(tse, assay.type = &quot;relabundance&quot;, method = &quot;clr&quot;, pseudocount = TRUE) # Add group information Feces yes/no tse$Group &lt;- tse$SampleType == &quot;Feces&quot; 8.1.1 Comparing communities by beta diversity analysis A typical comparison of community compositions starts with a visual representation of the groups by a 2D ordination. Then we estimate relative abundances and MDS ordination based on Bray-Curtis index between the groups, and visualize the results. In the following examples dissimilarity is calculated with the function supplied to the FUN argument. Several metrics of beta diversity are defined by the vegdist function of the vegan package, which is often used in this context. However, such custom functions created by the user also work, as long as they return a dist object. In either case, this function is then applied to calculate reduced dimensions via an ordination method, the results of which can be stored in the reducedDim slot of the TreeSE. This entire process is contained by the runMDS and runNMDS functions. # Load package to plot reducedDim library(scater) # Run PCoA on relabundance assay with Bray-Curtis distances tse &lt;- runMDS(tse, FUN = vegan::vegdist, method = &quot;bray&quot;, assay.type = &quot;relabundance&quot;, name = &quot;MDS_bray&quot;) Sample dissimilarity can be visualized on a lower-dimensional display (typically 2D) using the plotReducedDim function from the scater package. This also provides tools to incorporate additional information encoded by color, shape, size and other aesthetics. Can you find any difference between the groups? # Create ggplot object p &lt;- plotReducedDim(tse, &quot;MDS_bray&quot;, colour_by = &quot;Group&quot;) # Calculate explained variance e &lt;- attr(reducedDim(tse, &quot;MDS_bray&quot;), &quot;eig&quot;) rel_eig &lt;- e / sum(e[e &gt; 0]) # Add explained variance for each axis p &lt;- p + labs(x = paste(&quot;PCoA 1 (&quot;, round(100 * rel_eig[[1]], 1), &quot;%&quot;, &quot;)&quot;, sep = &quot;&quot;), y = paste(&quot;PCoA 2 (&quot;, round(100 * rel_eig[[2]], 1), &quot;%&quot;, &quot;)&quot;, sep = &quot;&quot;)) p Figure 8.1: MDS plot based on the Bray-Curtis distances on the GlobalPattern dataset. A few combinations of beta diversity metrics and assay types are typically used. For instance, Bray-Curtis dissimilarity and Euclidean distance are often applied to the relative abundance and the clr assays, respectively. Besides beta diversity metric and assay type, the PCoA algorithm is also a variable that should be considered. Below, we show how the choice of these three factors can affect the resulting lower-dimensional data. # Run NMDS on relabundance assay with Bray-Curtis distances tse &lt;- runNMDS(tse, FUN = vegan::vegdist, method = &quot;bray&quot;, assay.type = &quot;relabundance&quot;, name = &quot;NMDS_bray&quot;) # Run MDS on clr assay with Aitchison distances tse &lt;- runMDS(tse, FUN = vegan::vegdist, method = &quot;euclidean&quot;, assay.type = &quot;clr&quot;, name = &quot;MDS_aitchison&quot;) # Run NMDS on clr assay with Euclidean distances tse &lt;- runNMDS(tse, FUN = vegan::vegdist, method = &quot;euclidean&quot;, assay.type = &quot;clr&quot;, name = &quot;NMDS_aitchison&quot;) Multiple ordination plots are combined into a multi-panel plot with the patchwork package, so that different methods can be compared to find similarities between them or select the most suitable one to visualize beta diversity in the light of the research question. # Load package for multi-panel plotting library(patchwork) # Generate plots for all 4 reducedDims plots &lt;- lapply(c(&quot;MDS_bray&quot;, &quot;MDS_aitchison&quot;, &quot;NMDS_bray&quot;, &quot;NMDS_aitchison&quot;), plotReducedDim, object = tse, colour_by = &quot;Group&quot;) # Generate multi-panel plot wrap_plots(plots) + plot_layout(guides = &quot;collect&quot;) Figure 8.2: Comparison of MDS and NMDS plots based on the Bray-Curtis or Aitchison distances on the GlobalPattern dataset. The Unifrac method is a special case, as it requires data on the relationship of features in form on a phylo tree. calculateUnifrac performs the calculation to return a dist object, which can again be used within runMDS. tse &lt;- runMDS(tse, FUN = mia::calculateUnifrac, name = &quot;Unifrac&quot;, tree = rowTree(tse), ntop = nrow(tse), assay.type = &quot;counts&quot;) plotReducedDim(tse, &quot;Unifrac&quot;, colour_by = &quot;Group&quot;) Figure 8.3: Unifrac distances scaled by MDS of the GlobalPattern dataset. 8.1.2 Other ordination methods Other dimension reduction methods, such as PCA and UMAP, are inherited from the scater package. tse &lt;- runPCA(tse, name = &quot;PCA&quot;, assay.type = &quot;counts&quot;, ncomponents = 10) plotReducedDim(tse, &quot;PCA&quot;, colour_by = &quot;Group&quot;) Figure 8.4: PCA plot on the GlobalPatterns data set containing sample from different sources. As mentioned before, applicability of the different methods depends on your sample set and research question. tse &lt;- runUMAP(tse, name = &quot;UMAP&quot;, assay.type = &quot;counts&quot;, ncomponents = 3) plotReducedDim(tse, &quot;UMAP&quot;, colour_by = &quot;Group&quot;, ncomponents = c(1:3)) Figure 8.5: UMAP plot on the GlobalPatterns data set containing sample from different sources. 8.1.3 Explained variance The percentage of explained variance is typically shown for PCA ordination plots. This quantifies the proportion of overall variance in the data that is captured by the PCA axes, or how well the ordination axes reflect the original distances. Sometimes a similar measure is shown for MDS/PCoA. The interpretation is generally different, however, and hence we do not recommend using it. PCA is a special case of PCoA with Euclidean distances. With non-Euclidean dissimilarities PCoA uses a trick where the pointwise dissimilarities are first cast into similarities in a Euclidean space (with some information loss i.e. stress) and then projected to the maximal variance axes. In this case, the maximal variance axes do not directly reflect the correspondence of the projected distances and original distances, as they do for PCA. In typical use cases, we would like to know how well the ordination reflects the original similarity structures; then the quantity of interest is the so-called “stress” function, which measures the difference in pairwise similarities between the data points in the original (high-dimensional) vs. projected (low-dimensional) space. Hence, we propose that for PCoA and other ordination methods, users would report relative stress, which varies in the unit interval and is better if smaller. This can be calculated as shown below. # Load vegan package library(vegan) # Quantify dissimilarities in the original feature space x &lt;- assay(tse, &quot;relabundance&quot;) # Pick relabunance assay separately d0 &lt;- as.matrix(vegdist(t(x), &quot;bray&quot;)) # PCoA Ordination pcoa &lt;- as.data.frame(cmdscale(d0, k = 2)) names(pcoa) &lt;- c(&quot;PCoA1&quot;, &quot;PCoA2&quot;) # Quantify dissimilarities in the ordination space dp &lt;- as.matrix(dist(pcoa)) # Calculate stress i.e. relative difference in the original and # projected dissimilarities stress &lt;- sum((dp - d0)^2) / sum(d0^2) A Shepard plot visualizes the original versus the ordinated dissimilarity between the observations. ord &lt;- order(as.vector(d0)) df &lt;- data.frame(d0 = as.vector(d0)[ord], dmds = as.vector(dp)[ord]) ggplot(df, aes(x = d0, y = dmds)) + geom_smooth() + geom_point() + labs(title = &quot;Shepard plot&quot;, x = &quot;Original distance&quot;, y = &quot;MDS distance&quot;, subtitle = paste(&quot;Stress:&quot;, round(stress, 2))) + theme_bw() 8.2 Supervised ordination dbRDA is a supervised counterpart of PCoA, that is, it takes into account the covariates specified by the user to maximize the variance with respect to the them. The result shows how much each covariate affects beta diversity. The table below illustrates the relation between supervised and unsupervised ordination methods. supervised ordination unsupervised ordination Euclidean distance RDA PCA non-Euclidean distance dbRDA PCoA/MDS, NMDS and UMAP We demonstrate the usage of dbRDA with the enterotype dataset, where samples correspond to patients. The colData contains the clinical status of each patient and a few covariates such as their gender and age. # Load data data(&quot;enterotype&quot;, package = &quot;mia&quot;) tse2 &lt;- enterotype # Apply relative transform tse2 &lt;- transformAssay(tse2, method = &quot;relabundance&quot;) dbRDA can be perfomed with the runRDA function. In addition to the arguments previously defined for unsupervised ordination, this function takes a formula to control for variables and an action to treat missing values. Along with clinical status, which is the main outcome, we control for gender and age, and exclude observations where one of these variables is missing. # Perform RDA tse2 &lt;- runRDA(tse2, assay.type = &quot;relabundance&quot;, formula = assay ~ ClinicalStatus + Gender + Age, distance = &quot;bray&quot;, na.action = na.exclude) # Store results of PERMANOVA test rda_info &lt;- attr(reducedDim(tse2, &quot;RDA&quot;), &quot;significance&quot;) The importance of each variable on the similarity between samples can be assessed from the results of PERMANOVA, automatically provided by the runRDA function. We see that both clinical status and age explain more than 10% of the variance, but only age shows statistical significance. rda_info$permanova |&gt; knitr::kable() Df SumOfSqs F Pr(&gt;F) Total variance Explained variance Model 6 1.1157 1.940 0.030 3.991 0.2795 ClinicalStatus 4 0.5837 1.522 0.128 3.991 0.1463 Gender 1 0.1679 1.751 0.101 3.991 0.0421 Age 1 0.5245 5.471 0.001 3.991 0.1314 Residual 30 2.8757 NA NA 3.991 0.7205 To ensure that the homogeneity assumption holds, we retrieve the corresponding information from the results of RDA. In this case, none of the p-values is lower than the significance threshold, and thus homogeneity is observed. rda_info$homogeneity |&gt; knitr::kable() Df Sum Sq Mean Sq F N.Perm Pr(&gt;F) Total variance Explained variance ClinicalStatus 4 0.2511 0.0628 2.7440 999 0.113 1.0288 0.2440 Gender 1 0.0103 0.0103 0.4158 999 0.525 0.9283 0.0111 Age 29 0.3272 0.0113 17.0255 999 0.415 0.3319 0.9860 Next, we proceed to visualize the weight and significance of each variable on the similarity between samples with an RDA plot, which can be generated with the plotRDA function from the miaViz package. # Load packages for plotting function library(miaViz) # Generate RDA plot coloured by clinical status plotRDA(tse2, &quot;RDA&quot;, colour_by = &quot;ClinicalStatus&quot;) From the plot above, we can see that only age significantly describes differences between the microbial profiles of different samples. Such visual approach complements the previous results of PERMANOVA. 8.3 Case studies 8.3.0.1 Visualizing the most dominant genus on PCoA In this section, we visualize the most dominant genus on PCoA. A similar visualization was proposed by (2021). First, we agglomerate the data at the Genus level and get the dominant taxa per sample. # Agglomerate to genus level tse_genus &lt;- mergeFeaturesByRank(tse, rank = &quot;Genus&quot;) # Convert to relative abundances tse_genus &lt;- transformAssay(tse, method = &quot;relabundance&quot;, assay.type = &quot;counts&quot;) # Add info on dominant genus per sample tse_genus &lt;- addPerSampleDominantFeatures(tse_genus, assay.type = &quot;relabundance&quot;, name = &quot;dominant_taxa&quot;) # Overview countDominantFeatures(tse_genus, rank = &quot;Genus&quot;, digits = 3, name = &quot;dominant_taxa&quot;) ## # A tibble: 17 × 3 ## dominant_taxa n rel_freq ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Genus:Bacteroides 5 0.192 ## 2 Order:Stramenopiles 4 0.154 ## 3 Family:Desulfobulbaceae 2 0.077 ## 4 Genus:Streptococcus 2 0.077 ## 5 Class:Chloracidobacteria 1 0.038 ## 6 Family:ACK-M1 1 0.038 ## 7 Family:Flavobacteriaceae 1 0.038 ## 8 Family:Moraxellaceae 1 0.038 ## 9 Family:Ruminococcaceae 1 0.038 ## 10 Genus:CandidatusSolibacter 1 0.038 ## 11 Genus:Dolichospermum 1 0.038 ## 12 Genus:Faecalibacterium 1 0.038 ## 13 Genus:MC18 1 0.038 ## 14 Genus:Neisseria 1 0.038 ## 15 Genus:Prochlorococcus 1 0.038 ## 16 Genus:Veillonella 1 0.038 ## 17 Order:Chromatiales 1 0.038 Next, we perform PCoA with Bray-Curtis dissimilarity. tse_genus &lt;- runMDS(tse_genus, FUN = vegan::vegdist, name = &quot;PCoA_BC&quot;, assay.type = &quot;relabundance&quot;) Finally, we get the top taxa and and visualize their abundances on PCoA. Note that A 3D interactive version of the plot below can be found in 20. # Getting the top taxa top_taxa &lt;- getTopFeatures(tse_genus, top = 6, assay.type = &quot;relabundance&quot;) # Naming all the rest of non top-taxa as &quot;Other&quot; most_abundant &lt;- lapply(colData(tse_genus)$dominant_taxa, function(x) {if (x %in% top_taxa) {x} else {&quot;Other&quot;}}) # Storing the previous results as a new column within colData colData(tse_genus)$most_abundant &lt;- as.character(most_abundant) # Calculating percentage of the most abundant most_abundant_freq &lt;- table(as.character(most_abundant)) most_abundant_percent &lt;- round(most_abundant_freq / sum(most_abundant_freq) * 100, 1) # Retrieving the explained variance e &lt;- attr(reducedDim(tse_genus, &quot;PCoA_BC&quot;), &quot;eig&quot;) var_explained &lt;- e / sum(e[e &gt; 0]) * 100 # Define colors for visualization my_colors &lt;- c(&quot;black&quot;, &quot;blue&quot;, &quot;lightblue&quot;, &quot;darkgray&quot;, &quot;magenta&quot;, &quot;darkgreen&quot;, &quot;red&quot;) # Visualization plot &lt;-plotReducedDim(tse_genus, &quot;PCoA_BC&quot;, colour_by = &quot;most_abundant&quot;) + scale_colour_manual(values = my_colors, labels = paste0(names(most_abundant_percent), &quot;(&quot;, most_abundant_percent, &quot;%)&quot;)) + labs(x = paste(&quot;PC 1 (&quot;, round(var_explained[1], 1), &quot;%)&quot;), y = paste(&quot;PC 2 (&quot;, round(var_explained[2], 1), &quot;%)&quot;), color = &quot;&quot;) plot Similarly, we visualize and compare the sub-population. # Calculating the frequencies and percentages for both categories freq_TRUE &lt;- table(as.character(most_abundant[colData(tse_genus)$Group == TRUE])) freq_FALSE &lt;- table(as.character(most_abundant[colData(tse_genus)$Group == FALSE])) percent_TRUE &lt;- round(freq_TRUE / sum(freq_TRUE) * 100, 1) percent_FALSE &lt;- round(freq_FALSE / sum(freq_FALSE) * 100, 1) # Visualization plotReducedDim(tse_genus[ , colData(tse_genus)$Group == TRUE], &quot;PCoA_BC&quot;, colour_by = &quot;most_abundant&quot;) + scale_colour_manual(values = my_colors, labels = paste0(names(percent_TRUE), &quot;(&quot;, percent_TRUE, &quot;%)&quot;)) + labs(x = paste(&quot;PC 1 (&quot;, round(var_explained[1], 1), &quot;%)&quot;), y = paste(&quot;PC 2 (&quot;, round(var_explained[2], 1), &quot;%)&quot;), title = &quot;Group = TRUE&quot;, color = &quot;&quot;) plotReducedDim(tse_genus[ , colData(tse_genus)$Group == FALSE], &quot;PCoA_BC&quot;, colour_by = &quot;most_abundant&quot;) + scale_colour_manual(values = my_colors, labels = paste0(names(percent_FALSE), &quot;(&quot;, percent_FALSE, &quot;%)&quot;)) + labs(x = paste(&quot;PC 1 (&quot;, round(var_explained[1], 1), &quot;%)&quot;), y = paste(&quot;PC 2 (&quot;, round(var_explained[2], 1), &quot;%)&quot;), title = &quot;Group = FALSE&quot;, color = &quot;&quot;) 8.3.1 Testing differences in community composition between sample groups Permutational Analysis of Variance (PERMANOVA; (2001)) is a widely used non-parametric multivariate method that aims to estimate the actual statistical significance of differences in the observed community composition between two groups of samples. PERMANOVA tests the hypothesis that the centroids and dispersion of the community are equivalent between the compared groups. A p-value smaller than the significance threshold indicates that the groups have a different community composition. This method is implemented with the adonis2 function from the vegan package. By default, the argument by is set to \"terms\", in which the order of variables in the formula matters. In this case, each variable is analyzed sequentially, and the result is different when more than 1 variable is introduced and their order differs. Therefore, it is recommended to set by = \"margin\", which specifies that the marginal effect of each variable is analyzed individually. You can view a comparison between the two designs in chapter 20.2. We can perform PERMANOVA either with adonis2 function or by first performing dbRDA and then applying permutational test its results. An advantage of the latter approach is that by doing so we can get coefficients: how much each taxa affects the variation between communities. # Agglomerate data to Species level tse &lt;- mergeFeaturesByRank(tse, rank = &quot;Species&quot;) # Set seed for reproducibility set.seed(1576) # We choose 99 random permutations. Consider applying more (999 or 9999) in your # analysis. permanova &lt;- adonis2(t(assay(tse, &quot;relabundance&quot;)) ~ Group, by = &quot;margin&quot;, # each term (here only &#39;Group&#39;) analyzed individually data = colData(tse), method = &quot;euclidean&quot;, permutations = 99) # Set seed for reproducibility set.seed(1576) # Perform dbRDA dbrda &lt;- dbrda(t(assay(tse,&quot;relabundance&quot;)) ~ Group, data = colData(tse)) # Perform permutational analysis permanova2 &lt;- anova.cca(dbrda, by = &quot;margin&quot;, # each term (here only &#39;Group&#39;) analyzed individually method = &quot;euclidean&quot;, permutations = 99) # Get p-values p_values &lt;- c(permanova[&quot;Group&quot;, &quot;Pr(&gt;F)&quot;], permanova2[&quot;Group&quot;, &quot;Pr(&gt;F)&quot;]) p_values &lt;-as.data.frame(p_values) rownames(p_values) &lt;- c(&quot;adonis2&quot;, &quot;dbRDA+anova.cca&quot;) p_values ## p_values ## adonis2 0.02 ## dbRDA+anova.cca 0.02 As we can see, the community composition is significantly different between the groups (p &lt; 0.05), and these two methods give equal p-values. Let us visualize the model coefficients for species that exhibit the largest differences between the groups. This gives some insights into how the groups tend to differ from each other in terms of community composition. # Add taxa info sppscores(dbrda) &lt;- t(assay(tse, &quot;relabundance&quot;)) # Get coefficients coef &lt;- dbrda$CCA$v # Get the taxa with biggest weights top.coef &lt;- head(coef[rev(order(abs(coef))), , drop = FALSE], 20) # Sort weights in increasing order top.coef &lt;- top.coef[order(top.coef), ] # Get top names top_names &lt;- names(top.coef)[order(abs(top.coef), decreasing = TRUE)] df &lt;- data.frame(x = top.coef, y = factor(names(top.coef), unique(names(top.coef)))) ggplot(df, aes(x = x, y = y)) + geom_bar(stat = &quot;identity&quot;) + labs(x = &quot;&quot;, y= &quot;&quot;, title = &quot;Top Taxa&quot;) + theme_bw() In the example above, the largest differences between the two groups can be attributed to Genus:Bacteroides (elevated in the first group) and Family:Ruminococcaceae (elevated in the second group), and many other co-varying species. 8.3.2 Checking the homogeneity condition It is important to note that the application of PERMANOVA assumes homogeneous group dispersions (variances). This can be tested with the PERMDISP2 method (Anderson 2006) by using the same assay and distance method than in PERMANOVA. anova(betadisper(vegdist(t(assay(tse, &quot;counts&quot;))), colData(tse)$Group)) ## Analysis of Variance Table ## ## Response: Distances ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Groups 1 0.2385 0.2385 103 3.6e-10 *** ## Residuals 24 0.0554 0.0023 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If the groups have similar dispersion, PERMANOVA can be seen as an appropriate choice for comparing community compositions. 8.4 Summary As a final note, we provide a comprehensive list of functions for the evaluation of dissimilarity indices available in the mia and scater packages. The calculate methods return a reducedDim object as an output, whereas the run methods store the reducedDim object into the specified TreeSE. Canonical Correspondence Analysis (CCA): calculateCCA and runCCA dbRDA: calculateRDA and runRDA Double Principal Coordinate Analysis (DPCoA): calculateDPCoA and runDPCoA Jensen-Shannon Divergence (JSD): calculateJSD and runJSD MDS: calculateMDS and runMDS NMDS: calculateNMDS and runNMDS Overlap: calculateOverlap and runOverlap t-distributed Stochastic Neighbor Embedding (t-SNE): calculateTSNE and runTSNE UMAP: calculateUMAP and runUMAP For more information on clustering samples by beta diversity, you can refer to: How to extract information from clusters Chapter 10 on community typing "],["microbiome-community.html", "Chapter 9 Community Composition 9.1 Visualizing taxonomic composition", " Chapter 9 Community Composition .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } library(mia) data(&quot;GlobalPatterns&quot;, package=&quot;mia&quot;) tse &lt;- GlobalPatterns 9.1 Visualizing taxonomic composition 9.1.1 Composition barplot A typical way to visualize microbiome composition is by using composition barplot. In the following, relative abundance is calculated and top taxa are retrieved for the Phylum rank. Thereafter, the barplot is visualized ordering rank by abundance values and samples by “Bacteroidetes”: library(miaViz) # Computing relative abundance tse &lt;- transformAssay(tse, assay.type = &quot;counts&quot;, method = &quot;relabundance&quot;) # Getting top taxa on a Phylum level tse_phylum &lt;- mergeFeaturesByRank(tse, rank =&quot;Phylum&quot;, onRankOnly=TRUE) top_taxa &lt;- getTopFeatures(tse_phylum,top = 5, assay.type = &quot;relabundance&quot;) # Renaming the &quot;Phylum&quot; rank to keep only top taxa and the rest to &quot;Other&quot; phylum_renamed &lt;- lapply(rowData(tse)$Phylum, function(x){if (x %in% top_taxa) {x} else {&quot;Other&quot;}}) rowData(tse)$Phylum &lt;- as.character(phylum_renamed) # Visualizing the composition barplot, with samples order by &quot;Bacteroidetes&quot; plotAbundance(tse, assay.type=&quot;relabundance&quot;, rank = &quot;Phylum&quot;, order_rank_by=&quot;abund&quot;, order_sample_by = &quot;Bacteroidetes&quot;) 9.1.2 Composition heatmap Community composition can be visualized with heatmap, where the horizontal axis represents samples and the vertical axis the taxa. Color of each intersection point represents abundance of a taxon in a specific sample. Here, abundances are first CLR (centered log-ratio) transformed to remove compositionality bias. Then Z transformation is applied to CLR-transformed data. This shifts all taxa to zero mean and unit variance, allowing visual comparison between taxa that have different absolute abundance levels. After these rough visual exploration techniques, we can visualize the abundances at Phylum level. library(ggplot2) # Add clr-transformation on samples tse_phylum &lt;- transformAssay(tse_phylum, assay.type = &quot;counts&quot;, method = &quot;relabundance&quot;, pseudocount = 1) tse_phylum &lt;- transformAssay(tse_phylum, assay.type = &quot;relabundance&quot;, method = &quot;clr&quot;, pseudocount = 1) # Add z-transformation on features (taxa) tse_phylum &lt;- transformAssay(tse_phylum, assay.type = &quot;clr&quot;, MARGIN = &quot;features&quot;, method = &quot;z&quot;, name = &quot;clr_z&quot;) Visualize as heatmap. # Melt the assay for plotting purposes df &lt;- meltAssay(tse_phylum, assay.type = &quot;clr_z&quot;) # Determines the scaling of colours maxval &lt;- round(max(abs(df$clr_z))) limits &lt;- c(-maxval, maxval) breaks &lt;- seq(from = min(limits), to = max(limits), by = 0.5) colours &lt;- c(&quot;darkblue&quot;, &quot;blue&quot;, &quot;white&quot;, &quot;red&quot;, &quot;darkred&quot;) # Creates a ggplot object ggplot(df, aes(x = SampleID, y = FeatureID, fill = clr_z)) + geom_tile() + scale_fill_gradientn(name = &quot;CLR + Z transform&quot;, breaks = breaks, limits = limits, colours = colours) + theme(text = element_text(size=10), axis.text.x = element_text(angle=45, hjust=1), legend.key.size = unit(1, &quot;cm&quot;)) + labs(x = &quot;Samples&quot;, y = &quot;Taxa&quot;) pheatmap is a package that provides methods to plot clustered heatmaps. library(pheatmap) # Takes subset: only samples from feces, skin, or tongue tse_phylum_subset &lt;- tse_phylum[ , tse_phylum$SampleType %in% c(&quot;Feces&quot;, &quot;Skin&quot;, &quot;Tongue&quot;) ] # Add clr-transformation tse_phylum_subset &lt;- transformAssay(tse_phylum_subset, method = &quot;clr&quot;, pseudocount = 1) tse_phylum_subset &lt;- transformAssay(tse_phylum_subset, assay.type = &quot;clr&quot;, MARGIN = &quot;features&quot;, method = &quot;z&quot;, name = &quot;clr_z&quot;) # Get n most abundant taxa, and subsets the data by them top_taxa &lt;- getTopFeatures(tse_phylum_subset, top = 20) tse_phylum_subset &lt;- tse_phylum_subset[top_taxa, ] # Gets the assay table mat &lt;- assay(tse_phylum_subset, &quot;clr_z&quot;) # Creates the heatmap pheatmap(mat) We can create clusters by hierarchical clustering and add them to the plot. library(ape) # Hierarchical clustering taxa_hclust &lt;- hclust(dist(mat), method = &quot;complete&quot;) # Creates a phylogenetic tree taxa_tree &lt;- as.phylo(taxa_hclust) library(ggtree) # Plot taxa tree taxa_tree &lt;- ggtree(taxa_tree) + theme(plot.margin=margin(0,0,0,0)) # removes margins # Get order of taxa in plot taxa_ordered &lt;- get_taxa_name(taxa_tree) taxa_tree Based on phylo tree, we decide to create three clusters. # Creates clusters taxa_clusters &lt;- cutree(tree = taxa_hclust, k = 3) # Converts into data frame taxa_clusters &lt;- data.frame(clusters = taxa_clusters) taxa_clusters$clusters &lt;- factor(taxa_clusters$clusters) # Order data so that it&#39;s same as in phylo tree taxa_clusters &lt;- taxa_clusters[taxa_ordered, , drop = FALSE] # Prints taxa and their clusters taxa_clusters ## clusters ## Chloroflexi 3 ## Actinobacteria 3 ## Crenarchaeota 3 ## Planctomycetes 3 ## Gemmatimonadetes 3 ## Thermi 3 ## Acidobacteria 3 ## Spirochaetes 2 ## Fusobacteria 2 ## SR1 2 ## Cyanobacteria 2 ## Proteobacteria 2 ## Synergistetes 2 ## Lentisphaerae 1 ## Bacteroidetes 1 ## Verrucomicrobia 1 ## Tenericutes 1 ## Firmicutes 1 ## Euryarchaeota 1 ## SAR406 1 # Adds information to rowData rowData(tse_phylum_subset)$clusters &lt;- taxa_clusters[order(match(rownames(taxa_clusters), rownames(tse_phylum_subset))), ] # Prints taxa and their clusters rowData(tse_phylum_subset)$clusters ## [1] 1 1 2 3 2 2 1 1 1 1 3 2 3 3 3 2 2 3 3 1 ## Levels: 1 2 3 # Hierarchical clustering sample_hclust &lt;- hclust(dist(t(mat)), method = &quot;complete&quot;) # Creates a phylogenetic tree sample_tree &lt;- as.phylo(sample_hclust) # Plot sample tree sample_tree &lt;- ggtree(sample_tree) + layout_dendrogram() + theme(plot.margin=margin(0,0,0,0)) # removes margins # Get order of samples in plot samples_ordered &lt;- rev(get_taxa_name(sample_tree)) sample_tree # Creates clusters sample_clusters &lt;- factor(cutree(tree = sample_hclust, k = 3)) # Converts into data frame sample_data &lt;- data.frame(clusters = sample_clusters) # Order data so that it&#39;s same as in phylo tree sample_data &lt;- sample_data[samples_ordered, , drop = FALSE] # Order data based on tse_phylum_subset &lt;- tse_phylum_subset[ , rownames(sample_data)] # Add sample type data sample_data$sample_types &lt;- unfactor(colData(tse_phylum_subset)$SampleType) sample_data ## clusters sample_types ## M11Plmr 2 Skin ## M31Plmr 2 Skin ## F21Plmr 2 Skin ## M31Fcsw 1 Feces ## M11Fcsw 1 Feces ## TS28 3 Feces ## TS29 3 Feces ## M31Tong 3 Tongue ## M11Tong 3 Tongue Now we can create heatmap with additional annotations. # Determines the scaling of colorss # Scale colors breaks &lt;- seq(-ceiling(max(abs(mat))), ceiling(max(abs(mat))), length.out = ifelse( max(abs(mat))&gt;5, 2*ceiling(max(abs(mat))), 10 ) ) colors &lt;- colorRampPalette(c(&quot;darkblue&quot;, &quot;blue&quot;, &quot;white&quot;, &quot;red&quot;, &quot;darkred&quot;))(length(breaks)-1) pheatmap(mat, annotation_row = taxa_clusters, annotation_col = sample_data, breaks = breaks, color = colors) In addition, there are also other packages that provide functions for more complex heatmaps, such as iheatmapr and ComplexHeatmap (Gu 2022). sechm package provides wrapper for ComplexHeatmap and its usage is explained in chapter 16 along with the pheatmap package for clustered heatmaps. "],["clustering.html", "Chapter 10 Community Typing (Clustering) 10.1 Custom tools 10.2 Hierarchical clustering 10.3 Dirichlet Multinomial Mixtures (DMM) 10.4 Biclustering 10.5 Additional Community Typing", " Chapter 10 Community Typing (Clustering) .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } Clustering techniques are unsupervised machine learning that aims to find groups, called clusters, that share a pattern in the data. In the microbiome context, clustering techniques are included in microbiome community typing methods. For example, clustering allow samples to be distinguished from each other based on their microbiome community composition. There are multiple clustering algorithms available. The data can be clustered either based on features or samples. Hence, depending on the analysis goal the data might require transformation. The examples below are focused on sample clustering. To learn about feature clustering, check out chapter 6.3.1. 10.1 Custom tools bluster is a Bioconductor package providing tools for clustering data in in the SummarizedExperiment container. It offers multiple algorithms such as hierarchical clustering, DBSCAN, and K-means. # Load dependencies library(bluster) library(kableExtra) In the first examples of microbiome community typing we use enterotype data. library(mia) data(&quot;enterotype&quot;, package = &quot;mia&quot;) tse &lt;- enterotype # Apply transformation tse &lt;- transformAssay(tse, method = &quot;relabundance&quot;) The main focus in this example is to show how to use mia’s cluster function to cluster enterotype data. cluster function allows to choose a clustering algorithm and offers multiple parameters to shape the result. In this example, HclustParam() parameter is chosen for hierarchical clustering. HclustParam() parameter itself has parameters on its own HclustParam documentation. A parameter is MARGIN defines whether to cluster features or samples . # Simple use of the hierarchical clustering. Here, the default parameters # Set the cut height to half of the dendrogram height # Save as an alternative experiment that contains clustering information altExp(tse, &quot;hclust&quot;) &lt;- cluster(tse, assay.type = &quot;relabundance&quot;, MARGIN = &quot;samples&quot;, HclustParam()) # The result can be found in &#39;clusters&#39; column of colData # The number of samples included in each cluster summary(colData(altExp(tse, &quot;hclust&quot;))$clusters) ## 1 2 3 4 5 6 7 8 9 ## 92 25 21 2 55 41 20 23 1 Once the clustering on the samples is done, we can also plot the clusters. library(scater) # Add the MDS dimensions for plotting altExp(tse, &quot;hclust&quot;) &lt;- runMDS(altExp(tse, &quot;hclust&quot;), assay.type = &quot;relabundance&quot;, FUN = vegan::vegdist, method = &quot;bray&quot;) # Plot the clusters plotReducedDim(altExp(tse, &quot;hclust&quot;), &quot;MDS&quot;, colour_by = &quot;clusters&quot;) 10.2 Hierarchical clustering The hierarchical clustering algorithm aims to find hierarchy between samples/features. There are to approaches: agglomerative (“bottom-up”) and divisive (“top-down”). In agglomerative approach, each observation is first in a unique cluster. The algorithm continues by agglomerating similar clusters. The divisive approach, instead, starts with one cluster that contains all observations. Clusters are split recursively into clusters that differ the most. The clustering ends when each cluster contains only one observation. In this algorithm, the similarity of two clusters is based on the distance between them. Hierarchical clustering can be visualized with a dendrogram tree. In each splitting point, the tree is divided into two clusters leading to the hierarchy. Hierarchical clustering requires two steps. 1. Computation of the dissimilarities with a given distance. 2. Clustering based on dissimilarities. Additionally, since sequencing data is compositional, we apply relative transformation (as seen in the previous example). In this example, we want to add information on the clustering. To do so, we use the full parameter. We also compute the dissimilarities with the bray distance. Finally, the clust.col parameter allows us to choose the name of the column in the colData (default name is clusters). library(vegan) # Save another alternative experiment that contains full clustering information altExp(tse, &quot;hclust_full&quot;) &lt;- cluster(tse, assay.type = &quot;relabundance&quot;, MARGIN = &quot;samples&quot;, HclustParam(method = &quot;complete&quot;, dist.fun = vegdist, metric = &quot;bray&quot;), full = TRUE, clust.col = &quot;Hclust&quot;) We plot the dendrogram, which is possible since we got the additional information from the clustering. library(dendextend) # Get hclust data from metadata hclust_data &lt;- metadata(altExp(tse, &quot;hclust_full&quot;))$clusters$hclust # Get the dendrogram object dendro &lt;- as.dendrogram(hclust_data) # Plot dendrogram dendro %&gt;% set(&quot;labels&quot;, NULL) %&gt;% plot() In our case, we cut the dendrogram in half by default. To know how many clusters we have, we can check the colData. # Get the clusters summary(colData(altExp(tse, &quot;hclust_full&quot;))$Hclust) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## 3 65 35 12 9 2 23 23 27 15 4 11 6 11 2 1 9 7 4 4 1 2 1 1 1 1 We can see that there are 26 clusters, but that probably is not optimal since the the number of clusters has been chosen arbitrarily. To determine the number of clusters, we can use the dendrogram. Usually the tree is split where the branch length is the largest. However, as we can see from the dendrogram, clusters are not clear. There are algorithms to identify the optimal number of clusters. The NbClust library is useful to that end as it offers multiple methods to determine the optimal number of clusters. Here we will use the silhouette analysis to determine the optimal number of clusters. For each data point, this analysis measures the distance to other data points in the same cluster (cohesion), and the distance to the other clusters (separation), establishing a score. That score is then combined across the data points. NbClust does this for multiple number of clusters and the best score corresponds to the optimal number of clusters. library(NbClust) diss &lt;- metadata(altExp(tse, &quot;hclust_full&quot;))$clusters$dist # Apply the silhouette analysis on the distance matrix res &lt;- NbClust(diss = diss, distance = NULL, method = &quot;ward.D2&quot;, index = &quot;silhouette&quot;) ## ## Only frey, mcclain, cindex, sihouette and dunn can be computed. To compute the other indices, data matrix is needed res$Best.nc ## Number_clusters Value_Index ## 2.0000 0.4783 Based on the result, let’s divide observations into 2 clusters. library(dendextend) # Get optimal number of clusters k &lt;- res$Best.nc[1] # Making colors for 2 clusters col_val_map &lt;- randomcoloR::distinctColorPalette(k) %&gt;% as.list() %&gt;% setNames(paste0(&quot;clust_&quot;, seq(k))) dend &lt;- color_branches(dendro, k = k, col = unlist(col_val_map)) labels(dend) &lt;- NULL plot(dend) 10.3 Dirichlet Multinomial Mixtures (DMM) This section focus on Dirichlet-Multinomial Mixture Model analysis. It is a probabilistic technique that allows to search for sample patterns that reflect sample similarity in the data. DMM has a property of determining an optimal number of clusters (k) to obtain the best model. The minimum value of Laplace approximation to the negative log model evidence for DMM models as a function of k, determines an optimal k. The optimal k suggests to fit a model with k mixtures of Dirichlet distributions. For the best model, k probabilities for each sample to belong to each cluster are obtained. In this example, we cluster the data with DMM clustering. Since the data set is large, the algorithm requires a lot of computational capacity. Therefore, we use only a subset of the data that is agglomerated by Phylum as a rank. In the example of DMM we use GlobalPatterns data. # Get the data data(&quot;GlobalPatterns&quot;, package = &quot;mia&quot;) tse &lt;- GlobalPatterns # Agglomerate by rank tse &lt;- mergeFeaturesByRank(tse, rank = &quot;Phylum&quot;, agglomerateTree = TRUE) In the example below, we calculate model fit using Laplace approximation. The cluster information is added in the metadata with an optional name. # Run the model and calculates the most likely number of clusters from 1 to 7 # Save as an alternative experiment that contains clustering information altExp(tse, &quot;dmm&quot;) &lt;- cluster(tse, name = &quot;DMM&quot;, DmmParam(k = 1:7, type = &quot;laplace&quot;), MARGIN = &quot;samples&quot;, full = TRUE) # The dmm information is stored in the metadata under the &#39;DMM&#39; column that includes information about all seven models altExp(tse, &quot;dmm&quot;) ## class: TreeSummarizedExperiment ## dim: 67 26 ## metadata(2): agglomerated_by_rank DMM ## assays(1): counts ## rownames(67): Phylum:Crenarchaeota Phylum:Euryarchaeota ... ## Phylum:Synergistetes Phylum:SR1 ## rowData names(7): Kingdom Phylum ... Genus Species ## colnames(26): CL3 CC1 ... Even2 Even3 ## colData names(8): X.SampleID Primer ... Description clusters ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: a LinkDataFrame (67 rows) ## rowTree: 1 phylo tree(s) (66 leaves) ## colLinks: NULL ## colTree: NULL The plot below represents the Laplace approximation to the model evidence for each of the k models. We can see that the best number of clusters is two. library(miaViz) plotDMNFit(altExp(tse, &quot;dmm&quot;), type = &quot;laplace&quot;, name = &quot;DMM&quot;) The best model can be confirmed with the following operation. # Get the the best model bestFit &lt;- metadata(altExp(tse, &quot;dmm&quot;))$DMM$dmm[[metadata(altExp(tse, &quot;dmm&quot;))$DMM$best]] bestFit ## class: DMN ## k: 2 ## samples x taxa: 26 x 67 ## Laplace: 7673 BIC: 7927 AIC: 7842 The clusters for the best model are saved in the colData under ‘cluster’ column. head(colData(altExp(tse, &quot;dmm&quot;))$clusters, 10) ## CL3 CC1 SV1 M31Fcsw M11Fcsw M31Plmr M11Plmr F21Plmr M31Tong M11Tong ## 1 1 1 2 2 2 2 2 2 2 ## Levels: 1 2 More detailed information about the clusters can be accessed in the metadata. The metadata contains samples-cluster assignment probabilities that tell us the likelihood for each sample to belong to each cluster. head(metadata(altExp(tse, &quot;dmm&quot;))$DMM$prob, 10) ## 1 2 ## CL3 1.000e+00 5.009e-17 ## CC1 1.000e+00 3.801e-22 ## SV1 1.000e+00 2.017e-12 ## M31Fcsw 7.316e-26 1.000e+00 ## M11Fcsw 1.062e-16 1.000e+00 ## M31Plmr 9.971e-14 1.000e+00 ## M11Plmr 2.184e-06 1.000e+00 ## F21Plmr 2.840e-11 1.000e+00 ## M31Tong 1.304e-08 1.000e+00 ## M11Tong 2.082e-06 1.000e+00 Once the optimal model have been confirmed, we can find out which samples are grouped with each other. The table below shows one sample of each sample type clustered in either of the groups. We can notice that DMM can distinguish environmental samples into one group, and mock and human samples into another. For clarity, in this example, the probabilities for each sample to belong in each cluster have been rounded. library(dplyr) clusters &lt;- round(metadata(altExp(tse, &quot;dmm&quot;))$DMM$prob, 1) clusters &lt;- as.data.frame(cbind(clusters, levels(altExp(tse, &quot;dmm&quot;)$SampleType)[altExp(tse, &quot;dmm&quot;)$SampleType])) # add sample type information colnames(clusters) &lt;- c(&quot;Group1&quot;, &quot;Group2&quot;, &quot;SampleType&quot;) clusters %&gt;% group_by(SampleType) %&gt;% arrange(Group1) %&gt;% filter(row_number()==1) ## # A tibble: 9 × 3 ## # Groups: SampleType [9] ## Group1 Group2 SampleType ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0 1 Feces ## 2 0 1 Skin ## 3 0 1 Tongue ## 4 0 1 Mock ## 5 1 0 Soil ## 6 1 0 Freshwater ## 7 1 0 Freshwater (creek) ## 8 1 0 Ocean ## 9 1 0 Sediment (estuary) We can also plot the driver Phyla in each group. In this case, it reflects the differences between environmental and human samples. # Get the estimates on how much each phyla contributes on each cluster best_model &lt;- metadata(altExp(tse, &quot;dmm&quot;))$DMM$dmm[2] drivers &lt;- as.data.frame(best_model[[1]]@fit$Estimate) drivers$phyla &lt;- gsub(&quot;Phylum:&quot;, &quot;&quot;, rownames(drivers)) # Clean phylum names plots &lt;- c() for (i in 1:2) { drivers &lt;- drivers[order(drivers[[i]], decreasing = TRUE),] p &lt;- ggplot(head(drivers, 10), aes(x = reorder(head(phyla, 10), + head(drivers[[i]], 10)), y = head(drivers[[i]], 10))) + geom_bar(stat = &quot;identity&quot;, fill = &quot;deeppink4&quot;, alpha = 0.5) + coord_flip() + labs(title = paste(&quot;Top phyla in group&quot;, i)) + theme_light(base_size = 15) + labs(x=&quot;&quot;, y=&quot;&quot;) + scale_y_continuous(limits=c(0,7)) print(p) } We use calculateDMNgroup function to have an overview of the best model. The function groups samples by SampleType column from colData and returns DMNGroup object that contains a summary. dmm_group &lt;- calculateDMNgroup(altExp(tse, &quot;dmm&quot;), variable = &quot;SampleType&quot;, assay.type = &quot;counts&quot;, k = 2, seed = .Machine$integer.max) dmm_group ## class: DMNGroup ## summary: ## k samples taxa NLE LogDet Laplace BIC AIC ## Feces 2 4 67 1078.3 -106.26 901.1 1171.9 1213 ## Freshwater 2 2 67 889.6 -97.23 716.9 936.4 1025 ## Freshwater (creek) 2 3 67 1600.3 862.19 1907.3 1674.5 1735 ## Mock 2 3 67 1008.4 -55.40 856.6 1082.5 1143 ## Ocean 2 3 67 1096.7 -56.66 944.3 1170.9 1232 ## Sediment (estuary) 2 3 67 1195.5 18.63 1080.8 1269.7 1331 ## Skin 2 3 67 992.6 -85.05 826.1 1066.8 1128 ## Soil 2 3 67 1380.3 11.20 1261.8 1454.5 1515 ## Tongue 2 2 67 783.0 -107.79 605.0 829.8 918 Mixture weights can be used for having a rough approximation of the cluster size. DirichletMultinomial::mixturewt(bestFit) ## pi theta ## 1 0.5385 20.60 ## 2 0.4615 15.29 10.3.1 PCoA with DMM clusters In this section we show how to calculate principal coordinates for clr transformed abundance data. To calculate PCoA, we use Aitchison distance as a distance metrics that calculates Euclidean distances for clr transformed compositions. In the visualization section, we project the sample distances on two dimensional space of first two principal coordinates. We colour the samples based on their DMM clusters. The visualization demonstrates that the DMM clusters can be distinguished on a PCoA plot, although the clusters are not coherent. This means that two-dimensional representation of the data created by PCoA preserves similar information that drives the DMM cluster division. # add pseudocount, because data contains zeros assay(tse, &quot;pseudo&quot;) &lt;- assay(tse, &quot;counts&quot;) + 1 tse &lt;- transformAssay(tse, assay.type = &quot;pseudo&quot;, method = &quot;relabundance&quot;) # clr transformation tse &lt;- transformAssay(tse, &quot;relabundance&quot;, method = &quot;clr&quot;) # principal coordinate analysis df &lt;- calculateMDS(tse, assay.type = &quot;clr&quot;, method = &quot;euclidean&quot;) # Create a data frame from principal coordinates euclidean_pcoa_df &lt;- data.frame(pcoa1 = df[, 1], pcoa2 = df[, 2]) # Create a data frame that contains principal coordinates and DMM information euclidean_dmm_pcoa_df &lt;- cbind(euclidean_pcoa_df, dmm_component = colData(altExp(tse, &quot;dmm&quot;))$clusters) # Create a plot euclidean_dmm_plot &lt;- ggplot(data = euclidean_dmm_pcoa_df, aes(x = pcoa1, y = pcoa2, color = dmm_component)) + geom_point() + labs(x = &quot;Coordinate 1&quot;,y = &quot;Coordinate 2&quot;, title = &quot;PCoA with Aitchison distances&quot;) + theme(plot.title = element_text(size = 12, # makes titles smaller hjust = 0.5)) euclidean_dmm_plot 10.4 Biclustering Biclustering methods cluster rows and columns simultaneously in order to find subsets of correlated features/samples. Here, we use following packages: biclust cobiclust cobiclust is especially developed for microbiome data whereas biclust is more general method. In this section, we show two different cases and example solutions to apply biclustering to them. Taxa vs samples Taxa vs biomolecule/biomarker Biclusters can be visualized using heatmap or boxplot, for instance. For checking purposes, also scatter plot might be valid choice. Check more ideas for heatmaps from chapters 16 and 9. 10.4.1 Taxa vs samples When you have microbial abundance matrices, we suggest to use cobiclust which is designed for microbial data. Load example data library(cobiclust) data(&quot;HintikkaXOData&quot;) mae &lt;- HintikkaXOData Only the most prevalent taxa are included in analysis. # Subset data in the first experiment mae[[1]] &lt;- subsetByPrevalentFeatures(mae[[1]], rank = &quot;Genus&quot;, prevalence = 0.2, detection = 0.001) # rclr-transform in the first experiment mae[[1]] &lt;- transformAssay(mae[[1]], method = &quot;rclr&quot;) cobiclust takes counts table as an input and gives cobiclust object as an output. It includes clusters for taxa and samples. # Do clustering using counts table clusters &lt;- cobiclust(assay(mae[[1]], &quot;counts&quot;)) # Get clusters row_clusters &lt;- clusters$classification$rowclass col_clusters &lt;- clusters$classification$colclass # Add clusters to rowdata and coldata rowData(mae[[1]])$clusters &lt;- factor(row_clusters) colData(mae[[1]])$clusters &lt;- factor(col_clusters) # Order data based on clusters mae[[1]] &lt;- mae[[1]][order(rowData(mae[[1]])$clusters), order(colData(mae[[1]])$clusters)] # Print clusters clusters$classification ## $rowclass ## [1] 1 1 1 1 1 1 1 1 1 2 2 1 2 1 1 1 2 1 2 1 1 1 2 2 1 1 2 2 1 2 2 1 1 2 1 1 2 ## [38] 1 2 2 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 ## [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 1 1 1 1 1 1 2 2 1 1 1 1 ## [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## ## $colclass ## C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13 C14 C15 C16 C17 C18 C19 C20 ## 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## C21 C22 C23 C24 C25 C26 C27 C28 C29 C30 C31 C32 C33 C34 C35 C36 C37 C38 C39 C40 ## 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 Next we can plot clusters. Annotated heatmap is a common choice. library(pheatmap) # z-transform for heatmap mae[[1]] &lt;- transformAssay(mae[[1]], assay.type = &quot;rclr&quot;, MARGIN = &quot;features&quot;, method = &quot;z&quot;, name = &quot;rclr_z&quot;) # Create annotations. When column names are equal, they should share levels. # Here samples include 3 clusters, and taxa 2. That is why we have to make # column names unique. annotation_col &lt;- data.frame(colData(mae[[1]])[, &quot;clusters&quot;, drop = F]) colnames(annotation_col) &lt;- &quot;col_clusters&quot; annotation_row &lt;- data.frame(rowData(mae[[1]])[, &quot;clusters&quot;, drop = F]) colnames(annotation_row) &lt;- &quot;row_clusters&quot; Plot the heatmap. pheatmap(assay(mae[[1]], &quot;rclr_z&quot;), cluster_rows = F, cluster_cols = F, annotation_col = annotation_col, annotation_row = annotation_row) Boxplot is commonly used to summarize the results: library(ggplot2) library(patchwork) # ggplot requires data in melted format melt_assay &lt;- meltAssay(mae[[1]], assay.type = &quot;rclr&quot;, add_col_data = T, add_row_data = T) # patchwork two plots side-by-side p1 &lt;- ggplot(melt_assay) + geom_boxplot(aes(x = clusters.x, y = rclr)) + labs(x = &quot;Taxa clusters&quot;) p2 &lt;- ggplot(melt_assay) + geom_boxplot(aes(x = clusters.y, y = rclr)) + labs(x = &quot;Sample clusters&quot;) p1 + p2 10.4.2 Taxa vs biomolecules Here, we analyze cross-correlation between taxa and metabolites. This is a case, where we use biclust method which is suitable for numeric matrices in general. First we pre-process the data. # Samples must be in equal order # (Only 1st experiment was ordered in cobiclust step leading to unequal order) mae[[1]] &lt;- mae[[1]][, colnames(mae[[2]])] # Make rownames unique since it is required by other steps rownames(mae[[1]]) &lt;- make.unique(rownames(mae[[1]])) # Transform the metabolites to be in log basis mae[[2]] &lt;- transformAssay(mae[[2]], assay.type = &quot;nmr&quot;, method = &quot;log10&quot;) # Add missing data to the metabolites replace_na &lt;- function(row) { na_indices &lt;- which(is.na(row)) non_na_values &lt;- row[!is.na(row)] row[na_indices] &lt;- sample(non_na_values, length(na_indices), replace = TRUE) row } assay(mae[[2]], &quot;log10&quot;) &lt;- t(apply(assay(mae[[2]], &quot;log10&quot;), 1, replace_na)) Next, we compute the spearman correlation matrix. # Calculate correlations corr &lt;- getExperimentCrossCorrelation(mae, 1, 2, assay.type1 = &quot;rclr&quot;, assay.type2 = &quot;log10&quot;, mode = &quot;matrix&quot;, correlation = &quot;spearman&quot;) biclust takes a matrix as an input and returns a biclust object. library(biclust) # Set seed for reproducibility set.seed(3973) # Find biclusters bc &lt;- biclust(corr, method = BCPlaid(), verbose = FALSE) bc ## ## An object of class Biclust ## ## call: ## biclust(x = corr, method = BCPlaid(), verbose = FALSE) ## ## Number of Clusters found: 5 ## ## First 5 Cluster sizes: ## BC 1 BC 2 BC 3 BC 4 BC 5 ## Number of Rows: 21 18 7 3 3 ## Number of Columns: 13 11 9 8 6 The object includes cluster information. However compared to cobiclust, biclust object includes only information about clusters that were found, not general cluster. Meaning that if one cluster size of 5 features was found out of 20 features, those 15 features do not belong to any cluster. That is why we have to create an additional cluster for features/samples that are not assigned into any cluster. # Functions for obtaining biclust information # Get clusters for rows and columns .get_biclusters_from_biclust &lt;- function(bc, assay) { # Get cluster information for columns and rows bc_columns &lt;- t(bc@NumberxCol) bc_columns &lt;- data.frame(bc_columns) bc_rows &lt;- bc@RowxNumber bc_rows &lt;- data.frame(bc_rows) # Get data into right format bc_columns &lt;- .manipulate_bc_data(bc_columns, assay, &quot;col&quot;) bc_rows &lt;- .manipulate_bc_data(bc_rows, assay, &quot;row&quot;) return(list(bc_columns = bc_columns, bc_rows = bc_rows)) } # Input clusters, and how many observations there should be, i.e., # the number of samples or features .manipulate_bc_data &lt;- function(bc_clusters, assay, row_col) { # Get right dimension dim &lt;- ifelse(row_col == &quot;col&quot;, ncol(assay), nrow(assay)) # Get column/row names if (row_col == &quot;col&quot;) { names &lt;- colnames(assay) } else { names &lt;- rownames(assay) } # If no clusters were found, create one. Otherwise create additional # cluster which # contain those samples that are not included in clusters that were found. if (nrow(bc_clusters) != dim) { bc_clusters &lt;- data.frame(cluster = rep(TRUE, dim)) } else { # Create additional cluster that includes those samples/features that # are not included in other clusters. vec &lt;- ifelse(rowSums(bc_clusters) &gt; 0, FALSE, TRUE) # If additional cluster contains samples, then add it if (any(vec)) { bc_clusters &lt;- cbind(bc_clusters, vec) } } # Adjust row and column names rownames(bc_clusters) &lt;- names colnames(bc_clusters) &lt;- paste0(&quot;cluster_&quot;, 1:ncol(bc_clusters)) return(bc_clusters) } # Get biclusters bcs &lt;- .get_biclusters_from_biclust(bc, corr) bicluster_rows &lt;- bcs$bc_rows bicluster_columns &lt;- bcs$bc_columns # Print biclusters for rows head(bicluster_rows) ## cluster_1 ## D_1__Firmicutes_D_2__Bacilli_D_3__Bacillales_D_4__Staphylococcaceae_D_5__Staphylococcus FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Klebsiella FALSE ## D_1__Firmicutes_D_2__Bacilli_D_3__Lactobacillales_D_4__Streptococcaceae_D_5__Streptococcus FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Escherichia-Shigella FALSE ## D_1__Firmicutes_D_2__Clostridia_D_3__Clostridiales_D_4__Ruminococcaceae_D_5__Ruminiclostridium 5 FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Pseudomonadales_D_4__Pseudomonadaceae_D_5__Pseudomonas FALSE ## cluster_2 ## D_1__Firmicutes_D_2__Bacilli_D_3__Bacillales_D_4__Staphylococcaceae_D_5__Staphylococcus FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Klebsiella FALSE ## D_1__Firmicutes_D_2__Bacilli_D_3__Lactobacillales_D_4__Streptococcaceae_D_5__Streptococcus FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Escherichia-Shigella FALSE ## D_1__Firmicutes_D_2__Clostridia_D_3__Clostridiales_D_4__Ruminococcaceae_D_5__Ruminiclostridium 5 TRUE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Pseudomonadales_D_4__Pseudomonadaceae_D_5__Pseudomonas FALSE ## cluster_3 ## D_1__Firmicutes_D_2__Bacilli_D_3__Bacillales_D_4__Staphylococcaceae_D_5__Staphylococcus FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Klebsiella FALSE ## D_1__Firmicutes_D_2__Bacilli_D_3__Lactobacillales_D_4__Streptococcaceae_D_5__Streptococcus FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Escherichia-Shigella FALSE ## D_1__Firmicutes_D_2__Clostridia_D_3__Clostridiales_D_4__Ruminococcaceae_D_5__Ruminiclostridium 5 TRUE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Pseudomonadales_D_4__Pseudomonadaceae_D_5__Pseudomonas FALSE ## cluster_4 ## D_1__Firmicutes_D_2__Bacilli_D_3__Bacillales_D_4__Staphylococcaceae_D_5__Staphylococcus FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Klebsiella FALSE ## D_1__Firmicutes_D_2__Bacilli_D_3__Lactobacillales_D_4__Streptococcaceae_D_5__Streptococcus FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Escherichia-Shigella FALSE ## D_1__Firmicutes_D_2__Clostridia_D_3__Clostridiales_D_4__Ruminococcaceae_D_5__Ruminiclostridium 5 FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Pseudomonadales_D_4__Pseudomonadaceae_D_5__Pseudomonas FALSE ## cluster_5 ## D_1__Firmicutes_D_2__Bacilli_D_3__Bacillales_D_4__Staphylococcaceae_D_5__Staphylococcus FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Klebsiella FALSE ## D_1__Firmicutes_D_2__Bacilli_D_3__Lactobacillales_D_4__Streptococcaceae_D_5__Streptococcus TRUE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Escherichia-Shigella FALSE ## D_1__Firmicutes_D_2__Clostridia_D_3__Clostridiales_D_4__Ruminococcaceae_D_5__Ruminiclostridium 5 FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Pseudomonadales_D_4__Pseudomonadaceae_D_5__Pseudomonas FALSE ## cluster_6 ## D_1__Firmicutes_D_2__Bacilli_D_3__Bacillales_D_4__Staphylococcaceae_D_5__Staphylococcus TRUE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Klebsiella TRUE ## D_1__Firmicutes_D_2__Bacilli_D_3__Lactobacillales_D_4__Streptococcaceae_D_5__Streptococcus FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Enterobacteriales_D_4__Enterobacteriaceae_D_5__Escherichia-Shigella TRUE ## D_1__Firmicutes_D_2__Clostridia_D_3__Clostridiales_D_4__Ruminococcaceae_D_5__Ruminiclostridium 5 FALSE ## D_1__Proteobacteria_D_2__Gammaproteobacteria_D_3__Pseudomonadales_D_4__Pseudomonadaceae_D_5__Pseudomonas TRUE Let’s collect information for the scatter plot. # Function for obtaining sample-wise sum, mean, median, and mean variance # for each cluster .sum_mean_median_var &lt;- function(tse1, tse2, assay.type1, assay.type2, clusters1, clusters2) { list &lt;- list() # Create a data frame that includes all the information for (i in 1:ncol(clusters1)) { # Subset data based on cluster tse_subset1 &lt;- tse1[clusters1[, i], ] tse_subset2 &lt;- tse2[clusters2[, i], ] # Get assay assay1 &lt;- assay(tse_subset1, assay.type1) assay2 &lt;- assay(tse_subset2, assay.type2) # Calculate sum, mean, median, and mean variance sum1 &lt;- colSums2(assay1, na.rm = T) mean1 &lt;- colMeans2(assay1, na.rm = T) median1 &lt;- colMedians(assay1, na.rm = T) var1 &lt;- colVars(assay1, na.rm = T) sum2 &lt;- colSums2(assay2, na.rm = T) mean2 &lt;- colMeans2(assay2, na.rm = T) median2 &lt;- colMedians(assay2, na.rm = T) var2 &lt;- colVars(assay2, na.rm = T) list[[i]] &lt;- data.frame(sample = colnames(tse1), sum1, sum2, mean1, mean2, median1, median2, var1, var2) } return(list) } # Calculate info df &lt;- .sum_mean_median_var(mae[[1]], mae[[2]], &quot;rclr&quot;, &quot;log10&quot;, bicluster_rows, bicluster_columns) Now we can create a scatter plot. X-axis includes median clr abundance of microbiome and y-axis median absolute concentration of each metabolite. Each data point represents a single sample. From the plots, we can see that there is low negative correlation in both cluster 1 and 3. This means that when abundance of bacteria belonging to cluster 1 or 3 is higher, the concentration of metabolites of cluster 1 or 3 is lower, and vice versa. pics &lt;- list() for (i in seq_along(df)) { pics[[i]] &lt;- ggplot(df[[i]]) + geom_point(aes(x = median1, y = median2)) + labs(title = paste0(&quot;Cluster &quot;, i), x = &quot;Taxa (rclr median)&quot;, y = &quot;Metabolites (abs. median)&quot;) print(pics[[i]]) } pics[[1]] + pics[[2]] + pics[[3]] pheatmap does not allow boolean values, so they must be converted into factors. bicluster_columns &lt;- data.frame(apply(bicluster_columns, 2, as.factor)) bicluster_rows &lt;- data.frame(apply(bicluster_rows, 2, as.factor)) Again, we can plot clusters with heatmap. # Adjust colors for all clusters if (ncol(bicluster_rows) &gt; ncol(bicluster_columns)) { cluster_names &lt;- colnames(bicluster_rows) } else { cluster_names &lt;- colnames(bicluster_columns) } annotation_colors &lt;- list() for (name in cluster_names) { annotation_colors[[name]] &lt;- c(&quot;TRUE&quot; = &quot;red&quot;, &quot;FALSE&quot; = &quot;white&quot;) } # Create a heatmap pheatmap(corr, cluster_cols = F, cluster_rows = F, annotation_col = bicluster_columns, annotation_row = bicluster_rows, annotation_colors = annotation_colors) 10.5 Additional Community Typing For more community typing techniques applied to the ‘SprockettTHData’ data set, see the attached .Rmd file. Link: Rmd "],["differential-abundance.html", "Chapter 11 Differential Abundance 11.1 Statistical challenges of microbiome data 11.2 Using the tools 11.3 DAA with confounding 11.4 Additional resources", " Chapter 11 Differential Abundance .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } This section provides an introduction to Differential Abundance Analysis (DAA), which is used to identify differences in the abundances of individual taxa (at any taxonomic level) between two or more groups, such as treatment and control. Here, we demonstrate its implementation on Tengeler2020, described in chapter 3.3.1.1. The goal of DAA is to identify biomarkers of a certain phenotype or condition, and gain understanding of a complex system by looking at its isolated components. For example, the identification of a bacterial taxon that is differentially abundant between healthy patients and diseased patients can lead to important insights into the pathophysiology of the disease. In other words, differentially abundant taxa can be involved in the dynamics of the disease, which in turn helps understand the system as a whole. Despite its relevance in current research, the DAA approach has also been subject to debate (Quinn, Gordon-Rodriguez, and Erb 2021). 11.1 Statistical challenges of microbiome data Microbiome data display unique properties that are exclusively addressed by DAA tools developed for microbiome analysis. Specifically, microbiome data are characterized by high variability, zero-inflation and compositionality. High variability expresses that abundance of taxa often varies by several orders of magnitude from sample to sample. Zero-inflation means that typically more than 70% of the values are zeros, which could be due to either physical absence (structural zeros) or insufficient sampling effort (sampling zeros). Compositionality implies that a change in the absolute abundance of one taxon will lead to apparent variations in the relative abundances of other taxa in the same sample. If neglected, such properties may cause significant bias in the results of DAA. Therefore, several approaches have been developed to address the unique properties of microbiome data and provide statistically useful results. The first approach to target zero-inflated data consists of specialized models, such as over-dispersed count models and zero-inflated mixture models. DESeq2, edgeR and corncorb are based on over-dispersed count models, whereas metagenomeSeq, RAIDA, ZIBB and Omnibus implement zero-inflated mixture models to address zero-inflation. Typically, these models assume a negative binomial, beta-binomial or normal/log-normal distribution. Alternatively, zero imputation also represents a valid approach to deal with zero-inflated data. ALDEx2 and eBay apply a Bayesian model to impute the zeros when working with proportion data, accounting for sampling variability and sequencing depth variation. Other methods, such as MaAsLin2 and ANCOMBC impute the zeros with a pseudo-count strategy. Regarding the compositionality of microbiome data, several approaches have been developed to perform robust normalization with methods specifically designed to reduce the bias found in compositional data. Some examples include trimmed mean of M-values (TMM) normalization used by edgeR, relative log expression (RLE) normalization used by DESeq2, cumulative sum scaling (CSS) normalization used by metagenomeSeq, centered log-ratio transformation (CLR) normalization used by ALDEx2 and geometric mean of pairwise ratios (GMPR) normalization used by Omnibus and Wrench normalization (Kumar et al. 2018), which corrects the compositional bias by an empirical Bayes approach. Other methods to deal with compositional data entail reference taxa approach used by DACOMP and RAIDA, analyzing the pattern of pairwise log ratios as done by ANCOM and bias-correction applied by ANCOMBC. We recommend to have a look at Nearing et al. (2022). In this study, multiple DAA methods were applied to 38 different datasets and their results were compared to one another. Because each method follows a slightly different approach in terms of assumptions and normalization techniques, it was shown that results on the same dataset may differ substantially depending on the method. Recently, Yang and Chen (2022) comprehensively evaluated DAA methods via a semi-parametric framework and 106 real datasets. This study also concluded that different methods can produce contradictory results, creating the risk of cherry-picking the most favorable options for one’s own hypothesis. Therefore, it is highly recommended to perform DAA with multiple methods to determine whether the findings can be reproduced by different approaches. Built on the findings of Calgaro et al. (2020), the benchdamic (Calgaro et al. 2022) package could offer a valuable support in this regard. Through a comprehensive evaluation process it serves both practitioners by comparing DA methods from existing literature, and method developers by providing an impartial tool to evaluate their new approaches in comparison to what is already available. For details, check its extensive vignette. 11.2 Using the tools In this section we demonstrate the use of four methods that can be recommended based on recent literature (ANCOM-BC (Lin and Peddada 2020), ALDEx2 (Gloor, Macklaim, and Fernandes 2016), Maaslin2 (Mallick, Rahnavard, and McIver 2020), LinDA (H. Zhou et al. 2022) and ZicoSeq (Yang and Chen 2022)). The purpose of this section is to show how to perform DAA in R, not how to correctly do causal inference. Depending on your experimental setup and your theory, you must determine how to specify any model exactly. E.g., there might be confounding factors that might drive (the absence of) differences between the shown groups that we ignore here for simplicity. Or your dataset is repeated sampling design, matched-pair design or the general longitudianl design. We will demonstrate how to include covariates in those models. We picked a dataset that merely has microbial abundances in a TSE object as well as a grouping variable in the sample data. We simplify the examples by only including two of the three groups. library(mia) library(tidyverse) # Import dataset data(&quot;Tengeler2020&quot;, package = &quot;mia&quot;) tse &lt;- Tengeler2020 # Show patient status by cohort table(tse$patient_status, tse$cohort) %&gt;% knitr::kable() Cohort_1 Cohort_2 Cohort_3 ADHD 4 5 4 Control 6 5 3 11.2.1 Preparing the data for DAA Before starting the analysis, it is recommended to reduce the size and complexity of the data to make the results more reproducible. For this purpose, we agglomerate the features by genus and filter them by a prevalence threshold of 10%. # Agglomerate by genus and subset by prevalence tse &lt;- subsetByPrevalentFeatures(tse, rank = &quot;Genus&quot;, prevalence = 10 / 100) # Transform count assay to relative abundances tse &lt;- transformAssay(tse, assay.type = &quot;counts&quot;, method = &quot;relabundance&quot;) While some DAA tools provide optional arguments for prevalence filtering, here we filtered the tse object directly. This way, we ensure that the input data remains the same when multiple tools are used. 11.2.2 ALDEx2 In this section, we will show how to perform DAA with ALDEx2, which can be regarded as the method of choice for its consistency, as it normally identifies features that are also found by complementary methods (Nearing et al. 2022). A more extensive introduction to its functionality is available in the ALDEx2 vignette. ALDEx2 estimates technical variation within each sample per taxon by utilizing the Dirichlet distribution. It furthermore applies the CLR transformation (or closely related log-ratio transforms). Depending on the experimental setup, it will perform a two sample Welch’s t-test and Wilcoxon test or a one-way ANOVA and Kruskal-Wallis test. For more complex study designs, there is a possibility to utilize the glm functionality within ALDEx2. The Benjamini-Hochberg procedure is applied by default to correct for multiple testing. # Load package library(ALDEx2) # Generate Monte Carlo samples of the Dirichlet distribution for each sample. # Convert each instance using the centered log-ratio transform. # This is the input for all further analyses. set.seed(123) x &lt;- aldex.clr(assay(tse), tse$patient_status) The t-test: # calculates expected values of the Welch&#39;s t-test and Wilcoxon rank # test on the data returned by aldex.clr x_tt &lt;- aldex.ttest(x, paired.test = FALSE, verbose = FALSE) Effect sizes: # Determines the median clr abundance of the feature in all samples and in # groups, the median difference between the two groups, the median variation # within each group and the effect size, which is the median of the ratio # of the between group difference and the larger of the variance within groups x_effect &lt;- aldex.effect(x, CI = TRUE, verbose = FALSE) # combine all outputs aldex_out &lt;- data.frame(x_tt, x_effect) Now, we can create a so called Bland-Altman or MA plot (left). It shows the association between the relative abundance and the magnitude of the difference per sample. Next to that, we can also create a plot that shows the dispersion on the x-axis instead of log-ratio abundance. Red dots represent genera that are differentially abundant (\\(q \\leq 0.1\\)) between the 2 groups. Black points are rare taxa and grey ones are abundant taxa. The dashed line represent an effect size of 1. Gloor, Macklaim, and Fernandes (2016) provides more information on these plots. par(mfrow = c(1, 2)) aldex.plot(aldex_out, type = &quot;MA&quot;, test = &quot;welch&quot;, xlab = &quot;Log-ratio abundance&quot;, ylab = &quot;Difference&quot;, cutoff = 0.05) aldex.plot(aldex_out, type = &quot;MW&quot;, test = &quot;welch&quot;, xlab = &quot;Dispersion&quot;, ylab = &quot;Difference&quot;, cutoff = 0.05) The evaluation as differential abundant in above plots is based on the corrected p-value. According to the ALDEx2 developers, the safest approach is to identify those features where the 95% CI of the effect size does not cross 0. As we can see in below table, this is not the case for any of the identified genera (see overlap column, which indicates the proportion of overlap). Also, the authors recommend to focus on effect sizes and CIs rather than interpreting the p-value. To keep the comparison simple, we will here use the p-value as decision criterion. But please be aware that the effect size together with the CI is a better answer to the question we are typically interested in. aldex_out %&gt;% rownames_to_column(var = &quot;Genus&quot;) %&gt;% # here we choose the wilcoxon output rather than t-test output filter(wi.eBH &lt;= 0.05) %&gt;% dplyr::select(Genus, we.eBH, wi.eBH, effect, overlap) %&gt;% knitr::kable() Genus we.eBH wi.eBH effect overlap Genus:[Ruminococcus]_gauvreauii_group 0.073 0.0379 0.8151 0.1357 11.2.3 ANCOM-BC The analysis of composition of microbiomes with bias correction (ANCOM-BC) (Lin and Peddada 2020) is a recently developed method for differential abundance testing. It is based on an earlier published approach (Mandal et al. 2015). The previous version of ANCOM was among the methods that produced the most consistent results and is probably a conservative approach (Nearing et al. 2022). However, the new ANCOM-BC method operates quite differently compared to the former ANCOM method. As the only method, ANCOM-BC incorporates the so called sampling fraction into the model. The latter term could be empirically estimated by the ratio of the library size to the microbial load. According to the authors, ignoring variations in this sampling fraction would bias DAA results. Furthermore, this method provides p-values and confidence intervals for each taxon. It also controls the FDR and it is computationally simple to implement. Note that the original method was implemented in the ancombc() function (see extended tutorial). The method has since then been updated and new features have been added to enable multi-group comparisons and repeated measurements among other improvements. We do not cover the more advanced features of ANCOMBC in this tutorial as these features are documented in detail in this tutorial. We now proceed with a simple example. First, we specify a formula. In this formula, other covariates could potentially be included to adjust for confounding. We show this further below. Again, please make sure to check the function documentation as well as the linked tutorials to learn about the additional arguments that we specify. # Load package library(ANCOMBC) # Run ANCOM-BC at the genus level and only including the prevalent genera ancombc2_out &lt;- ancombc2(data = tse, assay.type = &quot;counts&quot;, fix_formula = &quot;patient_status&quot;, p_adj_method = &quot;fdr&quot;, prv_cut = 0, group = &quot;patient_status&quot;, struc_zero = TRUE, neg_lb = TRUE, # multi group comparison is deactivated automatically global = TRUE) The object out contains all model output. Again, see the documentation of the function under Value for details. Our question whether taxa are differentially abundant can be answered by looking at the res object, which contains dataframes with the coefficients, standard errors, p-values and q-values. Below we show the first entries of this dataframe. # store the FDR adjusted results ancombc2_out$res %&gt;% dplyr::select(taxon, lfc_patient_statusControl, q_patient_statusControl) %&gt;% filter(q_patient_statusControl &lt; 0.05) %&gt;% arrange(q_patient_statusControl) %&gt;% head() %&gt;% knitr::kable() taxon lfc_patient_statusControl q_patient_statusControl Genus:Subdoligranulum 1.817 0.0011 Genus:Ruminococcus_1 2.819 0.0011 Genus:[Ruminococcus]_gauvreauii_group 1.442 0.0016 Genus:[Clostridium]_innocuum_group 1.388 0.0053 Genus:[Eubacterium]_rectale_group 1.259 0.0053 Genus:Coprobacter -1.185 0.0053 11.2.4 MaAsLin2 Let us next illustrate MaAsLin2 (Mallick, Rahnavard, and McIver 2020). This method is based on generalized linear models and flexible for different study designs and covariate structures. For details, check their Biobakery tutorial. # Load package library(Maaslin2) # maaslin expects features as columns and samples as rows # for both the abundance table as well as metadata # We can specify different GLMs/normalizations/transforms. # specifying a ref is especially important if you have more than 2 levels maaslin2_out &lt;- Maaslin2(input_data = as.data.frame(t(assay(tse))), input_metadata = as.data.frame(colData(tse)), output = &quot;DAA example&quot;, transform = &quot;AST&quot;, fixed_effects = &quot;patient_status&quot;, # you can also fit MLM by specifying random effects # random_effects = c(...), reference = &quot;patient_status,Control&quot;, normalization = &quot;TSS&quot;, standardize = FALSE, # filtering was previously performed min_prevalence = 0) Which genera are identified as differentially abundant? (leave out “head” to see all). maaslin2_out$results %&gt;% filter(qval &lt; 0.05) %&gt;% knitr::kable() feature metadata value coef stderr pval name qval N N.not.zero Genus..Ruminococcus._gauvreauii_group patient_status ADHD -0.0662 0.0131 0.0000 patient_statusADHD 0.0017 27 21 Genus.Faecalibacterium patient_status ADHD 0.1195 0.0363 0.0030 patient_statusADHD 0.0391 27 11 Genus..Clostridium._innocuum_group patient_status ADHD -0.0678 0.0209 0.0033 patient_statusADHD 0.0391 27 25 Order.Bacteroidales patient_status ADHD 0.0447 0.0139 0.0035 patient_statusADHD 0.0391 27 6 Genus.Catabacter patient_status ADHD 0.0288 0.0090 0.0036 patient_statusADHD 0.0391 27 9 This will create a folder that is called like in the output specified above. It contains also figures to visualize difference between significant taxa. 11.2.5 LinDA Lastly, we cover linear models for differential abundance analysis of microbiome compositional data (H. Zhou et al. (2022)). This is very similar to ANCOMBC with few differences: 1) LinDA corrects for the compositional bias differently using the mode of all regression coefficients. 2) it is faster (100x-1000x than ANCOMBC and according to the authors); 3) it supports hierarchical models. The latest ANCOMBC versions are also supporting hierarchical models. Nevertheless, LinDA seems a promising tool that achieves a very good power/fdr trade-off together with ANCOMBC according to the review. The speed improvements might make it critical especially for datasets that have higher sample or feature set sizes. # Load package library(MicrobiomeStat) # Run LinDA linda_out &lt;- linda(feature.dat = as.data.frame(assay(tse)), meta.dat = as.data.frame(colData(tse)), formula = &quot;~ patient_status&quot;, alpha = 0.05, prev.filter = 0, mean.abund.filter = 0) ## 0 features are filtered! ## The filtered data has 27 samples and 54 features will be tested! ## Pseudo-count approach is used. ## Fit linear models ... ## Completed. # List genera for which H0 could be rejected: linda_out$output$patient_statusControl %&gt;% filter(reject) %&gt;% dplyr::select(stat, padj) %&gt;% rownames_to_column(var = &quot;feature&quot;) %&gt;% knitr::kable() feature stat padj Genus:Faecalibacterium -4.194 0.0101 Genus:Erysipelatoclostridium 3.031 0.0474 Genus:[Ruminococcus]_gauvreauii_group 4.108 0.0101 Genus:Barnesiella -3.091 0.0474 Order:Bacteroidales -3.606 0.0243 Genus:Ruminococcaceae_UCG-014 -2.993 0.0474 Genus:Catabacter -3.387 0.0316 11.2.6 ZicoSeq Subsequently, we demonstrate DAA with ZicoSeq, a method based on linear models and permutation. Further details can be found in this tutorial. This approach has been assessed to exhibit high power and a low false discovery rate, which has the following components: Winsorization to decrease the influence of outliers; Posterior sampling based on a beta mixture prior to address sampling variability and zero inflation; Reference-based multiple-stage normalization to address compositional effects; # Load package library(GUniFrac) set.seed(123) zicoseq_out &lt;- ZicoSeq(feature.dat = as.matrix(assay(tse)), meta.dat = as.data.frame(colData(tse)), grp.name = &quot;patient_status&quot;, feature.dat.type = &quot;count&quot;, return.feature.dat = TRUE, prev.filter = 0, mean.abund.filter = 0, max.abund.filter = 0, perm.no = 999) ## For sample size less than 40, posterior sampling will not be used! ## 0 features are filtered! ## The data has 27 samples and 54 features will be tested! ## On average, 1 outlier counts will be replaced for each feature! ## Finding the references ... ## Permutation testing ... ## ................................................................................................... ## ................................................................................................... ## ................................................................................................... ## ................................................................................................... ## ................................................................................................... ## ................................................................................................... ## Completed! zicoseq_res &lt;- cbind.data.frame(p.raw = zicoseq_out$p.raw, p.adj.fdr = zicoseq_out$p.adj.fdr) zicoseq_res %&gt;% filter(p.adj.fdr &lt; 0.05) %&gt;% arrange(p.adj.fdr) %&gt;% knitr::kable() p.raw p.adj.fdr Genus:[Ruminococcus]_gauvreauii_group 0.001 0.0040 Genus:[Clostridium]_innocuum_group 0.003 0.0210 Genus:Faecalibacterium 0.001 0.0230 Order:Bacteroidales 0.011 0.0358 Genus:Catabacter 0.005 0.0366 ## x-axis is the effect size: R2 * direction of coefficient ZicoSeq.plot(ZicoSeq.obj = zicoseq_out, pvalue.type = &#39;p.adj.fdr&#39;) 11.2.7 PhILR PhILR is a tree-based method that tests group-wise associations based on balances. A detailed introduction to this method is available in this Bioconductor tutorial. 11.2.8 Comparison of methods Although the methods described above yield unidentical results, they are expected to agree on a few differentially abundant taxa. To draw more informed conclusions, it is good practice to compare the outcomes of different methods in terms of found features, their effect sizes and significances, as well as other method-specific aspects. Such comparative approach is outlined in this exercise. 11.3 DAA with confounding Confounders can be defined as variables that are related to and affect the apparent dynamics between the response and the main independent variable. They are common in experimental studies. Generally, they can be classified into 3 groups: Biological confounders, such as age and sex Technical confounders produced during sample collection, processing and analysis Confounders resulting from experimental models, such as batch effects and sample history Controlling for confounders is an important practice to reach an unbiased conclusion. To perform causal inference, it is crucial that the method is able to include confounders in the model. This is not possible with statistical tests of general use, such as the Wilcoxon test. In contrast, methods that target DAA, such as those described in this chapter, allow controlling for confounders. In the following examples, we will perform DAA with a main independent variable and a few confounders. 11.3.1 Selecting confounders In addition to patient status, we will now control for two confounders: cohort and library size. The former is a categorical variable with three factors, whereas the latter is a discrete numerical variable. Remarkably, most DAA methods accept these two and several other data types. For demonstration, library size is treated as a confounder and included in the formulas of the DAA methods. Although this is a satisfactory approach to control for uneven sequencing efforts across samples, rarefaction generally represents a better solution (Schloss 2023). With that said, library size can be readily computed and added to the colData. # Compute and store library size in colData colData(tse)$library_size &lt;- colSums(assay(tse, &quot;counts&quot;)) 11.3.2 ANCOM-BC Here, confounders can be added to the formula along with patient status, the main outcome variable. This way, the model evaluates whether differentially abundant taxa are associated with one of the variables when the other two are kept constant. # perform the analysis ancombc2_out &lt;- ancombc2(tse, assay.type = &quot;counts&quot;, fix_formula = &quot;patient_status + cohort + library_size&quot;, p_adj_method = &quot;fdr&quot;, lib_cut = 0, group = &quot;patient_status&quot;, struc_zero = TRUE, neg_lb = TRUE, alpha = 0.05, # multi-group comparison is deactivated automatically global = TRUE) In the output, each taxon is assigned with several effect sizes (lfc, which stands for log-fold change) and adjusted p-values (q). For categorical variables such as patient status and cohort, the statistics indicate whether the abundance of a given taxon is significantly different between the specified group (column name) and the reference group (the group that does not appear in the column names), whereas for numerical variables such as library size, they indicate whether the abundance of a given taxon varies with that variable. ancombc2_out$res %&gt;% dplyr::select(starts_with(c(&quot;taxon&quot;, &quot;lfc&quot;, &quot;q&quot;))) %&gt;% arrange(q_patient_statusControl) %&gt;% head() %&gt;% knitr::kable() taxon lfc_(Intercept) lfc_patient_statusControl lfc_cohortCohort_2 lfc_cohortCohort_3 lfc_library_size q_(Intercept) q_patient_statusControl q_cohortCohort_2 q_cohortCohort_3 q_library_size Genus:Hungatella -0.2343 -0.7177 -0.1473 -0.0996 0e+00 0 0 0 0 0.0000 Genus:Ruminococcaceae_UCG-013 -1.0393 -0.7369 0.6178 -0.0217 1e-04 0 0 0 0 0.0000 Family:Lachnospiraceae -0.9455 -0.6247 0.5917 0.5393 0e+00 0 0 0 0 0.0000 Genus:Alistipes -0.0252 -0.4359 -0.4571 0.0100 0e+00 0 0 0 0 0.0035 Genus:Bacteroides -0.3896 -1.1519 0.0942 0.7381 0e+00 0 0 0 0 0.0001 Genus:Escherichia-Shigella -1.3544 -0.9021 1.3086 0.2621 1e-04 0 0 0 0 0.0000 11.3.3 LinDA As in the previous method, confounders can be included in the formula with the main outcome variable. linda_out &lt;- linda(as.data.frame(assay(tse, &quot;counts&quot;)), as.data.frame(colData(tse)), formula = &quot;~ patient_status + cohort + library_size&quot;, alpha = 0.05, prev.filter = 0, mean.abund.filter = 0) ## 0 features are filtered! ## The filtered data has 27 samples and 54 features will be tested! ## Imputation approach is used. ## Fit linear models ... ## Completed. The model returns an output for every variable included in the formula. Normally, only the results on the main outcome variable are relevant and can be retrieved as shown below. However, the statistics on the confounders can be similarly obtained by accessing the corresponding items from the output object. # Select results for the patient status linda_res &lt;- linda_out$output$patient_statusControl linda_res %&gt;% filter(reject) %&gt;% dplyr::select(log2FoldChange, stat, padj) %&gt;% rownames_to_column(var = &quot;feature&quot;) %&gt;% head() %&gt;% knitr::kable() feature log2FoldChange stat padj Genus:Faecalibacterium -5.843 -4.473 0.0051 Genus:Erysipelatoclostridium 3.827 3.048 0.0398 Genus:[Ruminococcus]_gauvreauii_group 4.303 4.494 0.0051 Genus:Barnesiella -3.802 -3.067 0.0398 Order:Bacteroidales -4.098 -3.938 0.0126 Genus:Ruminococcaceae_UCG-014 -2.980 -3.592 0.0175 The output shows effect sizes in terms of log-fold changes and a derived statistic (stat) as well as the corresponding adjusted p-values for differences in abundance of each taxon between the control and treated group. 11.3.4 ZicoSeq For this method, confounders can be added as a list to the adj.name argument. set.seed(123) zicoseq_out &lt;- ZicoSeq(feature.dat = as.matrix(assay(tse)), meta.dat = as.data.frame(colData(tse)), grp.name = &quot;patient_status&quot;, adj.name = c(&quot;cohort&quot;, &quot;library_size&quot;), feature.dat.type = &quot;count&quot;, return.feature.dat = TRUE, prev.filter = 0, mean.abund.filter = 0, max.abund.filter = 0, perm.no = 999) ## For sample size less than 40, posterior sampling will not be used! ## 0 features are filtered! ## The data has 27 samples and 54 features will be tested! ## On average, 1 outlier counts will be replaced for each feature! ## Finding the references ... ## Permutation testing ... ## ................................................................................................... ## ................................................................................................... ## ................................................................................................... ## ................................................................................................... ## ................................................................................................... ## ................................................................................................... ## Completed! The output shows the raw and adjusted p-values for clinical status. zicoseq_res &lt;- cbind.data.frame(p.raw = zicoseq_out$p.raw, p.adj.fdr = zicoseq_out$p.adj.fdr) zicoseq_res %&gt;% filter(p.adj.fdr &lt; 0.05) %&gt;% head() %&gt;% knitr::kable() p.raw p.adj.fdr Genus:Faecalibacterium 0.002 0.0167 Genus:[Clostridium]_innocuum_group 0.003 0.0206 Genus:[Ruminococcus]_gauvreauii_group 0.001 0.0005 Order:Bacteroidales 0.004 0.0167 Genus:Ruminococcus_2 0.003 0.0264 Genus:Ruminococcaceae_UCG-014 0.004 0.0316 11.4 Additional resources DAA can be performed by several means. Although most of them provide similar functionality, some may be more suitable than others given a certain study design or data type. Commonly used DAA tools include: ALDEx2 (Gloor, Macklaim, and Fernandes 2016) ANCOM (Mandal et al. 2015) ANCOMBC (Lin and Peddada 2020) corncob (Martin, Witten, and Willis 2021) DACOMP (Brill, Amnon, and Ruth 2022) DESeq2 (Love, Huber, and Anders 2014) eBay (T. Liu, Zhao, and Wang 2020) edgeR (Y. Chen, Lun, and Smyth 2016) fastANCOM (C. Zhou et al. 2022) LDM (Hu and Satten 2020) lefser (Khleborodova 2021) limma (Ritchie et al. 2015) LinDA (H. Zhou et al. 2022) MaAsLin2 (Mallick, Rahnavard, and McIver 2020) metagenomeSeq (Paulson, Talukder, and Bravo 2017) Omnibus (J. Chen et al. 2018) RAIDA (Sohn, Du, and An 2015) t-test Wilcoxon test ZicoSeq (Yang and Chen 2022) ZINQ (Ling et al. 2021) "],["network-learning.html", "Chapter 12 Network learning and analysis 12.1 Network learning 12.2 Network analysis with igraph 12.3 Network analysis with NetCoMi 12.4 Network visualization 12.5 Which method(s) to choose? 12.6 More about association measures 12.7 Comparison of association measures", " Chapter 12 Network learning and analysis .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } Learning and analyzing microbial association networks is another common exploratory data analysis approach aimed at understanding the complex interplay of microbial communities in their natural habitat. Microbial networks consist of nodes, representing microbial species or taxa, and edges, expressing their association. The mathematical representation of a network is the adjacency matrix, which has a non-zero entry for each edge in the network. A typical workflow for estimating a microbial network involves several steps, including data preprocessing, estimating microbial associations, and transforming them into edge weights. The resulting network can be analyzed using local network properties, such as centrality measures, or global measures that describe the overall structure of the network. Network plots provide further insight into the microbial community structure and allow for exploratory analysis of microbial relationships. In this chapter, we go through the complete workflow of constructing and analyzing a single microbial network, step by step. A potential next analysis task is to compare networks between groups, such as patients and controls, different environmental conditions, or different time points. How to compare networks between two groups is explained in chapter 13 12.1 Network learning 12.1.1 Typical workflow Figure 12.1 shows the workflow for learning/constructing a microbial association network as proposed by Peschel et al. (2021). The respective steps are explained below. Figure 12.1: The typical input is a \\(p\\) x \\(n\\) dimensional count matrix coming from a sequencing process, where \\(n\\) is the number of samples and \\(p\\) the number of features / ASVs / OTUs. Steps 1 through 6 are explained below. Each matrix resulting from steps 4, 5, and 6 plays a specific role in the final network: The adjacency matrix is used for edge colors, dissimilarity for layout, and similarity for edge weights. In weighted networks, the similarity matrix equals the adjacency matrix. Zero replacement: Since the following steps usually require non-zero entries in the read count matrix, zero counts must be replaced. A simple solution is to add a pseudo count to the data. Other possible approaches are implemented in the R package zCompositions Normalization: To avoid compositional effects, the data are normalized using a compositionality aware transformation. A common approach is the centered log-ratio (clr) transformation, which moves the data from a \\(p\\)-dimensional simplex to Euclidean space so that standard statistical analysis methods are valid. A variance stabilizing transformation (vst) is also a suitable approach for normalizing microbial count data (Badri et al. 2020). Association estimation: This is the crucial step in network learning to obtain statistical relations between the taxa. Common association measures include correlation, conditional dependence (which we will equate to partial correlation), and proportionality. Further information on these three types of association and their application can be found in Section 12.6. The following list gives a selection of compositionality aware approaches: Compositionality aware correlation estimation methods: Pearson’s correlation coefficient (+ normalization) Spearman’s rank correlation coefficient (+ normalization) Covariance shrinkage (corpcor package) (+ normalization) SparCC (implemented in SpiecEasi); applied in Section 12.7.1 CCREPE (ccrepe package) CCLasso (R code on GitHub) Compositionality aware measures of conditional dependence / partial correlation: SpiecEasi with Meinshausen and Bühlmann (MB) neighborhood selection; applied in Section 12.7.3 SpiecEasi with the graphical lasso (glasso) gCoda (R code on GitHub) SPRING; applied in Section 12.1.5 Proportionality measures (proportionality aware by definition): propr Shrinkage proportionality estimator; applied in Section 12.7.2 Sparsification: Transforming the estimated associations directly into adjacencies would lead to a dense network where all nodes are connected and only weighted network measures are meaningful. Therefore, the association matrix is usually sparsified to select edges of interest. A common sparsification approach for correlations is thresholding, where correlations with a magnitude below the threshold are set to zero. Another possibility is a statistical test (Student’s t-test or permutation test) with the null hypothesis that the correlation is equal to zero. SpiecEasi uses the StARS stability selection approach (H. Liu, Roeder, and Wasserman 2010) to decide on an appropriate sparsification level of the inferred conditional dependence graph. Transformation into dissimilarity: A common next step is to simply use the absolute values of the sparsified associations as edge weights. In this way, correlations of high magnitude (both positive and negative) will have a high edge weight. From a biological point of view, it would also make sense to assign a low edge weight to taxa that are strongly negatively associated, which would correspond to a high dissimilarity value. Here we follow Dongen and Enright (2012) to directly transform the sparse associations \\(r_{ij}^*\\) into dissimilarities, which can later be used for shortest path network measures. Depending on the desired handling of negative associations, one of the two proposed transformations should be chosen: 5a: “signed”: \\(d_{ij} = \\sqrt{0.5(1-r^*_{ij})}\\), where strongly negatively associated taxa have the largest distance and are placed further apart in the network. 5b: “unsigned”: \\(d_{ij} = \\sqrt{1-{r_{ij}^*}^2}\\), resulting in a small distance between strongly associated taxa (regardless of the sign). Transformation into similarity / edge weight: Finally, the dissimilarities are transformed into similarities by \\(s_{ij} = 1 - d_{ij}\\), which are used as edge weights. Thus, the similarity matrix is equal to the adjacency matrix in a weighted network. The main association measure used in this chapter is the SPRING (“Semi-Parametric Rank-based approach for INference in Graphical model”) method proposed by Yoon, Gaynanova, and Müller (2019). SPRING learns conditional dependency graphs for compositional data and follows the neighborhood selection method introduced by Meinshausen and Bühlmann (2006) (“MB”). We will show how to apply the method directly, as well as how to use it in conjunction with the R package NetCoMi, which is specifically designed for the construction and analysis of networks for microbiome data. See Section 12.6 for a comparison of all three association types (correlation, partial correlation, and proportionality) with more information on each measure and applications. We demonstrate the workflow using the the PeerJ data set (Potbhare et al. 2022). It contains skin microbial profiles of 58 subjects. library(mia) data(&quot;peerj13075&quot;, package = &quot;mia&quot;) tse0 &lt;- peerj13075 dim(tse0) ## [1] 674 58 12.1.2 Install packages Three packages used in this chapter are available on GitHub only: SpiecEasi, SPRING, and NetCoMi. We recommend that you install these packages before proceeding. if(!require(SpiecEasi)){ devtools::install_github(&quot;zdk123/SpiecEasi&quot;) } if(!require(SPRING)){ devtools::install_github(&quot;GraceYoon/SPRING&quot;) } if(!require(NetCoMi)){ devtools::install_github(&quot;stefpeschel/NetCoMi&quot;, force = TRUE, ref = &quot;TSE&quot;, dependencies = c(&quot;Depends&quot;, &quot;Imports&quot;, &quot;LinkingTo&quot;), repos = c(&quot;https://cloud.r-project.org/&quot;, BiocManager::repositories())) } 12.1.3 Data preparation Before applying the network learning methods, we perform some data preparation steps: Aggregation to genus level Add relative abundance assay Prevalence filtering (keep genera with prevalence &gt; 20%) Add assay with log10 transformed abundances Add assay with clr transformed abundances # Agglomerate to genus level tse &lt;- agglomerateByRank(tse0, rank = &quot;genus&quot;) # Add relative abundances tse &lt;- transformAssay(tse, assay.type = &quot;counts&quot;, method = &quot;relabundance&quot;, MARGIN = &quot;samples&quot;) # Filter by prevalence tse &lt;- subsetByPrevalentFeatures(tse, prevalence = 0.2, detection = 0, assay.type = &quot;relabundance&quot;) # Add log10-transformed abundances tse &lt;- transformAssay(tse, method = &quot;log10&quot;, pseudocount = 1) # Add clr-transformed abundances tse &lt;- transformAssay(tse, method = &quot;clr&quot;, pseudocount = 1) dim(tse) ## [1] 147 58 12.1.4 SPRING network As explained in Section 12.1.1, we use SPRING (“Semi-Parametric Rank-based approach for INference in Graphical model”) as association measure. We first use the SPRING function directly to construct a conditional dependency graph. Neither zero replacement nor normalization (steps 1 and 2 in our workflow) are required because SPRING uses a modified clr (mclr) transformation that can handle zero counts, and the correlation estimation method itself can also deal with zeros in the data. mclr is similar to the clr transformation except that mclr considers only the non-zero values. More precisely, the geometric mean is derived from positive values only, and zero counts remain zero after the transformation. This approach is similar to the “robust clr” (rclr) transformation included in the vegan package, except that mclr applies a positive shift to all non-zero values to make them strictly positive. See (Yoon, Gaynanova, and Müller 2019) for details. The Rmethod argument is set to “approx” to estimate the correlations using a hybrid multi-linear interpolation approach proposed by Yoon, Müller, and Gaynanova (2021). This method considerably reduces the runtime while controlling the approximation error. SPRING uses the StARS (“Stability Approach to Regularization Selection”) method (H. Liu, Roeder, and Wasserman 2010) to obtain a sparse association matrix. Thus, also step 4 of our workflow is already included. We set the StARS threshold to 0.05 to get a sparser graph. library(SPRING) set.seed(13075) spring_est &lt;- SPRING(t(assay(tse, &quot;counts&quot;)), Rmethod = &quot;approx&quot;, thresh = 0.05, lambdaseq = &quot;data-specific&quot;) # Get index of the optimal lambda selected by StARS opt.K &lt;- spring_est$output$stars$opt.index # Store partial correlation matrix belonging to the optimal lambda as matrix spring_cor &lt;- SpiecEasi::symBeta(as.matrix(spring_est$output$est$beta[[opt.K]])) spring_cor &lt;- as.matrix(spring_cor) rownames(spring_cor) &lt;- colnames(spring_cor) &lt;- rownames(tse) diag(spring_cor) &lt;- 1 As explained in Section 12.1.1, the estimated associations are sparsified, transformed into dissimilarities, and finally transformed into similarities, which are the adjacency values. We write a function for these steps, which will be reused later. Since SPRING already includes a sparsification approach, the thresh argument is not needed here, but will be needed in Section 12.6 for other association measures. To be consistent with the workflow, we provide two dissimilarity transformations: “signed” and “unsigned” (see Section 12.1.1 for an explanation). These transformations were introduced by Dongen and Enright (2012). We use the “signed” transformation in our examples so that strongly negatively associated genera have low edge weights. The output of the function is an igraph object, which can be plotted and analyzed using functions from the igraph package. # Arguments: # - assoMat: association matrix # - threshold: associations below the threshold are set to zero # - dissTrans: dissimilarity transformation (&quot;signed&quot; or &quot;unsigned&quot;) transform_asso &lt;- function(assoMat, thresh = NULL, dissTrans = &quot;signed&quot;) { # Sparsification if (!is.null(thresh)) { assoMat[abs(assoMat) &lt; thresh] &lt;- 0 } # Compute dissimilarity matrix if (dissTrans == &quot;signed&quot;) { dissMat &lt;- sqrt(0.5 * (1 - assoMat)) } else { dissMat &lt;- sqrt(1 - assoMat^2) } # Dissimilarity between nodes with zero correlation is set to 1 # (these nodes are unconnected and thus should have maximum dissimilarity) dissMat[assoMat == 0] &lt;- 1 # Compute similarity matrix simMat &lt;- 1 - dissMat # Turn into igraph object graphObj &lt;- SpiecEasi::adj2igraph(simMat) return(list(graph = graphObj, adja = simMat, asso = assoMat, diss = dissMat)) } # Create graph object spring_graph &lt;- transform_asso(spring_cor)$graph 12.1.5 NetCoMi network The NetCoMi (Peschel et al. 2021) package is specifically designed to construct, analyze, and compare networks for microbiome data and implements the complete workflow described in Section 12.1.1. Instead of using several functions for each of the steps, NetCoMi provides a single function for network construction (netConstruct()), so the package streamlines the workflow considerably. The user can choose from a variety of methods for data preprocessing, association estimation, sparsification, and transformation. The returned microNet object can then be passed to netAnalyze() (the network analysis function) so that all necessary information is available for the network analysis workflow. library(NetCoMi) We again use SPRING as one of the association measures available in NetCoMi to construct a conditional dependency graph. To demonstrate how taxa are filtered with netConstruct(), we will use the unfiltered tse object this time. The filtering is the same as before: Taxa occurring in less than 20% of the samples are removed. netcomi_net &lt;- netConstruct(tse, taxRank = &quot;genus&quot;, filtTax = &quot;numbSamp&quot;, filtTaxPar = list(numbSamp = 0.2), measure = &quot;spring&quot;, measurePar = list(thresh = 0.05, Rmethod = &quot;approx&quot;), sparsMethod = &quot;none&quot;, dissFunc = &quot;signed&quot;, seed = 13075) netConstruct() returns an object of the class microNet, which contains all matrices generated during network construction. The object also contains an edge list, giving each edge’s estimated association, dissimilarity, and adjacency. Let’s take a quick look at the edges with the highest and lowest edge weights: edgelist &lt;- netcomi_net$edgelist1[order(netcomi_net$edgelist1$adja, decreasing = TRUE), ] head(edgelist) ## v1 v2 asso diss adja ## 73 Citrobacter Escherichia 0.3682 0.5621 0.4379 ## 63 Buttiauxella Serratia 0.2426 0.6154 0.3846 ## 69 Chitinivibrio Pseudogracilibacillus 0.2203 0.6244 0.3756 ## 19 Algicola Siccibacter 0.2193 0.6248 0.3752 ## 111 Haliangium Marinobacter 0.2148 0.6266 0.3734 ## 143 Planomicrobium Virgibacillus 0.2006 0.6322 0.3678 tail(edgelist) ## v1 v2 asso diss adja ## 132 Mycobacterium Salinibacillus 0.0017483 0.7065 0.2935 ## 24 Amphritea Providencia 0.0014085 0.7066 0.2934 ## 102 Erwinia Siccibacter 0.0013114 0.7066 0.2934 ## 116 Holophaga Methylarcula 0.0007828 0.7068 0.2932 ## 17 Algicola Lysobacter 0.0005191 0.7069 0.2931 ## 95 Enterobacter Janibacter -0.0013921 0.7076 0.2924 As before, the adjacency matrix is converted into an igraph object. Further steps like sparsification and transformation are not necessary because they are done internally by netConstruct(). netcomi_graph &lt;- SpiecEasi::adj2igraph(abs(netcomi_net$adjaMat1)) 12.2 Network analysis with igraph The computed network is now analyzed using appropriate methods. We will first use the igraph package to analyze the SPRING network. NetCoMi’s netAnalyze() function will be used later to analyze the constructed microNet object. 12.2.1 Network plot To get an overview of the network structure, a first common analysis method is to plot the network. We here use the igraph package, which is a state-of-the-art package for network analysis and visualization. Other packages that could be used for network plotting are the qgraph package or the ggnet2 package. Since we will use igraph for network analysis later on, we are using its plotting function here as well. We use the Fruchterman-Reingold layout (a force-directed layout) for node placement. By placing strongly connected nodes close together and those with low edge weight far apart, this layout results in an easy-to-read network plot. The node sizes are proportional to a taxon’s log10-transformed abundance, which we previously added to the tse object, averaged across all samples. The values are rescaled to be visually distinguishable. Since we created two graph objects, one with SPRING and one with NetCoMi, we plot them side by side. The two plots should be identical. library(igraph) # Node sizes vsize &lt;- (colMeans(t(assay(tse, &quot;log10&quot;))) + 1) * 3 # Fruchterman-Reingold layout from igraph package set.seed(13075) lay_fr &lt;- layout_with_fr(spring_graph) par(mfrow = c(1,2)) plot(spring_graph, layout = lay_fr, vertex.size = vsize, vertex.label = NA, main = &quot;SPRING network&quot;) plot(netcomi_graph, layout = lay_fr, vertex.size = vsize, vertex.label = NA, main = &quot;NetCoMi network\\n(with SPRING associations)&quot;) 12.2.2 Centrality measures Centrality measures express the importance of nodes within the network. Common measures are the degree, betweenness, closeness, and eigenvector centrality. The igraph package provides functions to compute these measures. We wrap a function around the code to reuse it later. get_centr &lt;- function(graph_obj) { # We access igraph directly with &quot;::&quot; because there are more packages loaded in # this chapter that contain a degree() function. df &lt;- data.frame(Degree = igraph::degree(graph_obj)) df$Betweenness &lt;- betweenness(graph_obj) df$Closeness &lt;- closeness(graph_obj, normalized = TRUE) df$Eigenvector &lt;- eigen_centrality(graph_obj)$vector return(df) } centr_df &lt;- get_centr(spring_graph) rownames(centr_df) &lt;- rownames(spring_cor) head(centr_df, 15) ## Degree Betweenness Closeness Eigenvector ## Abyssicoccus 0 0 NaN 9.278e-18 ## Acidaminococcus 2 0 0.6464 1.862e-01 ## Acinetobacter 3 5 0.5544 1.257e-01 ## Actinomyces 0 0 NaN 9.278e-18 ## Actinoplanes 2 99 0.5104 1.099e-02 ## Aerococcus 0 0 NaN 9.278e-18 ## Aeromonas 4 351 0.7477 2.023e-01 ## Agromyces 6 435 0.7733 5.293e-01 ## Algicola 4 264 0.7206 1.500e-01 ## Alicyclobacillus 0 0 NaN 9.278e-18 ## Alteribacillus 0 0 NaN 9.278e-18 ## Ammoniibacillus 1 0 0.5064 5.419e-02 ## Amphritea 5 382 0.6256 1.335e-02 ## Amycolatopsis 1 0 0.6117 1.338e-01 ## Anaerococcus 2 392 0.4064 1.288e-03 The closeness centrality is “NaN” for some genera. These are unconnected nodes, as can be seen by the zero degree and betweenness centrality. 12.2.3 Scale node sizes by centrality Centrality measures can be visualized in the network plot by scaling the node sizes according to one of these measures. We plot the Spring graph using the same layout as before and with the node sizes scaled according to all four centrality measures. Of the four centrality measures, only the degree has a range suitable to be used as node size. The other centrality measures must be rescaled because their range is either too small or too large. The following scaling is a suggestion that works for this example. The values might be adapted for other data sets. get_vsizes &lt;- function(centr_df) { df &lt;- as.matrix(centr_df) df[, &quot;Betweenness&quot;] &lt;- log(df[, &quot;Betweenness&quot;]) df[, &quot;Closeness&quot;] &lt;- df[, &quot;Closeness&quot;] * 10 df[, &quot;Eigenvector&quot;] &lt;- df[, &quot;Eigenvector&quot;] * 10 df[is.infinite(df) | is.na(df)] &lt;- 0 return(df) } vsize_df &lt;- get_vsizes(centr_df ) head(vsize_df) ## Degree Betweenness Closeness Eigenvector ## Abyssicoccus 0 0.000 0.000 9.278e-17 ## Acidaminococcus 2 0.000 6.464 1.862e+00 ## Acinetobacter 3 1.609 5.544 1.257e+00 ## Actinomyces 0 0.000 0.000 9.278e-17 ## Actinoplanes 2 4.595 5.104 1.099e-01 ## Aerococcus 0 0.000 0.000 9.278e-17 par(mfrow = c(2,2)) for (i in seq_along(centr_df)) { plot(spring_graph, layout = lay_fr, vertex.size = vsize_df[, i], vertex.label = NA, main = colnames(centr_df)[i]) } We observe that the two-node component at the bottom has a much higher closeness centrality than the nodes belonging to the main component of the network. Obviously, closeness centrality as commonly defined is misleading when the network consists of disconnected components. Nodes belonging to smaller components are seen as closer to others than in larger components. To overcome this problem, centrality values, and especially closeness centrality, are often calculated only for the largest connected component (LCC), which we will do below. # Extract the LCC dg_net &lt;- igraph::decompose.graph(spring_graph) idx_lcc &lt;- which.max(unlist(lapply(dg_net, function(x) length(igraph::V(x))))) lcc &lt;- dg_net[[idx_lcc]] # Compute centrality values for the LCC centr_df_lcc &lt;- get_centr(lcc) # Replace centrality values by those for LCC and set all others to zero lcc_nodes &lt;- as.numeric(rownames(centr_df_lcc)) centr_df[lcc_nodes, ] &lt;- centr_df_lcc centr_df[-lcc_nodes, ] &lt;- 0 # Node/vertex sizes vsize_df &lt;- get_vsizes(centr_df) par(mfrow = c(2,2)) for (i in seq_along(centr_df)) { plot(spring_graph, layout = lay_fr, vertex.size = vsize_df[, i], vertex.label = NA, main = colnames(centr_df)[i]) } Note that NetCoMi follows a different approach to overcome this problem. NetCoMi uses the definition of closeness centrality proposed by Tore Opsahl, which is well defined even for disconnected networks and assigns higher closeness centrality values to nodes in larger components. This is more intuitive because nodes in a larger component are connected to a larger number of other nodes than in small components. 12.2.4 Degree distribution The degree distribution is another popular measure that expresses the probability distribution of degrees over the entire network. It thus provides insight into the overall network structure. We plot the degree distribution for all four association estimation methods to compare the network structure. library(ggplot2) # Compute degree distribution ddist&lt;- igraph::degree.distribution(spring_graph) # Data frame needed for ggplot2 df &lt;- data.frame(Degree = as.factor((seq_along(ddist)) - 1), Fraction = ddist) ggplot(data = df, aes(x = Degree, y = Fraction, group = 1)) + geom_line() + geom_point() + theme_bw() The network has a large number of singletons and sparsely connected nodes, and only a small number of nodes with a higher degree of 7 or more. 12.2.5 Clustered heatmaps Using the ComplexHeatmap package, we plot a heatmap of the association matrix estimated with SPRING. Rows and columns are sorted according to the clusters identified via hierarchical clustering. library(ComplexHeatmap) library(circlize) We select the 50 nodes with the highest sum of edge weights to get a smaller heatmap. sel &lt;- names(sort(rowSums(spring_cor), decreasing = TRUE))[seq_len(50)] adja_sel &lt;- spring_cor[sel, sel] # Color vector col &lt;- colorRamp2(c(-1, -0.5, 0, 0.5, 1), c(&quot;royalblue4&quot;, &quot;lightblue&quot;, &quot;white&quot;, &quot;orange&quot;, &quot;firebrick3&quot;)) Heatmap(adja_sel, col = col, rect_gp = gpar(col = &quot;gray&quot;, lwd = 1), show_row_names = FALSE, show_column_names = FALSE, name = &quot;Association&quot;) The associations are generally quite low, and there are no prominent clusters detected by hierarchical clustering. 12.2.6 Global network measures Global measures describe the overall network structure. We take a look at three common measures: density, transitivity, and average path length. The values are again computed with igraph functions. 12.2.6.1 Density Definition: Proportion of present edges from all possible edges. edge_density(spring_graph) ## [1] 0.01444 12.2.6.2 Transitivity (clustering coefficient) Here, we consider only the global clustering coefficient, which is defined as the ratio of triangles to connected triples. transitivity(spring_graph) ## [1] 0.1456 12.2.6.3 Average path length Definition: Mean of the shortest distance between each pair of nodes. average.path.length(spring_graph) ## [1] 1.699 12.3 Network analysis with NetCoMi The netcomi_net object of class microNet created before is now passed to netAnalyze() to perform network analysis with NetCoMi. The function computes several common network characteristics such as centrality measures, cluster assignment, the graphlet correlation matrix, as well as global network measures. The user has several options to choose from, such as a clustering method, how to define hubs, and whether or not to normalize centrality values. See the help page ?netAnalyze for a description of the arguments. By default, a heatmap of the Graphlet Correlation Matrix (GCM) is returned (with graphlet correlations in the upper triangle and significance codes resulting from Student’s t-test in the lower triangle). See ?calcGCM and ?testGCM for details. netcomi_netprops &lt;- netAnalyze(netcomi_net, clustMethod = &quot;cluster_fast_greedy&quot;, hubPar = &quot;eigenvector&quot;, normDeg = FALSE) summary(netcomi_netprops, numbNodes = 5) ## ## Component sizes ## ``````````````` ## size: 103 2 1 ## #: 1 1 42 ## ______________________________ ## Global network properties ## ````````````````````````` ## Largest connected component (LCC): ## ## Relative LCC size 0.70068 ## Clustering coefficient 0.21253 ## Modularity 0.70495 ## Positive edge percentage 99.35065 ## Edge density 0.02932 ## Natural connectivity 0.01173 ## Vertex connectivity 1.00000 ## Edge connectivity 1.00000 ## Average dissimilarity* 0.99084 ## Average path length** 3.81101 ## ## Whole network: ## ## Number of components 44.00000 ## Clustering coefficient 0.21253 ## Modularity 0.70787 ## Positive edge percentage 99.35484 ## Edge density 0.01444 ## Natural connectivity 0.00782 ## ----- ## *: Dissimilarity = 1 - edge weight ## **: Path length = Units with average dissimilarity ## ## ______________________________ ## Clusters ## - In the whole network ## - Algorithm: cluster_fast_greedy ## ```````````````````````````````` ## ## name: 0 1 2 3 4 5 6 7 8 9 10 ## #: 42 16 11 14 20 9 17 8 5 3 2 ## ## ______________________________ ## Hubs ## - In alphabetical/numerical order ## - Based on empirical quantiles of centralities ## ``````````````````````````````````````````````` ## Agromyces ## Aneurinibacillus ## Anoxybacillus ## Chitinivibrio ## Erwinia ## Geobacillus ## Janibacter ## Pseudogracilibacillus ## ## ______________________________ ## Centrality measures ## - In decreasing order ## - Centrality of disconnected components is zero ## ```````````````````````````````````````````````` ## Degree (unnormalized): ## ## Anoxybacillus 10 ## Erwinia 10 ## Chitinivibrio 9 ## Janibacter 8 ## Escherichia/Shigella 7 ## ## Betweenness centrality (normalized): ## ## Janibacter 0.3941 ## Chitinivibrio 0.3137 ## Enterobacter 0.2780 ## Buttiauxella 0.2454 ## Erwinia 0.2095 ## ## Closeness centrality (normalized): ## ## Chitinivibrio 0.5247 ## Janibacter 0.5067 ## Anoxybacillus 0.4841 ## Pseudogracilibacillus 0.4659 ## Erwinia 0.4653 ## ## Eigenvector centrality (normalized): ## ## Anoxybacillus 1.0000 ## Chitinivibrio 0.9831 ## Pseudogracilibacillus 0.8657 ## Janibacter 0.8361 ## Aneurinibacillus 0.6890 Interpretation of some findings: The largest connected component (LCC) has 103 nodes and the network contains 42 singletons. 10 clusters have been identified, containing 2 to 20 nodes. There are 8 hub nodes detected, which by definition are the nodes with the highest eigenvector centrality. The average path length in the LCC is 3.811. This means that on average it takes 3.811 steps (step length is the average dissimilarity) to get from one node to another. Note that the average path length in NetCoMi is defined differently than in the igraph package, which is why the values differ. Low values of edge density and the connectivity measures indicate that the network is rather sparse and not robust to perturbations (i.e., removal of nodes or edges). 12.4 Network visualization Further insight into the network structure can be gained by visualizing the network. We have already seen examples of how to plot a network using the igraph package. Here we will use NetCoMi’s plot function. It takes as input the microNetProps object returned by netAnalyze(), which contains all computed network properties. This has the advantage that the user can choose which properties to plot by simply changing some arguments. The plot function is based on qgraph, which is another state-of-the-art R package for network visualization. The help page can be accessed via ?plot.microNetProps. 12.4.1 Highlight node properties In the first plot, node colors represent the detected clusters and node sizes are scaled by eigenvector centrality. Hub nodes are highlighted by default. Singletons are not included in the plot. To improve the readability, NetCoMi’s “intelligent” label shortening approach is used. Note that nodes are sometimes placed too close together so that the labels overlap. You may need to play around with the repulsion argument until you find a value where the labels are legible, but also the clusters are still well recognizable. plot(netcomi_netprops, repulsion = 0.98, rmSingles = TRUE, shortenLabels = &quot;intelligent&quot;, labelScale = FALSE, nodeSize = &quot;eigenvector&quot;, nodeSizeSpread = 3, nodeColor = &quot;cluster&quot;, hubBorderCol = &quot;gray40&quot;, cexNodes = 1.8, edgeTranspHigh = 20, title1 = &quot;Network properties highlighted&quot;, showTitle = TRUE, cexTitle = 2.3, mar = c(1, 3, 4, 8)) legend(0.7, 1.1, cex = 1.7, title = &quot;estimated correlation:&quot;, legend = c(&quot;+&quot;,&quot;-&quot;), lty = 1, lwd = 3, col = c(&quot;#009900&quot;,&quot;red&quot;), bty = &quot;n&quot;, horiz = TRUE) 12.4.2 Highlight data features We now color nodes according to their phylum. The node sizes are proportional to a taxon’s sum of mclr-transformed abundances. As already mentioned in Section 12.1.4, this is the normalization method used by SPRING. A color palette from RColorBrewer is used here. library(RColorBrewer) # Generate vector with phylum names for node coloring phyla &lt;- as.factor(rowData(tse)$phylum) names(phyla) &lt;- rowData(tse)$genus # Create color vector colvec &lt;- RColorBrewer::brewer.pal(length(levels(phyla)), &quot;Set3&quot;) plot(netcomi_netprops, repulsion = 0.98, rmSingles = TRUE, shortenLabels = &quot;intelligent&quot;, labelScale = FALSE, nodeSize = &quot;mclr&quot;, nodeColor = &quot;feature&quot;, featVecCol = phyla, colorVec = colvec, nodeTransp = 20, highlightHubs = FALSE, cexNodes = 1.8, edgeTranspHigh = 20, title1 = &quot;Data features highlighted&quot;, showTitle = TRUE, cexTitle = 2.3, mar = c(1, 10, 4, 6)) # Add legends legend(0.7, 1.1, cex = 1.7, title = &quot;estimated correlation:&quot;, legend = c(&quot;+&quot;,&quot;-&quot;), lty = 1, lwd = 3, col = c(&quot;#009900&quot;,&quot;red&quot;), bty = &quot;n&quot;, horiz = TRUE) # Colors used in the legend should be equally transparent as in the plot col_transp &lt;- colToTransp(colvec, 20) legend(-1.8, 1.1, cex = 1.7, pt.cex = 2.5, title = &quot;Phylum:&quot;, legend=levels(phyla), col = col_transp, bty = &quot;n&quot;, pch = 16) A few things to observe: Genera belonging to the same phylum tend to cluster together, though not perfectly. Genera with a low total count play a rather unimportant role in the network, i.e., they have a low centrality. There is only one negative edge in the network. This edge is between two clusters, as expected when using the “signed” transformation. 12.5 Which method(s) to choose? Throughout all the steps from primary data to potentially significant network features, there is a variety of methods and parameters to choose from. However, there is no general consensus in the community on the “right” way to estimate and analyze microbial networks. In the absence of a “best method” for inferring and analyzing microbial networks, researchers may be tempted to try different methods and report only the optimal results or those that fit some prior knowledge. This carries the risk of “overfitting” the analysis to the existing data so that the results are not replicable for new data (Ullmann et al. 2023). Therefore, the selection of the workflow building blocks should be set up once and independently of any hypothesis about the data, thus avoiding the fallacy of starting to “fish” for results that best fit a previously formulated hypothesis. For example, one should ask prior to the analysis whether correlation or conditional dependence as a measure of association better fits the research question and choose the method accordingly. Another example is the choice of transformation from estimated association to dissimilarity (i.e., “signed” or “unsigned”), which completely changes the interpretation and characteristics of the network. This choice should be made based on the research question before starting the analysis. 12.6 More about association measures As mentioned in the introduction of this chapter, there are three types of association measures that are commonly used to express relationships between taxa: correlation, conditional dependence, and proportionality. Below, we provide a brief explanation of each of these measures, along with lists of available compositionality-aware approaches. Correlation: Two popular measures of ecological association are Pearson’s correlation coefficient and Spearman’s rank correlation coefficient, both of which can be inferred from empirical (sample) covariances. However, in the \\(p\\gg n\\) setting, which most microbiome datasets are in, sample covariances and correlations are unreliable because the parameters being estimated are typically underdetermined. One way to improve sample covariance estimates is to assume that the underlying covariance matrix is sparse and use a regularized covariance estimator to implement this structural assumption. The Schäfer-Strimmer shrinkage estimator (Schäfer and Strimmer 2005) is one possible method for estimating a sparse correlation matrix. Other popular methods, especially designed to estimate correlations for compositional data, are SparCC (“Sparse Correlations for Compositional data”) by J. Friedman and Alm (2012), CCREPE (“Compositionality Corrected by REnormalization and PErmutation”) by Faust et al. (2012), and CCLasso (“Correlation inference for Compositional data through Lasso”) by Huaying Fang et al. (2015). The latter three methods already include a compositionality aware normalization, and SparCC also includes a zero replacement approach. Conditional dependence: Since standard correlations include both direct and indirect dependencies, conditional dependence or partial correlation is often preferred for measuring association. Unlike (marginal) correlation, it expresses the relationship between two features conditioned on all other features in the data set. The approach and R package SpiecEasi (“Sparse InversE Covariance estimation for Ecological Association and Statistical Inference”) by Kurtz et al. (2015) is specifically designed for inferring ecological networks from microbiome data and includes two approaches for estimating conditional dependence structures between taxa: Neighborhood Selection; short “MB” (Meinshausen and Bühlmann 2006) and (inverse) covariance selection (Jerome Friedman, Hastie, and Tibshirani 2008), which is based on a penalized maximum likelihood approach and is also known as “graphical lasso”. Another approach and R package for inferring partial correlations from microbiome data is SPRING (“Semi-Parametric Rank-based approach for INference in Graphical model”) by Yoon, Gaynanova, and Müller (2019). They also use the MB neighborhood selection method, but introduce a novel semi-parametric rank-based approach for sparse partial correlation estimation that can naturally handle the excess of zeros in the data. gCoda (H. Fang et al. 2017) is another conditional dependence measure based on penalized maximum likelihood estimation. All of the aforementioned conditional dependence measures address the high dimensionality of microbiome data. Proportionality: Lovell et al. (2015) introduce proportionality as an alternative measure of pairwise association for compositional data. The idea is that if the relative abundances between two taxa \\(i\\) and \\(j\\) are proportional, then their corresponding absolute abundances are also proportional: \\(\\frac{\\omega_i}{m} \\propto \\frac{\\omega_j}{m} \\Rightarrow \\omega_i \\propto \\omega_j\\), where \\(m\\) is the sum of counts in the sample. It follows that proportionality is identical for the observed (relative) read counts and the true unobserved counts. The proportionality measure proposed by Lovell et al. (2015) is based on log-ratio variance \\(var(log \\frac{x_i}{x_j})\\), which is zero when \\(\\omega_i\\) and \\(\\omega_j\\) are perfectly proportional. Proportionality is implemented in the R package propr. Badri et al. (2020) extend the proportionality measure to a so-called “shrinkage proportionality estimator”. It combines proportionality with the covariance shrinkage approach to obtain consistent association estimates even with small sample sizes. 12.7 Comparison of association measures In this section, we provide three additional examples for constructing a network using each of the three types of association: Correlation using SparCC Partial correlation using SpiecEasi Proportionality using the shrinkage proportionality measure 12.7.1 SparCC The first association measure we look at is SparCC (“Sparse Correlations for Compositional data”), introduced by J. Friedman and Alm (2012). It estimates Pearson correlations while taking into account the compositional structure of the data. The SpiecEasi package provides an implementation of this method. # Set seed for reproducibility set.seed(13075) # Compute correlation matrix sparcc_cor &lt;- SpiecEasi::sparcc(t(assay(tse, &quot;counts&quot;)))$Cor rownames(sparcc_cor) &lt;- colnames(sparcc_cor) &lt;- rownames(tse) We reuse the transform_asso() function created in Section 12.1.4, which sparsifies the association matrix, transforms it into a similarity matrix, and finally returns an igraph object. Two threshold values are used to see the effect of sparsification later in the network plot. sparcc_trans03 &lt;- transform_asso(sparcc_cor, thresh = 0.3) sparcc_trans04 &lt;- transform_asso(sparcc_cor, thresh = 0.4) sparcc_graph03 &lt;- sparcc_trans03$graph sparcc_graph04 &lt;- sparcc_trans04$graph 12.7.2 Shrinkage proportionality In the second example, microbial associations are measured by proportionality, originally introduced by Lovell et al. (2015). We use the shrinkage proportionality estimator proposed by Badri et al. (2020), which gives consistent results even for small sample sizes. Since there is no R package implementing this estimator, we use the rho_shrink_est() function provided in the GitHub repository associated with the paper. The function is slightly modified to take normalized counts as input. library(corpcor) # norm_counts: clr-transformed count matrix with samples in rows rho_shrink_est &lt;- function(norm_counts, ...) { shrunk_cov &lt;- cov.shrink(norm_counts, ...) p &lt;- ncol(norm_counts) J &lt;- matrix(rep(diag(shrunk_cov), p), p) rho &lt;- 2 * shrunk_cov / (J + t(J)) (rho + t(rho)) / 2 } # Apply the shrinkage proportionality estimator to the clr-transformed counts prop_est &lt;- as(rho_shrink_est(t(assay(tse, &quot;clr&quot;))), &quot;matrix&quot;) ## Estimating optimal shrinkage intensity lambda.var (variance vector): 0.0857 ## ## Estimating optimal shrinkage intensity lambda (correlation matrix): 0.3634 Again, we use our transformation function to convert the association matrix into a graph object. prop_trans &lt;- transform_asso(prop_est, thresh = 0.4) prop_graph &lt;- prop_trans$graph 12.7.3 SpiecEasi - MB As third example, we use the SpiecEasi (“Sparse InversE Covariance estimation for Ecological Association and Statistical Inference”) approach proposed by Kurtz et al. (2015) to estimate a sparse conditional dependency graph. The neighborhood selection method (“MB”) introduced by Meinshausen and Bühlmann (2006) is used for network learning. The approach is implemented in the R package SpiecEasi. library(SpiecEasi) set.seed(13075) se_mb_est &lt;- spiec.easi(t(assay(tse, &quot;counts&quot;)), method = &#39;mb&#39;, nlambda = 20, pulsar.params = list(rep.num = 20)) Since SpiecEasi uses the StARS (“Stability Approach to Regularization Selection”) method (H. Liu, Roeder, and Wasserman 2010) to obtain a sparse association matrix, we don’t need to set a threshold here. We store the partial correlations corresponding to the StARS-optimal lambda and convert them into an igraph object. # Get optimal matrix with partial correlations se_mb_cor &lt;- as.matrix(getOptBeta(se_mb_est)) se_mb_cor &lt;- as.matrix(symBeta(se_mb_cor)) rownames(se_mb_cor) &lt;- colnames(se_mb_cor) &lt;- rownames(tse) diag(se_mb_cor) &lt;- 1 # Create graph object se_mb_graph &lt;- transform_asso(se_mb_cor)$graph 12.7.4 Network plots The graph objects can now be plotted using the igraph package. The same layout is used in all four plots so that the networks are comparable. library(igraph) # Node sizes vsize &lt;- (colMeans(t(assay(tse, &quot;log10&quot;))) + 1) * 3 # Use Fruchterman-Reingold (force-directed) layout set.seed(13075) lay_fr &lt;- layout_with_fr(se_mb_graph) par(mfrow = c(2,2)) plot(sparcc_graph03, layout = lay_fr, vertex.size = vsize, vertex.label = NA, main = &quot;SparCC (thresh 0.3)&quot;) plot(sparcc_graph04, layout = lay_fr, vertex.size = vsize, vertex.label = NA, main = &quot;SparCC (thresh 0.4)&quot;) plot(prop_graph, layout = lay_fr, vertex.size = vsize, vertex.label = NA, main = &quot;Shrinkage proportionality\\n(thresh 0.4)&quot;) plot(se_mb_graph, layout = lay_fr, vertex.size = vsize, vertex.label = NA, main = &quot;SpiecEasi (MB)&quot;) A few observations: The density of SparCC (threshold 0.4), proportionality and SpiecEasi is comparable, while the SparCC correlation network with threshold 0.3 is much denser. However, there are edges in the proportionality and SpiecEasi networks that are not present in the two SparCC networks. The SpiecEasi network has less highly connected nodes than the other three networks, but more nodes with one or two connections. We will look at the degree distribution in the next section to quantify these observations. 12.7.5 Network analysis Here we repeat some of the network analysis approaches explained in Section 12.2. The analyses are performed simultaneously for the three association measures as well as the SPRING network constructed in Section 12.1.4. Therefore, we start by creating a list of all the graph objects we need for the analyses. graphlist &lt;- list(SparCC = sparcc_graph04, Proportionality = prop_graph, SpiecEasi = se_mb_graph, SPRING = spring_graph) 12.7.5.1 Degree distribution The degree distribution is plotted for all four measures to compare the overall network structure. library(ggplot2) # Compute degree distributions ddlist &lt;- lapply(graphlist, igraph::degree.distribution) # Maximum degree maxdeg &lt;- max(lengths(ddlist)) # Make list elements the same length for(i in seq_along(graphlist)) { length(ddlist[[i]]) &lt;- maxdeg } # Data frame needed for ggplot2 df &lt;- data.frame(Degree = rep(seq_len(maxdeg), length(graphlist)), Fraction = unlist(ddlist), Method = rep(names(graphlist), each = maxdeg)) ggplot(df, aes(x = Degree, y = Fraction, group = Method)) + geom_line(aes(color = Method)) + geom_point(aes(color = Method)) + theme_bw() The SparCC and shrinkage proportionality networks have a considerably higher proportion of singletons (zero-degree nodes) than the two conditional dependency graphs, but a lower proportion of nodes with degrees between one and five. The SpiecEasi and SPRING graphs, on the other hand, have a higher proportion of low degree nodes, but no highly connected nodes with a degree greater than eleven. 12.7.5.2 Clustered heatmaps Using the ComplexHeatmap package, we plot heatmaps of the association matrices for the four considered association measures. Rows and columns are sorted according to the clusters identified via hierarchical clustering. library(ComplexHeatmap) library(circlize) library(patchwork) For each association measure, we select the 50 nodes with the highest sum of edge weights. # Function for selecting taxa with highest sum of edge weights select_taxa &lt;- function(adja, ntaxa = 50) { sel &lt;- names(sort(rowSums(adja), decreasing = TRUE))[seq_len(ntaxa)] adja[sel, sel] } assolist &lt;- list() assolist$SparCC &lt;- select_taxa(sparcc_trans04$adja) assolist$Proportionality &lt;- select_taxa(prop_trans$adja) assolist$SpiecEasi &lt;- select_taxa(se_mb_cor) assolist$SPRING &lt;- select_taxa(spring_cor) # Color vector for the legend col &lt;- colorRamp2(c(-1, -0.5, 0, 0.5, 1), c(&quot;royalblue4&quot;, &quot;lightblue&quot;, &quot;white&quot;, &quot;orange&quot;, &quot;firebrick3&quot;)) hm_list &lt;- list() for(i in seq_along(assolist)) { if (i %in% c(2, 4)) { showlegend &lt;- TRUE } else { showlegend &lt;- FALSE } hm_list[[i]] &lt;- Heatmap(assolist[[i]], col = col, rect_gp = gpar(col = &quot;gray&quot;, lwd = 1), show_row_names = FALSE, show_column_names = FALSE, column_title = names(assolist)[i], name = &quot;Association&quot;, show_heatmap_legend = showlegend) %&gt;% draw() %&gt;% grid.grabExpr() } # Plot with wrap_plots() function from patchwork package wrap_plots(hm_list, ncol = 2, widths = c(8, 10, 8, 10)) The SparCC and the proportionality network show a block structure, where each block corresponds to a cluster. The clusters are less pronounced in the conditional dependence networks. The latter also generally have lower edge weights. 12.7.5.3 Global network measures For each association measure, the three global network measures density, transitivity, and average path length are computed and stored in a data frame for comparison. # Compute density and store in a data frame glob &lt;- data.frame(Density = unlist(lapply(graphlist, edge_density))) # Transitivity glob$Transitivity &lt;- unlist(lapply(graphlist, transitivity)) # Average path length glob$Av.path &lt;- unlist(lapply(graphlist, average.path.length)) glob ## Density Transitivity Av.path ## SparCC 0.01510 0.5458 0.8932 ## Proportionality 0.01295 0.5402 1.1847 ## SpiecEasi 0.02162 0.2145 1.6322 ## SPRING 0.01444 0.1456 1.6993 "],["network-comparison.html", "Chapter 13 Network comparison 13.1 Data preparation 13.2 Network learning and analysis 13.3 Differential network analysis 13.4 Differential association analysis 13.5 Network comparison methods", " Chapter 13 Network comparison .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } This chapter assumes that you are already familiar with how to construct and analyze a single microbial network, which is explained in Chapter 12. Since microbial interactions are likely to change between conditions, such as between patients and healthy individuals or between different environmental states, identifying network differences between groups is often an integral secondary analysis step. Differences can be detected visually by plotting the networks side by side, or quantitatively using differential network analysis tools. Two approaches for comparing networks between two conditions are considered in this chapter: Differential network analysis, which analyzes differences in network metrics and network structure. Differential association analysis, which focuses on differences in the strength of individual associations. See Section 13.5 for further details on these two approaches and the methods used in this chapter. Here we use NetCoMi (Peschel et al. 2021) for network comparison, which includes several differential network analysis approaches as well as functionality for *differential association analysis**, i.e., generating a differential network. How to install NetCoMi from GitHub is explained in chapter 12. The PeerJ data set (Potbhare et al. 2022) containing skin microbial profiles for 58 subjects is again used in this chapter. The dataset also includes information on the subjects’ geographic location, gender, age and diet. Whether the skin microbiome differs between people with different diets is an interesting question that will be explored in this chapter. 13.1 Data preparation We perform the same data preprocessing steps as in chapter 12. library(NetCoMi) library(mia) data(&quot;peerj13075&quot;, package = &quot;mia&quot;) tse0 &lt;- peerj13075 # Agglomerate to genus level tse &lt;- agglomerateByRank(tse0, rank = &quot;genus&quot;) # Add relative abundances tse &lt;- transformAssay(tse, assay.type = &quot;counts&quot;, method = &quot;relabundance&quot;, MARGIN = &quot;samples&quot;) # Filter by prevalence tse &lt;- subsetByPrevalentFeatures(tse, prevalence = 0.2, detection = 0, assay.type = &quot;relabundance&quot;) # Add log10-transformed abundances tse &lt;- transformAssay(tse, method = &quot;log10&quot;, pseudocount = 1) # Add clr-transformed abundances tse &lt;- transformAssay(tse, method = &quot;clr&quot;, pseudocount = 1) Based on “Diet”, the tse object is then split into two groups: One with mixed diet subjects, and one with vegetarian subjects. Both subsets have nearly the same sample size and are therefore comparable. table(tse$Diet) ## ## Mixed Veg ## 28 30 tse_list &lt;- splitOn(tse, f = &quot;Diet&quot;, use_names = TRUE, MARGIN = 2) 13.2 Network learning and analysis The approach starts again with network construction and analysis, but this time we pass the two data sets to netConstruct to perform a network comparison. The rep.num argument is set to 10 to perform only 10 repetitions in the model selection approach. This speeds up the permutation tests performed later, and has a negligible effect for this data set. spring_net_diet &lt;- netConstruct(data = tse_list$Mixed, data2 = tse_list$Veg, taxRank = &quot;genus&quot;, filtTax = &quot;highestFreq&quot;, filtTaxPar = list(highestFreq = 100), measure = &quot;spring&quot;, measurePar = list(nlambda = 20, rep.num = 10, thresh = 0.05, Rmethod = &quot;approx&quot;), sparsMethod = &quot;none&quot;, dissFunc = &quot;signed&quot;, verbose = 3, seed = 13075) All network measures are now computed for both networks. Also, both GCMs are plotted together with a third matrix containing the differences between the GCMs and significance codes that express if the differences are significantly different from zero. spring_netprops_diet &lt;- netAnalyze(spring_net_diet, clustMethod = &quot;cluster_fast_greedy&quot;, hubPar = &quot;eigenvector&quot;, normDeg = FALSE) In both of the networks, some graphlet correlations are significantly different from zero. However, none of the correlations are significantly different between the groups. summary(spring_netprops_diet, groupNames = c(&quot;Mixed diet&quot;, &quot;Vegetarian&quot;)) ## ## Component sizes ## ``````````````` ## Mixed diet: ## size: 18 8 4 3 2 1 ## #: 1 1 1 1 3 43 ## Vegetarian: ## size: 21 5 4 2 1 ## #: 1 1 1 3 46 ## ______________________________ ## Global network properties ## ````````````````````````` ## Largest connected component (LCC): ## Mixed diet Vegetarian ## Relative LCC size 0.21951 0.25610 ## Clustering coefficient 0.10393 0.22433 ## Modularity 0.51500 0.47707 ## Positive edge percentage 80.00000 100.00000 ## Edge density 0.13072 0.12381 ## Natural connectivity 0.07312 0.06253 ## Vertex connectivity 1.00000 1.00000 ## Edge connectivity 1.00000 1.00000 ## Average dissimilarity* 0.96009 0.96154 ## Average path length** 2.22768 2.24622 ## ## Whole network: ## Mixed diet Vegetarian ## Number of components 50.00000 52.00000 ## Clustering coefficient 0.07506 0.16530 ## Modularity 0.75184 0.66821 ## Positive edge percentage 88.57143 100.00000 ## Edge density 0.01054 0.01084 ## Natural connectivity 0.01342 0.01347 ## ----- ## *: Dissimilarity = 1 - edge weight ## **: Path length = Units with average dissimilarity ## ## ______________________________ ## Clusters ## - In the whole network ## - Algorithm: cluster_fast_greedy ## ```````````````````````````````` ## Mixed diet: ## name: 0 1 2 3 4 5 6 7 8 9 ## #: 43 8 6 6 6 4 3 2 2 2 ## ## Vegetarian: ## name: 0 1 2 3 4 5 6 7 8 ## #: 46 9 8 4 5 4 2 2 2 ## ## ______________________________ ## Hubs ## - In alphabetical/numerical order ## - Based on empirical quantiles of centralities ## ``````````````````````````````````````````````` ## Mixed diet Vegetarian ## Citrobacter Aeromonas ## Erwinia Erwinia ## Escherichia Escherichia/Shigella ## Serratia Salmonella ## Shewanella Siccibacter ## ## ______________________________ ## Centrality measures ## - In decreasing order ## - Centrality of disconnected components is zero ## ```````````````````````````````````````````````` ## Degree (unnormalized): ## Mixed diet Vegetarian ## Erwinia 6 6 ## Citrobacter 4 0 ## Serratia 4 3 ## Streptococcus 3 0 ## Pantoea 2 1 ## ______ ______ ## Erwinia 6 6 ## Aeromonas 1 5 ## Escherichia/Shigella 2 5 ## Siccibacter 0 5 ## Salmonella 2 3 ## ## Betweenness centrality (normalized): ## Mixed diet Vegetarian ## Erwinia 0.60294 0.54737 ## Serratia 0.41912 0.1 ## Streptococcus 0.27941 0 ## Enterobacter 0.24265 0 ## Citrobacter 0.24265 0 ## ______ ______ ## Erwinia 0.60294 0.54737 ## Escherichia/Shigella 0.11765 0.53158 ## Aeromonas 0 0.36842 ## Siccibacter 0 0.3 ## Thiolamprovum 0 0.20526 ## ## Closeness centrality (normalized): ## Mixed diet Vegetarian ## Erwinia 0.85251 0.80605 ## Serratia 0.78452 0.56308 ## Citrobacter 0.71265 0 ## Enterobacter 0.64822 0.49217 ## Streptococcus 0.61535 0 ## ______ ______ ## Escherichia/Shigella 0.57246 0.80747 ## Erwinia 0.85251 0.80605 ## Aeromonas 0.4768 0.74282 ## Siccibacter 0 0.72705 ## Salmonella 0.57364 0.66777 ## ## Eigenvector centrality (normalized): ## Mixed diet Vegetarian ## Serratia 1 0.41577 ## Erwinia 0.97703 1 ## Citrobacter 0.80991 0 ## Escherichia 0.60982 0 ## Shewanella 0.41174 0.1238 ## ______ ______ ## Erwinia 0.97703 1 ## Escherichia/Shigella 0.38659 0.96856 ## Siccibacter 0 0.888 ## Aeromonas 0.26033 0.68516 ## Salmonella 0.3153 0.64405 For each centrality measure, the five nodes with the highest centrality in each group are plotted by default. We notice some differences in the network properties. The differential network analysis performed in the next section will show if the differences are significant. 13.3 Differential network analysis 13.3.1 Visual comparison We start with a visual comparison of the two networks using NetCoMi’s plot function. The same configuration as in chapter 12 is used. plot(spring_netprops_diet, repulsion = 0.97, rmSingles = TRUE, labelScale = FALSE, nodeSize = &quot;eigenvector&quot;, nodeSizeSpread = 2, nodeColor = &quot;cluster&quot;, sameColThresh = 2, hubBorderCol = &quot;darkgray&quot;, cexNodes = 2, edgeTranspHigh = 20, title1 = &quot;Mixed diet&quot;, title2 = &quot;Vegetarian&quot;, showTitle = TRUE, cexTitle = 2, mar = c(1, 4, 4, 4)) # Overlay a transparent plot on which the legend is plotted par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), mar=c(0, 0, 0, 0), new=TRUE) plot(0, 0, type=&#39;n&#39;, bty=&#39;n&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;) legend(-0.2, -0.9, cex = 1.5, title = &quot;estimated correlation:&quot;, legend = c(&quot;+&quot;,&quot;-&quot;), lty = 1, lwd = 3, col = c(&quot;#009900&quot;,&quot;red&quot;), bty = &quot;n&quot;, horiz = TRUE) The layout is computed separately for each network, making it difficult to visually compare certain associations. It is therefore recommended to use the same layout for both groups (argument sameLayout). Instead of simply copying one layout to the other network, we set layoutGroup to “union”. This ensures that the nodes are placed as optimally as possible for both networks. plot(spring_netprops_diet, sameLayout = TRUE, repulsion = 0.95, rmSingles = &quot;inboth&quot;, labelScale = FALSE, nodeSize = &quot;eigenvector&quot;, nodeSizeSpread = 2, nodeColor = &quot;cluster&quot;, sameColThresh = 2, hubBorderCol = &quot;darkgray&quot;, cexNodes = 2, edgeTranspHigh = 20, title1 = &quot;Mixed diet&quot;, title2 = &quot;Vegetarian&quot;, showTitle = TRUE, cexTitle = 2, mar = c(1, 4, 4, 4)) # Add legend par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), mar=c(0, 0, 0, 0), new=TRUE) plot(0, 0, type=&#39;n&#39;, bty=&#39;n&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;) legend(-0.2, -0.8, cex = 1.7, title = &quot;estimated correlation:&quot;, legend = c(&quot;+&quot;,&quot;-&quot;), lty = 1, lwd = 3, col = c(&quot;#009900&quot;,&quot;red&quot;), bty = &quot;n&quot;, horiz = TRUE) A few notes: Differences in the edge weights can now be seen at first glance. For example, Serratia and Citrobacter are strongly associated in the mixed diet group, but not at all in the vegetarian group. Clusters must share at least two nodes (sameColThresh argument) to be colored equally in both networks, which is why the color of some clusters differs between the groups. The clustering generally differs markedly. In particular, the cluster assignment of many of the nodes in the largest connected component differs between the two groups. As in Chapter 12, we also generate a network plot using phylum names to color the nodes and mclr-transformed abundances to scale node sizes. library(RColorBrewer) # Generate vector with phylum names for node coloring phyla &lt;- as.factor(rowData(tse)$phylum) names(phyla) &lt;- rowData(tse)$genus # Create color vector colvec &lt;- RColorBrewer::brewer.pal(length(levels(phyla)), &quot;Set3&quot;) p_diet &lt;- plot(spring_netprops_diet, sameLayout = TRUE, repulsion = 0.95, rmSingles = &quot;inboth&quot;, labelScale = FALSE, nodeSize = &quot;clr&quot;, nodeColor = &quot;feature&quot;, featVecCol = phyla, colorVec = colvec, nodeTransp = 20, sameColThresh = 2, highlightHubs = FALSE, cexNodes = 2, edgeTranspHigh = 20, title1 = &quot;Mixed diet&quot;, title2 = &quot;Vegetarian&quot;, showTitle = TRUE, cexTitle = 2, mar = c(1, 4, 4, 4)) # Add legends # Colors used in the legend should be equally transparent as in the plot col_transp &lt;- colToTransp(colvec, 20) par(fig=c(0, 1, 0, 1), oma=c(0, 0, 0, 0), mar=c(0, 0, 0, 0), new=TRUE) plot(0, 0, type=&#39;n&#39;, bty=&#39;n&#39;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;) legend(-0.15, -0.8, cex = 1.7, title = &quot;estimated correlation:&quot;, legend = c(&quot;+&quot;,&quot;-&quot;), lty = 1, lwd = 3, col = c(&quot;#009900&quot;,&quot;red&quot;), bty = &quot;n&quot;, horiz = TRUE) legend(-0.15, 1.3, cex = 1.7, pt.cex = 2.5, title = &quot;Phylum:&quot;, legend=levels(phyla), col = col_transp, bty = &quot;n&quot;, pch = 16) 13.3.2 Quantitative comparison netCompare() enables a quantitative network comparison using comparative measures such as Jaccard’s Index, Adjusted Rand Index, and permutation tests. To test for statistical significance of differences in network properties, we perform permutation tests with 1000 permutations. Multiple CPU cores are used to save run time. The association matrices estimated for all permutations are stored in an external file. We will reuse them later when performing differential association analysis. They could also be used to rerun netCompare() with different parameter settings. Note that unless running on a cluster with considerably more CPU cores, a network comparison with permutation tests may take several hours. You should test the code below with a small number of permutations to make sure it works before applying it to your data. spring_netcomp_diet &lt;- netCompare(spring_netprops_diet, permTest = TRUE, nPerm = 1000, cores = 6, seed = 13075, storeAssoPerm = TRUE, fileStoreAssoPerm = &quot;general/network_data/spring_assoPerm&quot;, verbose = TRUE) summary(spring_netcomp_diet, groupNames = c(&quot;Mixed diet&quot;, &quot;Vegetarian&quot;), numbNodes = 5) ## ## Comparison of Network Properties ## ---------------------------------- ## CALL: ## netCompare(x = spring_netprops_diet, permTest = TRUE, verbose = TRUE, ## nPerm = 1000, cores = 19, libPathsClust = &quot;/dss/dsshome1/07/di93fen/R&quot;, ## seed = 13075, storeAssoPerm = TRUE, fileStoreAssoPerm = &quot;general/network_data/spring_assoPerm&quot;) ## ## ______________________________ ## Global network properties ## ````````````````````````` ## Largest connected component (LCC): ## Mixed diet Vegetarian abs.diff. p-value ## Relative LCC size 0.220 0.256 0.037 0.838162 ## Clustering coefficient 0.104 0.224 0.120 0.450549 ## Modularity 0.515 0.477 0.038 0.801199 ## Positive edge percentage 80.000 100.000 20.000 0.000999 *** ## Edge density 0.131 0.124 0.007 0.910090 ## Natural connectivity 0.073 0.063 0.011 0.812188 ## Vertex connectivity 1.000 1.000 0.000 1.000000 ## Edge connectivity 1.000 1.000 0.000 1.000000 ## Average dissimilarity* 0.960 0.962 0.001 0.945055 ## Average path length** 2.228 2.246 0.019 0.984016 ## ## Whole network: ## Mixed diet Vegetarian abs.diff. p-value ## Number of components 50.000 52.000 2.000 0.93706 ## Clustering coefficient 0.075 0.165 0.090 0.46254 ## Modularity 0.752 0.668 0.084 0.24975 ## Positive edge percentage 88.571 100.000 11.429 0.02398 * ## Edge density 0.011 0.011 0.000 0.97303 ## Natural connectivity 0.013 0.013 0.000 0.89011 ## ----- ## p-values: one-tailed test with null hypothesis diff=0 ## *: Dissimilarity = 1 - edge weight ## **: Path length = Units with average dissimilarity ## ## ______________________________ ## Jaccard index (similarity betw. sets of most central nodes) ## ``````````````````````````````````````````````````````````` ## Jacc P(&lt;=Jacc) P(&gt;=Jacc) ## degree 0.560 0.9944 0.01637 * ## betweenness centr. 0.294 0.4777 0.71860 ## closeness centr. 0.560 0.9944 0.01637 * ## eigenvec. centr. 0.560 0.9944 0.01637 * ## hub taxa 0.111 0.1431 0.97399 ## ----- ## Jaccard index in [0,1] (1 indicates perfect agreement) ## ## ______________________________ ## Adjusted Rand index (similarity betw. clusterings) ## `````````````````````````````````````````````````` ## wholeNet LCC ## ARI 0.367 0.035 ## p-value 0.000 0.563 ## ----- ## ARI in [-1,1] with ARI=1: perfect agreement betw. clusterings ## ARI=0: expected for two random clusterings ## p-value: permutation test (n=1000) with null hypothesis ARI=0 ## ## ______________________________ ## Graphlet Correlation Distance ## ````````````````````````````` ## wholeNet LCC ## GCD 1.5980 2.1770 ## p-value 0.4226 0.7143 ## ----- ## GCD &gt;= 0 (GCD=0 indicates perfect agreement between GCMs) ## p-value: permutation test with null hypothesis GCD=0 ## ## ______________________________ ## Centrality measures ## - In decreasing order ## - Centrality of disconnected components is zero ## ```````````````````````````````````````````````` ## Degree (unnormalized): ## Mixed diet Vegetarian abs.diff. adj.p-value ## Siccibacter 0 5 5 1 ## Aeromonas 1 5 4 1 ## Citrobacter 4 0 4 1 ## Escherichia/Shigella 2 5 3 1 ## Streptococcus 3 0 3 1 ## ## Betweenness centrality (normalized): ## Mixed diet Vegetarian abs.diff. adj.p-value ## Escherichia/Shigella 0.118 0.532 0.414 1 ## Aeromonas 0.000 0.368 0.368 1 ## Serratia 0.419 0.100 0.319 1 ## Siccibacter 0.000 0.300 0.300 1 ## Streptococcus 0.279 0.000 0.279 1 ## ## Closeness centrality (normalized): ## Mixed diet Vegetarian abs.diff. adj.p-value ## Siccibacter 0.000 0.727 0.727 0.2676 ## Citrobacter 0.713 0.000 0.713 0.9799 ## Streptococcus 0.615 0.000 0.615 0.2676 ## Escherichia 0.587 0.000 0.587 0.9799 ## Xenorhabdus 0.000 0.527 0.527 0.2676 ## ## Eigenvector centrality (normalized): ## Mixed diet Vegetarian abs.diff. adj.p-value ## Siccibacter 0.000 0.888 0.888 0.6553 ## Citrobacter 0.810 0.000 0.810 1.0000 ## Escherichia 0.610 0.000 0.610 1.0000 ## Serratia 1.000 0.416 0.584 1.0000 ## Escherichia/Shigella 0.387 0.969 0.582 1.0000 ## ## _________________________________________________________ ## Significance codes: ***: 0.001, **: 0.01, *: 0.05, .: 0.1 Interpreting some results: Almost all global network properties are significantly different between the groups (for \\(\\alpha=0.1\\)), thus reflecting the different overall network structure we already have seen in the network plots. For the Jaccard index of degree, closeness, and eigenvector centrality, the probability P(&gt;=Jacc) is significant, meaning that the sets of the most central nodes are quite similar for these three measures. The Jaccard index for the hub nodes, on the other hand, is low because the two networks share only one hub node (“Erwinia”). As indicated by some similarities in the clusterings, the adjusted Rand index (ARI) of the whole network is significantly different from zero and thus from random clustering. The ARI of the largest connected component (LCC), however, is close to zero due to the different clusterings in the LCC. The two GCD values are significantly different from zero, indicating substantial differences in the overall network structures. All nodes are also tested for having significantly different centrality (only the five nodes with the highest absolute difference are shown in the summary). For \\(\\alpha=0.05\\), some nodes have a significantly different closeness centrality, and for \\(\\alpha=0.1\\) also a significantly different eigenvector centrality. Most of these nodes have a high centrality in the one group, but are not connected in the other group. 13.4 Differential association analysis The diffnet() function provides statistical tests to assess whether the associations themselves are significantly different between the two groups. NetCoMi also provides a plot function to generate a differential network, where two nodes are connected if they are differentially associated between the groups. Since we have already computed the permutation association matrices before, we can reuse them here (argument fileLoadAssoPerm). The local false discovery rate is controlled at level 0.2 to account for multiplicity. spring_diffnet &lt;- diffnet(spring_net_diet, diffMethod = &quot;perm&quot;, fileLoadAssoPerm = &quot;general/network_data/spring_assoPerm&quot;, adjust = &quot;lfdr&quot;) sum(spring_diffnet$pAdjustVec &lt; 0.05) ## [1] 0 sum(spring_diffnet$pvalsVec &lt; 0.05) ## [1] 12 Some of the unadjusted p-values are below the usual 5% significance level. However, none of the differences remain significant after adjusting for multiple testing so that the differential network would be empty. To demonstrate the interpretation of a differential network, we set adjust to “none”, which is actually statistically incorrect. spring_diffnet_unadj &lt;- diffnet(spring_net_diet, pvalsVec = spring_diffnet$pvalsVec, diffMethod = &quot;perm&quot;, alpha = 0.05, adjust = &quot;none&quot;) The diffnet object it now plotted using NetCoMi’s plot function. plot(spring_diffnet_unadj, cexLabels = 2, cexNodes = 0.7, cexLegend = 2.5, cexTitle = 3, mar = c(3,2,5,15), legendGroupnames = c(&quot;Mixed diet&quot;, &quot;Vegetarian&quot;), legendPos = c(1.2,1.5), legendArgs = list(lwd = 4), fade = FALSE) Edge colors represent the direction of the associations in the two groups. For example, if two OTUs are positively correlated in the mixed diet group and uncorrelated in the vegetarian group (such as Serratia and Citrobacter), the edge color is dark green. 13.5 Network comparison methods While many approaches exist for the detection of differential correlations, e.g. (Yu et al. 2019; McKenzie et al. 2016; Siska and Kechris 2017), the literature on the more general case of differential association detection is scarce. Bhuva et al. (2019) compare various methods in a simulation study, which again includes many differential correlation approaches, but also more general methods such as latent differential graphical models. Gill, Datta, and Datta (2010) introduce an approach to analyze whether the connectivity of individual nodes is different between two groups using permutation tests, which is applicable to any kind of association. He et al. (2019) propose a test to infer the differential network structure for two conditional dependence networks. Performing differential network analysis is challenging because network measures do not follow classical statistical distributions. Shojaie (2021) provide an overview of differential network analysis methods, but focus only on changes in edge sets. Lichtblau et al. (2017) compare differential network analysis methods that incorporate multiple local and global network measures. Jardim et al. (2019) present a tool “BioNetStat” for differential analysis of biological networks, which is able to compare certain network measures between groups. The NetCoMi package used for network comparison in this chapter includes the following differential network analysis approaches:: Permutation approach to test global network measures (e.g., transitivity, connectivity, or average path length) as well as centrality measures for group differences. Jaccard index to assess the similarity between sets of most central nodes Adjusted Rand index to assess the similarity between clusterings Graphlet Correlation Distance (GCM) See (Peschel et al. 2021) for an explanation of the first three approaches. The GCM was proposed by Yaveroğlu et al. (2014). Two methods (Fisher’s z-test (Fisher 1970) and the Discordant method (Siska, Bowler, and Kechris 2016)) are available for identifying differential correlations, and permutation tests for the more general case of identifying differential associations. See (Peschel et al. 2021) for details. NetCoMi offers also a function for plotting a differential network. "],["machine_learning.html", "Chapter 14 Machine Learning 14.1 Supervised machine learning 14.2 Unsupervised machine learning", " Chapter 14 Machine Learning .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } Machine learning (ML) is a part of artificial intelligence. There are multiple definitions, but “machine” refers to computation and “learning” to improving performance based on the data by finding patterns from it. Machine learning includes wide variety of methods from simple statistical methods to more complex methods such as neural-networks. Machine learning can be divided into supervised and unsupervised machine learning. Supervised ML is used to predict outcome based on the data. Unsupervised ML is used, for example, to reduce dimensionality (e.g. PCA) and to find clusters from the data (e.g., k-means clustering). 14.1 Supervised machine learning “Supervised” means that the training data is introduced before. The training data contains labels (e.g., patient status), and the model is fitted based on the training data. After fitting, the model is utilized to predict labels of data whose labels are not known. library(mia) # Load experimental data data(peerj13075, package=&quot;mia&quot;) tse &lt;- peerj13075 Let’s first preprocess the data. # Agglomerate data tse &lt;- mergeFeaturesByRank(tse, rank = &quot;order&quot;) # Apply CLR transform tse &lt;- transformAssay(tse, assay.type = &quot;counts&quot;, method = &quot;clr&quot;, MARGIN=&quot;samples&quot;, pseudocount=1) # Get assay assay &lt;- assay(tse, &quot;clr&quot;) # Transpose assay assay &lt;- t(assay) # Convert into data.frame df &lt;- as.data.frame(assay) # Add labels to assay labels &lt;- colData(tse)$Diet labels &lt;- as.factor(labels) df$diet &lt;- labels df[5, 5] ## [1] -0.4612 In the example below, we use mikropml package. We try to predict the diet type based on the data. library(mikropml) # Run random forest results &lt;- run_ml(df, &quot;rf&quot;, outcome_colname = &quot;diet&quot;, kfold = 2, cv_times = 5, training_frac = 0.8) # Print result confusionMatrix(data = results$trained_model$finalModel$predicted, reference = results$trained_model$finalModel$y) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Mixed Veg ## Mixed 12 12 ## Veg 11 12 ## ## Accuracy : 0.511 ## 95% CI : (0.361, 0.659) ## No Information Rate : 0.511 ## P-Value [Acc &gt; NIR] : 0.558 ## ## Kappa : 0.022 ## ## Mcnemar&#39;s Test P-Value : 1.000 ## ## Sensitivity : 0.522 ## Specificity : 0.500 ## Pos Pred Value : 0.500 ## Neg Pred Value : 0.522 ## Prevalence : 0.489 ## Detection Rate : 0.255 ## Detection Prevalence : 0.511 ## Balanced Accuracy : 0.511 ## ## &#39;Positive&#39; Class : Mixed ## mikropml offers easier interface to caret package. However, we can also use it directly. Let’s use xgboost model which is another commonly used algorithm in bioinformatics. # Set seed for reproducibility set.seed(6358) # Specify train control train_control &lt;- trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, savePredictions = &quot;final&quot;, allowParallel = TRUE) # Specify hyperparameter tuning grid tune_grid &lt;- expand.grid(nrounds = c(50, 100, 200), max_depth = c(6, 8, 10), colsample_bytree = c(0.6, 0.8, 1), eta = c(0.1, 0.3), gamma = 0, min_child_weight = c(3, 4, 5), subsample = c(0.6, 0.8) ) # Train the model, use LOOCV to evaluate performance model &lt;- train(x = assay, y = labels, method = &quot;xgbTree&quot;, objective = &quot;binary:logistic&quot;, trControl = train_control, tuneGrid = tune_grid, metric = &quot;AUC&quot;, verbosity = 0 ) Let’s create ROC curve which is a commonly used method in binary classification. For unbalanced data, you might want to plot precision-recall curve. library(MLeval) # Calculate different evaluation metrics res &lt;- evalm(model, showplots = FALSE) # Use patchwork to plot ROC and precision-recall curve side-by-side library(patchwork) res$roc + res$proc + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &#39;bottom&#39;) 14.2 Unsupervised machine learning “Unsupervised” means that the labels (e.g., patient status is not known), and patterns are learned based only the abundance table, for instance. Unsupervised ML is also known as a data mining where patterns are extracted from big datasets. For unsupervised machine learning, please refer to chapters that are listed below: Chapter 10 Chapter 8 "],["multi-assay-analyses.html", "Chapter 15 Multi-Assay Analyses 15.1 Cross-correlation Analysis 15.2 Multi-Omics Factor Analysis", " Chapter 15 Multi-Assay Analyses .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } library(mia) Multi-omics approaches integrate data from multiple sources. For example, we can integrate taxonomic abundance profiles with metabolomic or other biomolecular profiling data to observe associations, make predictions, or aim at causal inferences. Integrating evidence across multiple sources can lead to enhanced predictions, more holistic understanding, or facilitate the discovery of novel biomarkers. In this section we demonstrate common multi-assay data integration tasks. Cross-correlation analysis is a straightforward approach that can reveal strength and type of assocations between data sets. For instance, we can analyze if higher presence of a specific taxon relates to higher levels of a biomolecule. The analyses can be facilitated by the multi-assay data containers, TreeSummarizedExperiment and MultiAssayExperiment. These are scalable and contain different types of data in a single container, making this framework particularly suited for multi-assay microbiome data incorporating different types of complementary data sources in a single reproducible workflow. Solutions to a number of data integration problems are discussed in more detail in Section 3. Another experiment can be stored in altExp slot of SE data container. Alternatively, both experiments can be stored side-by-side in a MAE data container (see sections 3.2.5 and 3.2.6 to learn more about altExp and MAE objects, respectively). Different experiments are first imported as single-assay data containers similarly to the case when only one experiment is present. After that, the different experiments can be combined into one multi-assay data container. The result is a MAE object with multiple experiments in its experiment slot, or a TreeSE object with alternative experiments in the altExp slot. As an example, we use a dataset from the following publication: (2021) Xylo-oligosaccharides in prevention of hepatic steatosis and adipose tissue inflammation: associating taxonomic and metabolomic patterns in fecal microbiota with biclustering. In this study, mice were fed either with a high-fat or a low-fat diet, and with or without prebiotics, for the purpose studying whether prebiotics attenuate the negative impact of a high-fat diet on health. This example data can be loaded from microbiomeDataSets. The data is already in MAE format. It includes three different experiments: microbial abundance data, metabolite concentrations, and data about different biomarkers. If you like to construct the same data object from the original files instead, here you can find help for importing data into an SE object. # Load the data data(HintikkaXOData, package = &quot;mia&quot;) mae &lt;- HintikkaXOData library(stringr) # Drop off those bacteria that do not include information in Phylum or lower levels mae[[1]] &lt;- mae[[1]][!is.na(rowData(mae[[1]])$Phylum), ] # Clean taxonomy data, so that names do not include additional characters rowData(mae[[1]]) &lt;- DataFrame(apply(rowData(mae[[1]]), 2, str_remove, pattern = &quot;._[0-9]__&quot;)) # Available alternative experiments experiments(mae) ## ExperimentList class object of length 3: ## [1] microbiota: TreeSummarizedExperiment with 12613 rows and 40 columns ## [2] metabolites: TreeSummarizedExperiment with 38 rows and 40 columns ## [3] biomarkers: TreeSummarizedExperiment with 39 rows and 40 columns # Microbiome data getWithColData(mae, &quot;microbiota&quot;) ## class: TreeSummarizedExperiment ## dim: 12613 40 ## metadata(0): ## assays(1): counts ## rownames(12613): GAYR01026362.62.2014 CVJT01000011.50.2173 ... ## JRJTB:03787:02429 JRJTB:03787:02478 ## rowData names(7): Phylum Class ... Species OTU ## colnames(40): C1 C2 ... C39 C40 ## colData names(6): Sample Rat ... Fat XOS ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: NULL ## rowTree: NULL ## colLinks: NULL ## colTree: NULL # Metabolite data getWithColData(mae, &quot;metabolites&quot;) ## class: TreeSummarizedExperiment ## dim: 38 40 ## metadata(0): ## assays(1): nmr ## rownames(38): Butyrate Acetate ... Malonate 1,3-dihydroxyacetone ## rowData names(0): ## colnames(40): C1 C2 ... C39 C40 ## colData names(6): Sample Rat ... Fat XOS ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: NULL ## rowTree: NULL ## colLinks: NULL ## colTree: NULL # Biomarker data getWithColData(mae, &quot;biomarkers&quot;) ## class: TreeSummarizedExperiment ## dim: 39 40 ## metadata(0): ## assays(1): signals ## rownames(39): Triglycerides_liver CLSs_epi ... NPY_serum Glycogen_liver ## rowData names(0): ## colnames(40): C1 C2 ... C39 C40 ## colData names(6): Sample Rat ... Fat XOS ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0): ## rowLinks: NULL ## rowTree: NULL ## colLinks: NULL ## colTree: NULL 15.1 Cross-correlation Analysis Next we can perform a cross-correlation analysis. Let us analyze if individual bacteria genera are correlated with concentrations of individual metabolites. This helps to answer the following question: “If bacterium X is present, is the concentration of metabolite Y lower or higher”? # Agglomerate microbiome data at family level mae[[1]] &lt;- mergeFeaturesByPrevalence(mae[[1]], rank = &quot;Family&quot;) # Does log10 transform for microbiome data mae[[1]] &lt;- transformAssay(mae[[1]], method = &quot;log10&quot;, pseudocount = TRUE) # Give unique names so that we do not have problems when we are creating a plot rownames(mae[[1]]) &lt;- getTaxonomyLabels(mae[[1]]) # Cross correlates data sets correlations &lt;- testExperimentCrossCorrelation(mae, experiment1 = 1, experiment2 = 2, assay.type1 = &quot;log10&quot;, assay.type2 = &quot;nmr&quot;, method = &quot;spearman&quot;, p_adj_threshold = NULL, cor_threshold = NULL, # Remove when mia is fixed mode = &quot;matrix&quot;, sort = TRUE, show_warnings = FALSE) Next, we create a heatmap depicting all cross-correlations between bacterial genera and metabolite concentrations. library(ComplexHeatmap) # Create a heatmap and store it plot &lt;- Heatmap(correlations$cor, # Print values to cells cell_fun = function(j, i, x, y, width, height, fill) { # If the p-value is under threshold if( !is.na(correlations$p_adj[i, j]) &amp; correlations$p_adj[i, j] &lt; 0.05 ){ # Print &quot;X&quot; grid.text(sprintf(&quot;%s&quot;, &quot;X&quot;), x, y, gp = gpar(fontsize = 10, col = &quot;#1dff00&quot;)) } }, heatmap_legend_param = list(title = &quot;&quot;, legend_height = unit(5, &quot;cm&quot;)) ) plot 15.2 Multi-Omics Factor Analysis Multi-Omics Factor Analysis (MOFA) is an unsupervised method for integrating multi-omic data sets in a downstream analysis (Argelaguet 2018). It could be seen as a generalization of principal component analysis. Yet, with the ability to infer a latent (low-dimensional) representation, shared among the multiple (-omics) data sets in hand. We use the R MOFA2 package for the analysis, and install the corresponding dependencies. library(MOFA2) # For inter-operability between Python and R, and setting Python dependencies, # reticulate package is needed library(reticulate) # Let us assume that these have been installed already. #reticulate::install_miniconda(force = TRUE) #reticulate::use_miniconda(condaenv = &quot;env1&quot;, required = FALSE) #reticulate::py_install(packages = c(&quot;mofapy2&quot;), pip = TRUE, python_version=3.6) The mae object could be used straight to create the MOFA model. Yet, we transform our assays since the model assumes normality per default. We can also use Poisson or Bernoulli distributions among others. Note that duplicates, such as “uncultured”, might appear when aggregating the microbiome data by a taxonomic rank. To check for duplicates, run any(duplicated(rownames(mae[[1]]))). If it returns TRUE, then the duplicates are present. We can add rownames(mae[[1]]) &lt;- getTaxonomyLabels(mae[[1]], make_unique=TRUE) to remove them. library(MOFA2) # For simplicity, classify all high-fat diets as high-fat, and all the low-fat # diets as low-fat diets colData(mae)$Diet &lt;- ifelse(colData(mae)$Diet == &quot;High-fat&quot; | colData(mae)$Diet == &quot;High-fat + XOS&quot;, &quot;High-fat&quot;, &quot;Low-fat&quot;) # Transforming microbiome data with rclr mae[[1]] &lt;- transformAssay(mae[[1]], method = &quot;relabundance&quot;) mae[[1]] &lt;- transformAssay(mae[[1]], assay.type = &quot;relabundance&quot;, method = &quot;rclr&quot;) # Transforming metabolomic data with log10 mae[[2]] &lt;- transformAssay(mae[[2]], assay.type = &quot;nmr&quot;, MARGIN = &quot;samples&quot;, method = &quot;log10&quot;) # Transforming biomarker data with z-transform mae[[3]] &lt;- transformAssay(mae[[3]], assay.type = &quot;signals&quot;, MARGIN = &quot;features&quot;, method = &quot;z&quot;, pseudocount = 1) # Removing assays no longer needed assay(mae[[1]], &quot;counts&quot;) &lt;- NULL assay(mae[[1]], &quot;log10&quot;) &lt;- NULL assay(mae[[2]], &quot;nmr&quot;) &lt;- NULL assay(mae[[3]], &quot;signals&quot;) &lt;- NULL # Building our mofa model model &lt;- create_mofa_from_MultiAssayExperiment(mae, groups = &quot;Diet&quot;, extract_metadata = TRUE) model ## Untrained MOFA model with the following characteristics: ## Number of views: 3 ## Views names: microbiota metabolites biomarkers ## Number of features (per view): 39 38 39 ## Number of groups: 2 ## Groups names: High-fat Low-fat ## Number of samples (per group): 20 20 ## Model options can be defined as follows: model_opts &lt;- get_default_model_options(model) model_opts$num_factors &lt;- 5 head(model_opts) ## $likelihoods ## microbiota metabolites biomarkers ## &quot;gaussian&quot; &quot;gaussian&quot; &quot;gaussian&quot; ## ## $num_factors ## [1] 5 ## ## $spikeslab_factors ## [1] FALSE ## ## $spikeslab_weights ## [1] FALSE ## ## $ard_factors ## [1] TRUE ## ## $ard_weights ## [1] TRUE Training options for the model are defined in the following way: train_opts &lt;- get_default_training_options(model) head(train_opts) ## $maxiter ## [1] 1000 ## ## $convergence_mode ## [1] &quot;fast&quot; ## ## $drop_factor_threshold ## [1] -1 ## ## $verbose ## [1] FALSE ## ## $startELBO ## [1] 1 ## ## $freqELBO ## [1] 5 The model is then prepared with prepare_mofa and trained with run_mofa: model.prepared &lt;- prepare_mofa( object = model, model_options = model_opts ) # Some systems may require the specification `use_basilisk = TRUE` # so it has been added to the following code model.trained &lt;- run_mofa(model.prepared, use_basilisk = TRUE) ## ## ######################################################### ## ### __ __ ____ ______ ### ## ### | \\/ |/ __ \\| ____/\\ _ ### ## ### | \\ / | | | | |__ / \\ _| |_ ### ## ### | |\\/| | | | | __/ /\\ \\_ _| ### ## ### | | | | |__| | | / ____ \\|_| ### ## ### |_| |_|\\____/|_|/_/ \\_\\ ### ## ### ### ## ######################################################### ## ## ## ## use_float32 set to True: replacing float64 arrays by float32 arrays to speed up computations... ## ## Successfully loaded view=&#39;microbiota&#39; group=&#39;High-fat&#39; with N=20 samples and D=39 features... ## Successfully loaded view=&#39;microbiota&#39; group=&#39;Low-fat&#39; with N=20 samples and D=39 features... ## Successfully loaded view=&#39;metabolites&#39; group=&#39;High-fat&#39; with N=20 samples and D=38 features... ## Successfully loaded view=&#39;metabolites&#39; group=&#39;Low-fat&#39; with N=20 samples and D=38 features... ## Successfully loaded view=&#39;biomarkers&#39; group=&#39;High-fat&#39; with N=20 samples and D=39 features... ## Successfully loaded view=&#39;biomarkers&#39; group=&#39;Low-fat&#39; with N=20 samples and D=39 features... ## ## ## Model options: ## - Automatic Relevance Determination prior on the factors: True ## - Automatic Relevance Determination prior on the weights: True ## - Spike-and-slab prior on the factors: False ## - Spike-and-slab prior on the weights: False ## Likelihoods: ## - View 0 (microbiota): gaussian ## - View 1 (metabolites): gaussian ## - View 2 (biomarkers): gaussian ## ## ## ## ## ###################################### ## ## Training the model with seed 42 ## ## ###################################### ## ## ## ELBO before training: -22197.07 ## ## Iteration 1: time=0.00, ELBO=-4038.83, deltaELBO=18158.238 (81.80467202%), Factors=5 ## Iteration 2: time=0.00, Factors=5 ## Iteration 3: time=0.00, Factors=5 ## Iteration 4: time=0.00, Factors=5 ## Iteration 5: time=0.00, Factors=5 ## Iteration 6: time=0.00, ELBO=939.45, deltaELBO=4978.275 (22.42762338%), Factors=5 ## Iteration 7: time=0.00, Factors=5 ## Iteration 8: time=0.00, Factors=5 ## Iteration 9: time=0.00, Factors=5 ## Iteration 10: time=0.00, Factors=5 ## Iteration 11: time=0.00, ELBO=974.43, deltaELBO=34.980 (0.15758673%), Factors=5 ## Iteration 12: time=0.00, Factors=5 ## Iteration 13: time=0.00, Factors=5 ## Iteration 14: time=0.00, Factors=5 ## Iteration 15: time=0.00, Factors=5 ## Iteration 16: time=0.00, ELBO=980.98, deltaELBO=6.558 (0.02954301%), Factors=5 ## Iteration 17: time=0.00, Factors=5 ## Iteration 18: time=0.00, Factors=5 ## Iteration 19: time=0.00, Factors=5 ## Iteration 20: time=0.00, Factors=5 ## Iteration 21: time=0.00, ELBO=984.05, deltaELBO=3.071 (0.01383618%), Factors=5 ## Iteration 22: time=0.00, Factors=5 ## Iteration 23: time=0.00, Factors=5 ## Iteration 24: time=0.00, Factors=5 ## Iteration 25: time=0.00, Factors=5 ## Iteration 26: time=0.00, ELBO=985.89, deltaELBO=1.837 (0.00827635%), Factors=5 ## Iteration 27: time=0.00, Factors=5 ## Iteration 28: time=0.00, Factors=5 ## Iteration 29: time=0.00, Factors=5 ## Iteration 30: time=0.00, Factors=5 ## Iteration 31: time=0.00, ELBO=987.13, deltaELBO=1.240 (0.00558588%), Factors=5 ## Iteration 32: time=0.00, Factors=5 ## Iteration 33: time=0.00, Factors=5 ## Iteration 34: time=0.00, Factors=5 ## Iteration 35: time=0.00, Factors=5 ## Iteration 36: time=0.00, ELBO=988.02, deltaELBO=0.893 (0.00402099%), Factors=5 ## Iteration 37: time=0.00, Factors=5 ## Iteration 38: time=0.00, Factors=5 ## Iteration 39: time=0.00, Factors=5 ## Iteration 40: time=0.00, Factors=5 ## Iteration 41: time=0.00, ELBO=988.70, deltaELBO=0.676 (0.00304483%), Factors=5 ## Iteration 42: time=0.00, Factors=5 ## Iteration 43: time=0.00, Factors=5 ## Iteration 44: time=0.00, Factors=5 ## Iteration 45: time=0.00, Factors=5 ## Iteration 46: time=0.00, ELBO=989.23, deltaELBO=0.531 (0.00239446%), Factors=5 ## Iteration 47: time=0.00, Factors=5 ## Iteration 48: time=0.00, Factors=5 ## Iteration 49: time=0.00, Factors=5 ## Iteration 50: time=0.00, Factors=5 ## Iteration 51: time=0.00, ELBO=989.66, deltaELBO=0.428 (0.00192983%), Factors=5 ## Iteration 52: time=0.00, Factors=5 ## Iteration 53: time=0.00, Factors=5 ## Iteration 54: time=0.00, Factors=5 ## Iteration 55: time=0.00, Factors=5 ## Iteration 56: time=0.00, ELBO=990.01, deltaELBO=0.355 (0.00159733%), Factors=5 ## Iteration 57: time=0.00, Factors=5 ## Iteration 58: time=0.00, Factors=5 ## Iteration 59: time=0.00, Factors=5 ## Iteration 60: time=0.00, Factors=5 ## Iteration 61: time=0.00, ELBO=990.31, deltaELBO=0.299 (0.00134727%), Factors=5 ## Iteration 62: time=0.00, Factors=5 ## Iteration 63: time=0.00, Factors=5 ## Iteration 64: time=0.00, Factors=5 ## Iteration 65: time=0.00, Factors=5 ## Iteration 66: time=0.00, ELBO=990.57, deltaELBO=0.260 (0.00117140%), Factors=5 ## Iteration 67: time=0.00, Factors=5 ## Iteration 68: time=0.00, Factors=5 ## Iteration 69: time=0.00, Factors=5 ## Iteration 70: time=0.00, Factors=5 ## Iteration 71: time=0.00, ELBO=990.80, deltaELBO=0.224 (0.00100732%), Factors=5 ## Iteration 72: time=0.00, Factors=5 ## Iteration 73: time=0.00, Factors=5 ## Iteration 74: time=0.00, Factors=5 ## Iteration 75: time=0.00, Factors=5 ## Iteration 76: time=0.00, ELBO=991.00, deltaELBO=0.199 (0.00089717%), Factors=5 ## Iteration 77: time=0.00, Factors=5 ## Iteration 78: time=0.00, Factors=5 ## Iteration 79: time=0.00, Factors=5 ## Iteration 80: time=0.00, Factors=5 ## Iteration 81: time=0.00, ELBO=991.17, deltaELBO=0.176 (0.00079169%), Factors=5 ## Iteration 82: time=0.00, Factors=5 ## Iteration 83: time=0.00, Factors=5 ## Iteration 84: time=0.00, Factors=5 ## Iteration 85: time=0.00, Factors=5 ## Iteration 86: time=0.00, ELBO=991.33, deltaELBO=0.159 (0.00071840%), Factors=5 ## Iteration 87: time=0.00, Factors=5 ## Iteration 88: time=0.00, Factors=5 ## Iteration 89: time=0.00, Factors=5 ## Iteration 90: time=0.00, Factors=5 ## Iteration 91: time=0.00, ELBO=991.48, deltaELBO=0.144 (0.00064963%), Factors=5 ## Iteration 92: time=0.00, Factors=5 ## Iteration 93: time=0.00, Factors=5 ## Iteration 94: time=0.00, Factors=5 ## Iteration 95: time=0.00, Factors=5 ## Iteration 96: time=0.00, ELBO=991.61, deltaELBO=0.133 (0.00059834%), Factors=5 ## Iteration 97: time=0.00, Factors=5 ## Iteration 98: time=0.00, Factors=5 ## Iteration 99: time=0.00, Factors=5 ## Iteration 100: time=0.00, Factors=5 ## Iteration 101: time=0.00, ELBO=991.73, deltaELBO=0.122 (0.00054741%), Factors=5 ## Iteration 102: time=0.00, Factors=5 ## Iteration 103: time=0.00, Factors=5 ## Iteration 104: time=0.00, Factors=5 ## Iteration 105: time=0.00, Factors=5 ## Iteration 106: time=0.00, ELBO=991.84, deltaELBO=0.111 (0.00050136%), Factors=5 ## Iteration 107: time=0.00, Factors=5 ## Iteration 108: time=0.00, Factors=5 ## Iteration 109: time=0.00, Factors=5 ## Iteration 110: time=0.00, Factors=5 ## Iteration 111: time=0.00, ELBO=991.94, deltaELBO=0.103 (0.00046562%), Factors=5 ## Iteration 112: time=0.00, Factors=5 ## Iteration 113: time=0.00, Factors=5 ## Iteration 114: time=0.00, Factors=5 ## Iteration 115: time=0.00, Factors=5 ## Iteration 116: time=0.00, ELBO=992.04, deltaELBO=0.096 (0.00043145%), Factors=5 ## ## Converged! ## ## ## ## ####################### ## ## Training finished ## ## ####################### ## ## ## Saving model in /tmp/Rtmp8h9dTR/mofa_20231203-163758.hdf5... The explained variance is visualized with the plot_variance_explained function: library(patchwork) library(ggplot2) plot_list &lt;- plot_variance_explained(model.trained, x = &quot;view&quot;, y = &quot;factor&quot;, plot_total = T) wrap_plots(plot_list, nrow = 2) + plot_annotation(title = &quot;Variance Explained per factor and assay&quot;, theme = theme(plot.title = element_text(hjust = 0.5))) The top weights for each assay using all five factors: custom_plotter &lt;- function(name) { p &lt;- plot_top_weights(model.trained, view = name, factors = &quot;all&quot;, nfeatures = 10) + labs(title = paste0(&quot;Top weights of the &quot;, name, &quot; assay&quot;)) } plot_list &lt;- lapply(c(&quot;microbiota&quot;, &quot;metabolites&quot;, &quot;biomarkers&quot;), custom_plotter) wrap_plots(plot_list, nrow = 3) &amp; theme(text = element_text(size = 8)) More tutorials and examples of using the package are found at link "],["viz-chapter.html", "Chapter 16 Visualization 16.1 Pre-analysis exploration 16.2 Diversity estimation 16.3 Statistical analysis", " Chapter 16 Visualization .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } Data visualization will inevitably shape interpretation and motivate the next steps of the analysis. A variety of visualization methods are available for microbiome analysis but the application requires careful attention to details. Knowledge on the available tools and their limitations plays an important role in selecting the most suitable methods to address a given question. This chapter introduces the reader to a number of visualization techniques found in this book, such as: barplots boxplots heatmaps ordination charts regression charts trees The toolkit which provides the essential plotting functionality includes the following packages: patchwork, cowplot, ggpubr and gridExtra: plot layout and multi-panel plotting miaViz: specific visualization tools for TreeSummaizedExperiment objects scater: specific visualization tools for SingleCellExperiment objects ggplot2, pheatmap, ggtree, sechm: composition heatmaps ANCOMBC, ALDEx2 and Maaslin2: visual differential abundance fido: tree-based methods for differential abundance plotly: animated and 3D plotting For systematic and extensive tutorials on the visual tools available in mia, readers can refer to the following material: microbiome tutorials 16.1 Pre-analysis exploration 16.1.1 Accessing row and column data SCE and TreeSE objects contain multiple layers of information in the form of rows, columns and meta data. The scater package supports in accessing, modifying and graphing the meta data related to features as well as samples. # list row meta data names(rowData(tse)) ## [1] &quot;Kingdom&quot; &quot;Phylum&quot; &quot;Class&quot; &quot;Order&quot; &quot;Family&quot; &quot;Genus&quot; &quot;Species&quot; # list column meta data names(colData(tse)) ## [1] &quot;X.SampleID&quot; &quot;Primer&quot; ## [3] &quot;Final_Barcode&quot; &quot;Barcode_truncated_plus_T&quot; ## [5] &quot;Barcode_full_length&quot; &quot;SampleType&quot; ## [7] &quot;Description&quot; Such meta data can be directly plotted with the functions plotRowData and plotColData. # obtain QC data tse &lt;- addPerCellQC(tse) tse &lt;- addPerFeatureQC(tse) # plot QC Mean against Species plotRowData(tse, &quot;mean&quot;, &quot;Species&quot;) + theme(axis.text.x = element_blank()) + labs(x = &quot;Species&quot;, y = &quot;QC Mean&quot;) # plot QC Sum against Sample ID, colour-labeled by Sample Type plotColData(tse, &quot;sum&quot;, &quot;X.SampleID&quot;, colour_by = &quot;SampleType&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(x = &quot;Sample ID&quot;, y = &quot;QC Sum&quot;) Alternatively, they can be converted to a data.frame object and passed to ggplot. # store colData into a data frame coldata &lt;- as.data.frame(colData(tse)) # plot Number of Samples against Sampling Site ggplot(coldata, aes(x = SampleType)) + geom_bar(width = 0.5) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(x = &quot;Sampling Site&quot;, y = &quot;Number of Samples&quot;) Further methods of application can be found in the chapters 5.3 and 7.1.1 and in a few external tutorials with open data. Additionally, rowData and colData allow manipulation and subsetting of large data sets into smaller units, as explained in chapter 4. 16.1.2 Viewing abundance and prevalence patterns Prior-to-analysis exploration may involve questions such as how microorganisms are distributed across samples (abundance) and what microorganisms are present in most of the samples (prevalence). The information on abundance and prevalence can be summarized into a jitter or density plot and a tree, respectively, with the miaViz package. Specifically, the functions plotAbundance, plotAbundanceDensity and plotRowTree are used, and examples on their usage are discussed throughout chapter 5. 16.2 Diversity estimation Alpha diversity is commonly measured as one of the diversity indices explained in chapter 7. Because the focus lies on each sample separately, one-dimensional plots, such as scatter, violin and box plots, are suitable. Beta diversity is generally evaluated as one of the dissimilarity indices reported in chapter 8. Unlike alpha diversity, samples are compared collectively to estimate the heterogeneity across them, therefore multidimensional plots, such as Shepard and ordination plots are suitable. alpha diversity beta diversity used metrics diversity indices dissimilarity indices metric dimensionality one-dimensional multidimensional suitable visualization scatter, violin, box plots Shepard, ordination plots In conclusion, visualization techniques for alpha and beta diversity significantly differ from one another. 16.2.1 Alpha diversity with scatter, violin and box plots The basic method to visualize the diversity values assigned to the different samples in a TSE object includes the following, where each data point represents one sample: # estimate shannon diversity index tse &lt;- mia::estimateDiversity(tse, assay.type = &quot;counts&quot;, index = &quot;shannon&quot;, name = &quot;shannon&quot;) # plot shannon diversity index, colour-labeled by Sample Type plotColData(tse, &quot;shannon&quot;, colour_by = &quot;SampleType&quot;) The several indices available for the evaluation of alpha diversity often return slightly divergent results, which can be visually compared with a multiple violin or box plot. For this purpose, plotColData (for violin plots) or ggplot (for box plots) are recursively applied to a number of diversity indices with the function lapply and the multi-panel plotting functionality of the patchwork package is then exploited. # estimate faith diversity index tse &lt;- mia::estimateFaith(tse, assay.type = &quot;counts&quot;) # store colData into a data frame coldata &lt;- as.data.frame(colData(tse)) # generate plots for shannon and faith indices # and store them into a list plots &lt;- lapply(c(&quot;shannon&quot;, &quot;faith&quot;), function(i) ggplot(coldata, aes_string(y = i)) + geom_boxplot() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())) # combine plots with patchwork plots[[1]] + plots[[2]] The analogous output in the form of a violin plot is obtained in chapter 7.1.3. In addition, box plots that group samples according to certain information, such as origin, sex, age and health condition, can be labeled with p-values for significant differences with the package ggsignif package, as shown in chapter 7.1.2. 16.2.2 Beta diversity with Shepard and coordination plots The scater package offers the general function plotReducedDim. In its basic form, it takes a TSE object and the results on sample similarity stored in the same object, which can be evaluated with the following coordination methods: runMDS runNMDS runPCA runTSNE runUMAP Since these clustering techniques allow for multiple coordinates or components, coordination plots can also span multiple dimensions, which is explained in chapter 20. # perform NMDS coordination method tse &lt;- runNMDS(tse, FUN = vegan::vegdist, name = &quot;NMDS&quot;) ## initial value 47.733208 ## iter 5 value 33.853364 ## iter 10 value 32.891200 ## final value 32.823570 ## converged # plot results of a 2-component NMDS on tse, # coloured-scaled by shannon diversity index plotReducedDim(tse, &quot;NMDS&quot;, colour_by = &quot;shannon&quot;) Multiple combinations of coordinates or dimensions can also be integrated into a multi-panel arrangement. # perform MDS coordination method tse &lt;- runMDS(tse, FUN = vegan::vegdist, method = &quot;bray&quot;, name = &quot;MDS&quot;, assay.type = &quot;counts&quot;, ncomponents = 3) # plot results of a 3-component MDS on tse, # coloured-scaled by faith diversity index plotReducedDim(tse, &quot;MDS&quot;, ncomponents = c(1:3), colour_by = &quot;faith&quot;) Similarly to iterating plotColData over indices of alpha diversity, lapply can be used in combination with patchwork to recursively apply plotReducedDim and visually compare results among various coordination methods. # generate plots for MDS and NMDS methods # and store them into a list plots &lt;- lapply(c(&quot;MDS&quot;, &quot;NMDS&quot;), plotReducedDim, object = tse, colour_by = &quot;shannon&quot;) # combine plots with patchwork plots[[1]] + plots[[2]] + plot_layout(guides = &quot;collect&quot;) For similar examples, readers are referred to chapter 8. Further material on the graphic capabilities of patchwork is available in its official package tutorial. 16.3 Statistical analysis 16.3.1 Heatmaps As described in chapter 9.1, bar plots and heatmaps can offer a useful insight into the composition of a community. Simple methods involve the functions plotAbundance and geom_tile in combination with scale_fill_gradientn from the packages miaViz and ggplot2, respectively. For instance, below the composition of multiple samples (x axis) is reported in terms of relative abundances (y axis) for the top 10 taxa at the Order rank. Bar plots and heatmaps with analogous information at the Phylum level are available in the aforementioned chapter. # agglomerate tse by Order tse_order &lt;- mergeFeaturesByRank(tse, rank = &quot;Order&quot;, onRankOnly = TRUE) # transform counts into relative abundance tse_order &lt;- transformAssay(tse_order, assay.type = &quot;counts&quot;, method = &quot;relabundance&quot;) # get top orders top_taxa &lt;- getTopFeatures(tse_order, top = 10, assay.type = &quot;relabundance&quot;) # leave only names for top 10 orders and label the rest with &quot;Other&quot; order_renamed &lt;- lapply(rowData(tse_order)$Order, function(x){if (x %in% top_taxa) {x} else {&quot;Other&quot;}}) rowData(tse_order)$Order &lt;- as.character(order_renamed) # plot composition as a bar plot plotAbundance(tse_order, assay.type = &quot;relabundance&quot;, rank = &quot;Order&quot;, order_rank_by = &quot;abund&quot;, order_sample_by = &quot;Clostridiales&quot;) To add a sample annotation, you can combine plots that you get from the output of plotAbundance. # Create plots plots &lt;- plotAbundance(tse_order, assay.type = &quot;relabundance&quot;, rank = &quot;Order&quot;, order_rank_by = &quot;abund&quot;, order_sample_by = &quot;Clostridiales&quot;, features = &quot;SampleType&quot;) # Modify the legend of the first plot to be smaller plots[[1]] &lt;- plots[[1]] + theme(legend.key.size = unit(0.3, &#39;cm&#39;), legend.text = element_text(size = 6), legend.title = element_text(size = 8)) # Modify the legend of the second plot to be smaller plots[[2]] &lt;- plots[[2]] + theme(legend.key.height = unit(0.3, &#39;cm&#39;), legend.key.width = unit(0.3, &#39;cm&#39;), legend.text = element_text(size = 6), legend.title = element_text(size = 8), legend.direction = &quot;vertical&quot;) # Load required packages library(ggpubr) library(patchwork) # Combine legends legend &lt;- wrap_plots(as_ggplot(get_legend(plots[[1]])), as_ggplot(get_legend(plots[[2]])), ncol = 1) # Remove legends from the plots plots[[1]] &lt;- plots[[1]] + theme(legend.position = &quot;none&quot;) plots[[2]] &lt;- plots[[2]] + theme(legend.position = &quot;none&quot;, axis.title.x=element_blank()) # Combine plots plot &lt;- wrap_plots(plots[[2]], plots[[1]], ncol = 1, heights = c(2, 10)) # Combine the plot with the legend wrap_plots(plot, legend, nrow = 1, widths = c(2, 1)) For more sophisticated visualizations than those produced with plotAbundance and ggplot2, the packages pheatmap and sechm provide methods to include feature and sample clusters in a heatmap, along with further functionality. # Agglomerate tse by phylum tse_phylum &lt;- mergeFeaturesByRank(tse, rank = &quot;Phylum&quot;, onRankOnly = TRUE) # Add clr-transformation on samples tse_phylum &lt;- transformAssay(tse_phylum, MARGIN = &quot;samples&quot;, method = &quot;clr&quot;, assay.type = &quot;counts&quot;, pseudocount=1) # Add z-transformation on features (taxa) tse_phylum &lt;- transformAssay(tse_phylum, assay.type = &quot;clr&quot;, MARGIN = &quot;features&quot;, method = &quot;z&quot;, name = &quot;clr_z&quot;) # Take subset: only samples from feces, skin, or tongue tse_phylum_subset &lt;- tse_phylum[ , tse_phylum$SampleType %in% c(&quot;Feces&quot;, &quot;Skin&quot;, &quot;Tongue&quot;) ] # Add clr-transformation tse_phylum_subset &lt;- transformAssay(tse_phylum_subset, method = &quot;clr&quot;, MARGIN=&quot;samples&quot;, assay.type = &quot;counts&quot;, pseudocount=1) # Does z-transformation tse_phylum_subset &lt;- transformAssay(tse_phylum_subset, assay.type = &quot;clr&quot;, MARGIN = &quot;features&quot;, method = &quot;z&quot;, name = &quot;clr_z&quot;) # Get n most abundant taxa, and subsets the data by them top_taxa &lt;- getTopFeatures(tse_phylum_subset, top = 20) tse_phylum_subset &lt;- tse_phylum_subset[top_taxa, ] # Gets the assay table mat &lt;- assay(tse_phylum_subset, &quot;clr_z&quot;) # Creates the heatmap pheatmap(mat) We can cluster both samples and features hierarchically and add them to the x and y axes of the heatmap, respectively. # Hierarchical clustering taxa_hclust &lt;- hclust(dist(mat), method = &quot;complete&quot;) # Creates a phylogenetic tree taxa_tree &lt;- as.phylo(taxa_hclust) # Plot taxa tree taxa_tree &lt;- ggtree(taxa_tree) + theme(plot.margin=margin(0,0,0,0)) # removes margins # Get order of taxa in plot taxa_ordered &lt;- get_taxa_name(taxa_tree) # to view the tree, run # taxa_tree Based on phylo tree, we decide to create three clusters. # Creates clusters taxa_clusters &lt;- cutree(tree = taxa_hclust, k = 3) # Converts into data frame taxa_clusters &lt;- data.frame(clusters = taxa_clusters) taxa_clusters$clusters &lt;- factor(taxa_clusters$clusters) # Order data so that it&#39;s same as in phylo tree taxa_clusters &lt;- taxa_clusters[taxa_ordered, , drop = FALSE] # Prints taxa and their clusters taxa_clusters ## clusters ## Chloroflexi 3 ## Actinobacteria 3 ## Crenarchaeota 3 ## Planctomycetes 3 ## Gemmatimonadetes 3 ## Thermi 3 ## Acidobacteria 3 ## Spirochaetes 2 ## Fusobacteria 2 ## SR1 2 ## Cyanobacteria 2 ## Proteobacteria 2 ## Synergistetes 2 ## Lentisphaerae 1 ## Bacteroidetes 1 ## Verrucomicrobia 1 ## Tenericutes 1 ## Firmicutes 1 ## Euryarchaeota 1 ## SAR406 1 The information on the clusters is then added to the feature meta data. # Adds information to rowData rowData(tse_phylum_subset)$clusters &lt;- taxa_clusters[order(match(rownames(taxa_clusters), rownames(tse_phylum_subset))), ] # Prints taxa and their clusters rowData(tse_phylum_subset)$clusters ## [1] 1 1 2 3 2 2 1 1 1 1 3 2 3 3 3 2 2 3 3 1 ## Levels: 1 2 3 Similarly, samples are hierarchically grouped into clusters, the most suitable number of clusters for the plot is selected and the new information is stored into the sample meta data. # Hierarchical clustering sample_hclust &lt;- hclust(dist(t(mat)), method = &quot;complete&quot;) # Creates a phylogenetic tree sample_tree &lt;- as.phylo(sample_hclust) # Plot sample tree sample_tree &lt;- ggtree(sample_tree) + layout_dendrogram() + theme(plot.margin=margin(0,0,0,0)) # removes margins # Get order of samples in plot samples_ordered &lt;- rev(get_taxa_name(sample_tree)) # to view the tree, run # sample_tree # Creates clusters sample_clusters &lt;- factor(cutree(tree = sample_hclust, k = 3)) # Converts into data frame sample_data &lt;- data.frame(clusters = sample_clusters) # Order data so that it&#39;s same as in phylo tree sample_data &lt;- sample_data[samples_ordered, , drop = FALSE] # Order data based on tse_phylum_subset &lt;- tse_phylum_subset[ , rownames(sample_data)] # Add sample type data sample_data$sample_types &lt;- unfactor(colData(tse_phylum_subset)$SampleType) sample_data ## clusters sample_types ## M11Plmr 2 Skin ## M31Plmr 2 Skin ## F21Plmr 2 Skin ## M31Fcsw 1 Feces ## M11Fcsw 1 Feces ## TS28 3 Feces ## TS29 3 Feces ## M31Tong 3 Tongue ## M11Tong 3 Tongue Now we can create heatmap with additional annotations. # Determines the scaling of colorss # Scale colors breaks &lt;- seq(-ceiling(max(abs(mat))), ceiling(max(abs(mat))), length.out = ifelse( max(abs(mat))&gt;5, 2*ceiling(max(abs(mat))), 10 ) ) colors &lt;- colorRampPalette(c(&quot;darkblue&quot;, &quot;blue&quot;, &quot;white&quot;, &quot;red&quot;, &quot;darkred&quot;))(length(breaks)-1) pheatmap(mat, annotation_row = taxa_clusters, annotation_col = sample_data, breaks = breaks, color = colors) The package sechm allows for further visual capabilities and flexibility. In this case, the clustering step is automatically performed by the plotting function and does not need to be executed in advance. # Stores annotation colros to metadata metadata(tse_phylum_subset)$anno_colors$SampleType &lt;- c(Feces = &quot;blue&quot;, Skin = &quot;red&quot;, Tongue = &quot;gray&quot;) # Create a plot sechm(tse_phylum_subset, features = rownames(tse_phylum_subset), assayName = &quot;clr&quot;, do.scale = TRUE, top_annotation = c(&quot;SampleType&quot;), gaps_at = &quot;SampleType&quot;, cluster_cols = TRUE, cluster_rows = TRUE) It is also possible to create an analogous heatmap by just using the ggplot2 package. However, a relatively long code is required to generate an identical output. # Add feature names to column as a factor taxa_clusters$Feature &lt;- rownames(taxa_clusters) taxa_clusters$Feature &lt;- factor(taxa_clusters$Feature, levels = taxa_clusters$Feature) # Create annotation plot row_annotation &lt;- ggplot(taxa_clusters) + geom_tile(aes(x = NA, y = Feature, fill = clusters)) + coord_equal(ratio = 1) + theme( axis.text.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.y=element_blank(), axis.title.x = element_text(angle = 90, vjust = 0.5, hjust=1), plot.margin=margin(0,0,0,0), ) + labs(fill = &quot;Clusters&quot;, x = &quot;Clusters&quot;) # to view the notation, run # row_annotation # Add sample names to one of the columns sample_data$sample &lt;- factor(rownames(sample_data), levels = rownames(sample_data)) # Create annotation plot sample_types_annotation &lt;- ggplot(sample_data) + scale_y_discrete(position = &quot;right&quot;, expand = c(0,0)) + geom_tile(aes(y = NA, x = sample, fill = sample_types)) + coord_equal(ratio = 1) + theme( axis.text.x=element_blank(), axis.text.y=element_blank(), axis.title.x=element_blank(), axis.ticks.x=element_blank(), plot.margin=margin(0,0,0,0), axis.title.y.right = element_text(angle=0, vjust = 0.5) ) + labs(fill = &quot;Sample types&quot;, y = &quot;Sample types&quot;) # to view the notation, run # sample_types_annotation # Create annotation plot sample_clusters_annotation &lt;- ggplot(sample_data) + scale_y_discrete(position = &quot;right&quot;, expand = c(0,0)) + geom_tile(aes(y = NA, x = sample, fill = clusters)) + coord_equal(ratio = 1) + theme( axis.text.x=element_blank(), axis.text.y=element_blank(), axis.title.x=element_blank(), axis.ticks.x=element_blank(), plot.margin=margin(0,0,0,0), axis.title.y.right = element_text(angle=0, vjust = 0.5) ) + labs(fill = &quot;Clusters&quot;, y = &quot;Clusters&quot;) # to view the notation, run # sample_clusters_annotation # Order data based on clusters and sample types mat &lt;- mat[unfactor(taxa_clusters$Feature), unfactor(sample_data$sample)] # ggplot requires data in melted format melted_mat &lt;- melt(mat) colnames(melted_mat) &lt;- c(&quot;Taxa&quot;, &quot;Sample&quot;, &quot;clr_z&quot;) # Determines the scaling of colorss maxval &lt;- round(max(abs(melted_mat$clr_z))) limits &lt;- c(-maxval, maxval) breaks &lt;- seq(from = min(limits), to = max(limits), by = 0.5) colours &lt;- c(&quot;darkblue&quot;, &quot;blue&quot;, &quot;white&quot;, &quot;red&quot;, &quot;darkred&quot;) heatmap &lt;- ggplot(melted_mat) + geom_tile(aes(x = Sample, y = Taxa, fill = clr_z)) + theme( axis.title.y=element_blank(), axis.title.x=element_blank(), axis.ticks.y=element_blank(), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), plot.margin=margin(0,0,0,0), # removes margins legend.key.height= unit(1, &#39;cm&#39;) ) + scale_fill_gradientn(name = &quot;CLR + Z transform&quot;, breaks = breaks, limits = limits, colours = colours) + scale_y_discrete(position = &quot;right&quot;) heatmap library(patchwork) # Create layout design &lt;- c( patchwork::area(3, 1, 4, 1), patchwork::area(1, 2, 1, 3), patchwork::area(2, 2, 2, 3), patchwork::area(3, 2, 4, 3) ) # to view the design, run # plot(design) # Combine plots plot &lt;- row_annotation + sample_clusters_annotation + sample_types_annotation + heatmap + plot_layout(design = design, guides = &quot;collect&quot;, # Specify layout, collect legends # Adjust widths and heights to align plots. # When annotation plot is larger, it might not fit into # its column/row. # Then you need to make column/row larger. # Relative widths and heights of each column and row: # Currently, the width of the first column is 15 % and the height of # first two rows are 30 % the size of others # To get this work most of the times, you can adjust all sizes to be 1, i.e. equal, # but then the gaps between plots are larger. widths = c(0.15, 1, 1), heights = c(0.3, 0.3, 1, 1)) # plot # Create layout design &lt;- c( patchwork::area(4, 1, 5, 1), patchwork::area(4, 2, 5, 2), patchwork::area(1, 3, 1, 4), patchwork::area(2, 3, 2, 4), patchwork::area(3, 3, 3, 4), patchwork::area(4, 3, 5, 4) ) # to view the design, run # plot(design) # Combine plots plot &lt;- taxa_tree + row_annotation + sample_tree + sample_clusters_annotation + sample_types_annotation + heatmap + plot_layout(design = design, guides = &quot;collect&quot;, # Specify layout, collect legends widths = c(0.2, 0.15, 1, 1, 1), heights = c(0.1, 0.15, 0.15, 0.25, 1, 1)) plot Heatmaps find several other applications in biclustering and multi-assay analyses. These are discussed further in chapters 10 and 15. "],["training.html", "Chapter 17 Training 17.1 Checklist 17.2 Recommended software 17.3 Study material 17.4 Support and resources 17.5 Further reading 17.6 Code of Conduct", " Chapter 17 Training .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } The page provides practical information to support training and self-study. 17.1 Checklist Brief checklist to prepare for training (see below for links). Install the recommended software If the time allows, watch the short online videos and familiarize with the other available material Join Gitter online chat for support 17.2 Recommended software We recommend to install and set up the relevant software packages on your own computer as this will support later use. The essential components to install include: R (the latest official release) RStudio; choose “Rstudio Desktop” to download the latest version. Check the Rstudio home page for more information. RStudio is optional. Install key R packages (Section 2 provides an installation script) After a successful installation you can consider trying out examples from Section 19 already before training. You can run the workflows by simply copy-pasting examples. You can then test further examples from this tutorial, modifying and applying these techniques to your own data. Plain source code for the individual chapters of this book are available via Github 17.3 Study material We encourage to familiarize with the material and test examples in advance but this is optional: Introduction to data analysis with R and Bioconductor (for beginners with R) Short online videos on microbiome data science with R/Bioconductor Quarto presentations Orchestrating Microbiome Analysis with Bioconductor (OMA) (this book) Other outreach material Exercises for self-study Resources and links to complementary external material 17.4 Support and resources For online support on installation and other matters, join us at Gitter. You are also welcome to connect through various channels with our broader developer and user community. 17.5 Further reading The following online books provide good general data science background: (Data science basics in R](https://r4ds.had.co.nz) (Modern Statistics for Modern Biology)[https://www.huber.embl.de/msmb/] open access book (Holmes S, Huber W) The Bioconductor project (background on the Bioconductor project; Carpentries workshop) 17.6 Code of Conduct We support the Bioconductor Code of Conduct. The community values an open approach to science that promotes sharing of ideas, code, software and expertise a kind and welcoming environment, diversity and inclusivity community contributions and collaboration "],["resources.html", "Chapter 18 Resources 18.1 Data containers 18.2 R programming resources 18.3 Reproducible reporting with Quarto", " Chapter 18 Resources 18.1 Data containers 18.1.1 Data container documentation SingleCellExperiment (Lun and Risso 2020) Online tutorial Project page Publication SummarizedExperiment (Morgan et al. 2020) Online tutorial Project page TreeSummarizedExperiment (Huang 2020) Online tutorial Project page Publication MultiAssayExperiment (Ramos et al. 2017) Online tutorial Project page Publication 18.1.2 Other relevant containers DataFrame which behaves similarly to data.frame, yet efficient and fast when used with large datasets. DNAString along with DNAStringSet,RNAString and RNAStringSet efficient storage and handling of long biological sequences are offered within the Biostrings package (Pagès et al. 2020). GenomicRanges ((Lawrence et al. 2013)) offers an efficient representation and manipulation of genomic annotations and alignments, see e.g. GRanges and GRangesList at An Introduction to the GenomicRangesPackage. NGS Analysis Basics provides a walk-through of the above-mentioned features with detailed examples. 18.1.3 phyloseq: an alternative container for microbiome data The phyloseq package and class became the first widely used data container for microbiome data science in R. Many methods for taxonomic profiling data are readily available for this class. We provide here a short description how phyloseq and *Experiment classes relate to each other. assays : This slot is similar to otu_table in phyloseq. In a SummarizedExperiment object multiple assays, raw counts, transformed counts can be stored. See also (2017) for storing data from multiple experiments such as RNASeq, Proteomics, etc. rowData : This slot is similar to tax_table in phyloseq to store taxonomic information. colData : This slot is similar to sample_data in phyloseq to store information related to samples. rowTree : This slot is similar to phy_tree in phyloseq to store phylogenetic tree. In this book, you will encounter terms such as FeatureIDs and SampleIDs. FeatureIDs : These are basically OTU/ASV ids which are row names in assays and rowData. SampleIDs : As the name suggests, these are sample ids which are column names in assays and row names in colData. FeatureIDs and SampleIDs are used but the technical terms rownames and colnames are encouraged to be used, since they relate to actual objects we work with. 18.1.3.1 Benchmarking TreeSE with phyloseq TreeSE objects can be converted into phyloseq objects and vice versa, therefore it is possible to compare the two containers in terms of computational efficiency. Remarkably, TreeSE and phyloseq were benchmarked against one another in mia v1.2.3 and phyloseq v1.38.0, respectively. 5 standard microbiome analysis operationswere applied to 4 datasets of varying size with both containers. In a nutshell, TreeSE and phyloseq showed a similar performance for datasets of small and medium size for most of the operations. However, TreeSE performed more efficiently as the size of the datasets increased. Further details on such results can be found in the benchmarking repository. 18.1.3.2 Resources on phyloseq The phyloseq container provides analogous methods to TreeSE. The following material can be used to familiarize with such alternative methods: List of R tools for microbiome analysis phyloseq (P. McMurdie and Holmes 2013) microbiome tutorial microbiomeutilities Bioconductor Workflow for Microbiome Data Analysis: from raw reads to community analyses (Ben J. Callahan et al. 2016). 18.2 R programming resources 18.2.1 Base R and RStudio If you are new to R, you could try swirl for a kickstart to R programming. Further support resources are available through the Bioconductor project (Huber et al. 2015). Base R and RStudio cheatsheets Package-specific cheatsheets Visualization with ggplot2 R graphics cookbook 18.2.2 Bioconductor Classes S4 system S4 class system has brought several useful features to the object-oriented programming paradigm within R, and it is constantly deployed in R/Bioconductor packages (Huber et al. 2015).   Online Document: Hervé Pagès, A quick overview of the S4 class system. Laurent Gatto, A practical tutorial on S4 programming How S4 Methods Work (J. Chambers 2006)   Books: John M. Chambers. Software for Data Analysis: Programming with R. Springer, New York, 2008. ISBN-13 978-0387759357 (J. M. Chambers 2008) I Robert Gentleman. R Programming for Bioinformatics. Chapman &amp; Hall/CRC, New York, 2008. ISBN-13 978-1420063677 (R. Gentleman 2008) 18.3 Reproducible reporting with Quarto 18.3.1 Learn Quarto Reproducible reporting is the starting point for robust interactive data science. Perform the following tasks: If you are entirely new to Quarto, take this short tutorial to get introduced to the most important functions within Quarto. Then experiment with different options from the Quarto cheatsheet. Create a Quarto template in RStudio, and render it into a document (markdown, PDF, docx or other format). In case you are new to Quarto, its documentation provides guidelines to use Quarto with the R language (here) and the RStudio IDE (here). Further examples are tips for Quarto are available in this online tutorial to interactive reproducible reporting. 18.3.2 Additional material on Rmarkdown Being able to use Quarto in R partly relies on your previous knowledge of Rmarkdown. The following resources can help you get familiar with Rmarkdown: Online tutorial Cheatsheet Documentation Dr. C Titus Brown’s tutorial Figure sources: Original article - Huang R et al. (2021) TreeSummarizedExperiment: a S4 class for data with hierarchical structure. F1000Research 9:1246. (Huang et al. 2021) Reference Sequence slot extension - Lahti L et al. (2020) Upgrading the R/Bioconductor ecosystem for microbiome research F1000Research 9:1464 (slides). "],["exercises.html", "Chapter 19 Exercises 19.1 Basics of R/Bioconductor 19.2 Workflows 19.3 Data containers: TreeSE 19.4 Data manipulation 19.5 Abundance tables 19.6 Community (alpha) diversity 19.7 Community similarity 19.8 Differential abundance 19.9 Visualization 19.10 Multiomics", " Chapter 19 Exercises Here you can find assignments on different topics. Tips for exercises: Add comments that explain what each line or lines of code do. This helps you and others understand your code and find bugs. Furthermore, it is easier for you to reuse the code, and it promotes transparency. Interpret results by comparing them to literature. List main findings, so that results can easily be understood by others without advanced knowledge on data analytics. Avoid hard-coding. Use variables which get values in the beginning of the pipeline. That way it is easier for you to modify parameters and reuse the code. 19.1 Basics of R/Bioconductor Bioconductor training material has been contributed to Carpentries. You can check the following lessons for basic background of R and Bioconductor. Introduction to data analysis with R and Bioconductor Introduction to the Bioconductor project 19.2 Workflows 19.2.1 Reproducible reporting with Quarto The following batch of exercises walks you through typical use cases of Quarto in RStudio. Before heading to the exercises, it is recommended to read the Quarto guidelines for RStudio 19.2.1.1 New document This exercise gets you started with creating a Quarto document and adding text to it with typing conventions borrowed from the markdown syntax. Feel free to render the document with the Render button after each step to see the changes in the final report. Open RStudio and create a new Quarto file named My first Quarto. Add the subtitle My first section and write some text of your choice underneath. You can choose the level of headings by the number of preceding hashes (#). Add a subsection named List of items and list three items underneath, both ordered and unordered. You can initialize items with numbers (1., 2., 3., …) or stars (*) for the ordered and unordered case, respectively. Add another subsection named Link to web and add a clickable link to the OMA book, using the pattern [text](url). Render the document and check its appearance Nice start! You are now able to create a Quarto document, understand its syntax and can render it into a reproducible report. If you got stuck, you can look up the docs on creating and rendering Quarto documents. 19.2.1.2 Code chunks While customizable text is nothing new by itself, the advantage of Quarto (and previously Rmakdown) is to combine text with code in R or other programming languages, so that both the analytical pipeline and verbal description can be put in one place. In this exercise, we learn how to write and run code in Quarto. Open RStudio and create a new Quarto file. Initialize a code chunk by pressing alt + cmd + i and define the variables A &lt;- \"my name\" and B &lt;- 0 in it. Write the text Below is my first code chunk just above the code chunk. Initialize one more code chunk and add 100 to the variable B in it. Write the text Below I change variable B just above the second chunk. Extra: Write the following line of text: my name is A and I am B years old, where A and B are variables defined in the code chunks upstream and change if those variables are modified. Be aware that inline code can be added as &gt; r my_inline_command (without &gt;). Good job. You are now able to combine text and code in a Quarto document. If you got stuck, you can refer to the Quarto docs on using code chunks. 19.2.1.3 Knitr options Code chunks can be greatly customized in terms of visibility and execution, output size and location and much more. This is possible with the knitr chunk options, which usually appear at the beginning of the target chunk with the syntax #| option-name: value, also described here. In this exercise, we explore part of the knitr potential. Open RStudio and create a new Quarto file. Initialize three code chunks and label them as setup, fig-box and tbl-coldata, respectively. Remember that the name of a chunk can be specified with the label option. Write the following code in the corresponding chunk and render the document. # setup library(mia) data(&quot;GlobalPatterns&quot;, package = &quot;mia&quot;) tse &lt;- GlobalPatterns # this line sets some options for all the chunks (global chunk options) knitr::opts_chunk$set(message = FALSE, warning = FALSE) # fig-box boxplot(colSums(assay(tse)) ~ tse$SampleType) # tbl-coldata knitr::kable(head(colData(tse))) Set include: false in the setup chunk, fig-width: 10 in the fig-box chunk and echo: false in the tbl-coldata chunk. Render the document again and find the differences from before. Add the options fig-cap and tab-cap to the fig-box and tbl-coldata chunks, respectively. They require some text input, which makes for the caption of the figures or tables. Extra: Create a cross-reference to fig-box and tbl-coldata in the text above the respective code chunk. You can do that with the syntax @chunk-name. Extra: Define a custom folder for storing figures with fig-path. Insert it in knitr::opts_chunk$set, so that it applies globally to all the figures generated in the document. Congratulations! You are now familiar with the great potential and flexibility of knitr chunk options. An exhaustive list of available options can be found in the knitr documentation. 19.2.1.4 YAML instructions The box at the beginning of every Quarto document contains yaml options that let you define the metadata of the document. They will affect the appearance of the document when it is rendered. By default, the box includes yaml options for the title, format and editor to be used, but much more information on layout, code execution and figures can be specified. A comprehensive list of yaml options is available here. In this exercise, we will get a tiny taste of such functionality. Open RStudio and create a new Quarto file. In the yaml box at the beginning of the document, change the title from Untitled to My first Quarto. In the same box, add the two lines author and date followed by your name and today’s date, respectively. Render the document and check its appearance. Extra: Set toc: true to produce a table of contents. This line should follow format and html at the second level of indentation. Well done! Now you are able to specify yaml options and understand how they affect your Quarto document. If you got stuck, you can check this section of the Quarto documentation. 19.2.1.5 Quarto parameters An advanced feature of Quarto consists of execution parameters, which are externally pre-defined variables that are also accessible in the Quarto document. They can be specified in the yaml box as params. Here we learn how to use them. Open RStudio and create a new Quarto file. In the yaml box at the beginning of the document, add a line named params followed by an indented line with gamma: 10 Initialize a code chunk and type str(params$gamma) in it. Render the document and check what happened. Define one more parameter beta: 3 and multiply gamma by beta in a code chunk below. Render the document again and check what happened. Well done! You can now use an advanced feature of Quarto such as parameters. If you got stuck, here you can find more information about parameter definition and usage. 19.3 Data containers: TreeSE TreeSE containers represent the working unit of the mia package. In the following exercises we learn how to construct, explore and work with them. A few demo datasets can be imported with mia and can be accessed as explained in chapter 3.3. 19.3.1 Constructing a data object Here we cover how to construct a TreeSE from CSV files, using the components of OKeefeDSData from the microbiomeDataSets package as an example dataset. Fetch or download the files in this directory. Read in the csv files with read.csv and store them into the variables assays, rowdata and coldata, respectively. Create a TreeSE from the individual components with TreeSummarizedExperiment. Note that the function requires three arguments: assays, rowData and colData, to which you can give the appropriate item. Check that importing is done correctly. E.g., choose random samples and features, and check that their values equal between raw files and TreeSE. Usefuls functions: DataFrame, TreeSummarizedExperiment, matrix, rownames, colnames, SimpleList 19.3.2 Importing data Raw data of different types can be imported as a TreeSE with a number of functions explained in chapter 3.4.2. You can also check the function reference in the mia package. Get familiar with the microbiome data repository and read the instructions in its README to import and construct datasets from there. Import data from another format (functions: loadFromMetaphlan | loadFromMothur | loadFromQIIME2 | makeTreeSummarizedExperimentFromBiom | makeTreeSummarizedExperimentFromDADA2 …) Try out conversions between TreeSE and phyloseq data containers (makeTreeSummarizedExperimentFromPhyloseq; makephyloseqFromTreeSummarizedExperiment) 19.3.3 Preliminary exploration Import the mia package, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Get a summary about the TreeSE with summary. What is the mean count across samples? How many features recur only once (singletons)? Check the dimensions of the TreeSE with dim or alternatively with nrow and ncol. How many samples and features are present? List sample and features names with rownames and colnames. Check what information about samples and features is contained by the colData and rowData of the TreeSE with names. Extra: Calculate the number of unique taxa for each taxonomic rank. You can use apply to count unique elements for each column of rowData. 19.3.4 Assay retrieval Import the mia package, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. List the names of all available assays with assayNames. Fetch the list of assays with assays. Retrieve the first assay of the TreeSE with assay, where the second argument can be either the name or the index of the desired assay. Well done! You can now locate and retrieve individual assays of a TreeSE. If you got stuck, you can refer to chapter 3.2.1 of this book. 19.3.5 Sample information Import the mia package, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Check the names of the samples with colnames. List the information on samples available in colData with names. Visualize the colData with View and briefly look at the information stored in the different columns. Get the abundances of all features for a specific sample, such as ID34, for an assay of your choice. 19.3.6 Feature information Import the mia package, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Check the names of the features with rownames. List the information on features available in rowData with names. Visualize the rowData with View and briefly look at the information stored in the different columns. Get the abundances for a specific feature, such as OTU1810, in all the samples. You can access feature-specific abundances for an assay of your choice. Extra: Create a taxonomy tree based on the taxonomy mappings with addTaxonomyTree and display its content with taxonomyTree and ggtree. If you got stuck, you can look up chapters @fref{datamanipulation} and 6.2.1 on how to pick specific abundances and generate row trees, respectively. 19.3.7 Other elements Try to extract some of the other TreeSE elements listed in chapter 3. However, such data are not always included. Import the mia package, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Fetch the metadata of the TreeSE. Is there any iformation available? Access the phylogenetic tree with rowTree. How big is it in terms of tips and nodes. If you like you can visualize it with ggtree. Check if a sample tree is available with colTree, which is suitable for hierarchical or nested study designs. If present, obtain the information on feature DNA sequences from the DNA sequence slot. 19.4 Data manipulation 19.4.1 Subsetting Subset the TreeSE object to specific samples Subset the TreeSE object to specific features Subset the TreeSE object to specific samples and features 19.4.2 Library sizes Calculate library sizes Subsample / rarify the counts (see: subsampleCounts) Useful functions: nrow, ncol, dim, summary, table, quantile, unique, addPerCellQC, mergeFeaturesByRank 19.4.3 Prevalent and core taxonomic features Estimate prevalence for your chosen feature (row, taxonomic group) Identify all prevalent features and subset the data accordingly Report the thresholds and the dimensionality of the data before and after subsetting Visualize prevalence Useful functions: getPrevalence, getPrevalentFeatures, subsetByPrevalentFeatures 19.4.4 Data exploration Summarize sample metadata variables. (How many age groups, how they are distributed? 0%, 25%, 50%, 75%, and 100% quantiles of library size?) Create two histograms. Another shows the distribution of absolute counts, another shows how CLR transformed values are distributed. Visualize how relative abundances are distributed between taxa in samples. Useful functions: nrow, ncol, dim, summary, table, quantile, unique, transformAssay, ggplot, wilcox.test, mergeFeaturesByRank, plotAbundance 19.4.5 Other functions Merge data objects (merge, mergeSEs) Melt the data for visualization purposes (meltAssay) 19.4.6 Assay transformation Import the mia package, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Transform the counts assay into relative abundances with transformAssay and store it into the TreeSE as an assay named relabund (see chapter 6.4). Similarly, perform a clr transformation on the counts assay with a pseudocount of 1 and add it to the TreeSE as a new assay. List the available assays by name with assays. Access the clr assay and select a subset of its first 100 features and 10 samples. Remember that assays are subsettable with assay[row_idx, col_idx]. Take the same subset from the TreeSE, and check how this affects the individual transformed assays. TreeSE can also be subsetted with tse[row_idx, col_idx]. Extra: If the data has phylogenetic tree, perform the phILR transformation. 19.5 Abundance tables 19.5.1 Taxonomic levels Import the mia package, load one of the example data sets mentioned in Chapter 3.3 with data (you need one with taxonomic information at Phylum level) and store it into a variable named tse. List the available taxonomic ranks in the data with taxonomyRanks. Agglomerate the data to Phylum level with mergeFeaturesByRank and the appropriate value for Rank. Report the dimensions of the TreeSE before and after agglomerating. You can use dim for that. Extra: Perform CLR transformation on the data. Does this affect agglomeration? Extra: List full taxonomic information for a few selected taxa, such as OTU1 and OTU1368. For that you can use mapTaxonomy on a specific subset of the TreeSE. 19.5.2 Alternative experiments Import the mia package, load one of the example data sets mentioned in Chapter 3.3 with data (you need one with taxonomic information) and store it into a variable named tse. Check the taxonomic ranks of the features with taxonomyRanks. What is the deepest taxonomic rank available? Agglomerate the TreeSE to each taxonomic rank and store the resulting experiments as altExps. This can be performed automatically with splitByRanks. Check the names of the generated altExps with altExpNames and retrieve a complete list with altExps. Retrieve the data agglomerated by genus from the corresponding altExp. As for assays, you can access the desired altExp by name or index. Extra: Split the data based on other features with splitOn. 19.6 Community (alpha) diversity 19.6.1 Estimation Import the mia package, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Calculate multiple alpha diversity indices with estimateDiversity without any additional arguments. Check the names of colData with names. Can you identify which columns contain the alpha diversity indices? Extra: Agglomerate the TreeSE by phylum and compare the mean Shannon diversity of the original experiment with its agglomerated version. You can use mergeFeaturesByRank to perform agglomeration and mean to calculate the mean values of the respective columns in colData. 19.6.2 Visualization Import the mia and scater packages, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Calculate Shannon diversity index and Faith’s phylogenetic diversity with estimateDiversity and the appropriate arguments for index. Make a boxplot of Shannon diversity on the y axis and sample type on the x axis with plotColData. Repeat the previous point with Faith’s phylogenetic diversity and compare the sample distributions of the two alpha diversity indices. How greatly do they differ? Extra: Make a scatterplot of Shannon diversity on the y axis and Faith’s phylogenetic diversity on the x axis with plotColData. Colour the points by sample type with the appropriate optional argument. 19.6.3 Correlation Import the mia and scater packages, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Calculate coverage and Shannon diversity index with estimateDiversity and the appropriate arguments for index. Test the correlation between the two indices with cor.test. Remember that colData parameters are accessible with tse$param_name. Use Kendall tau coefficients as method to measure correlation. Is the correlation weak or strong, significant or not? Make a scatterplot of Shannon diversity index on the y axis and coverage on the x axis. You can do that with plotColData. How do the two indices relate to one another? Extra: Compute the library size of the samples by applying colSums to the counts assay of the TreeSE, and test the correlation of library size with Shannon diversity or coverage. Which index is more correlated with library size? In this example, we inspected the correlation between two related variables, also known as multicollinearity, and checked the correlation to library size, which is part of quality control. However, the correlation between alpha diversity and other numerical data about samples, such as participant’s age and weight, also represent an important analysis in several studies. 19.6.4 Differences between groups Import the mia package, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Calculate the Gini-Simpson diversity with estimateDiversity and the appropriate argument for index. Set name to simpson. You will use this name to access the diversity index from colData. Inspect the Diet column in the colData. Determine how the samples are grouped in terms of diet. You can see the number of unique elements in a column with unique. Test differences in Gini-Simpson diversity between different diets with kruskal.test. Remember that colData parameters are accessible with tse$param_name. Is diversity significantly different between vegan and mixed diet? To visualize that, make a boxplot of Gini-Simpson diversity on the y axis and diet on the x axis with plotColData. Extra: Repeat points 3 through 5, this time for age groups. Make sure that you are using an appropriate statistical test for the number of groups and distribution. 19.7 Community similarity 19.7.1 Reduced dimensions retrieval Import the mia package, load enterotype with data and store it into a variable named tse. List all available reduced dimensions with reducedDims. At this point, no reducedDims are likely found, because we haven’t created any yet. Perform PCA and store its output in the TreeSE by running tse &lt;- runPCA(tse, assay.type = \"counts\"). Note that it is required to specify the assay on which dimensionality reduction should be conducted. View the names of all available reduced dimensions with reducedDimNames. Has something new appeared? Extra: Access the PCA reducedDim object with reducedDim and explore its content. How are the different dimensions stored? Try to extract an array with only the values from the second dimension by indexing the object with [ , 2]. 19.7.2 Visualization basics with PCA Import the mia and scater packages, load enterotype with data and store it into a variable named tse. Perform a 3-component PCA based on the counts assay. You can use runPCA and set the optional arguments ncomponents and assay.type to the appropriate values. Plot the first two dimensions of PCA with plotReducedDim, to which you should give the appropriate reducedDim name as the second argument. Note that by default only the first two dimensions are shown. Check which information is stored in the ColData of the TreeSE. What would be worth visualizing in our coordination plot? Make the same plot again, but this time colour the observations by Enterotype. You can do that by setting colour_by to the appropriate colname in the colData of the TreeSE. Extra: Plot all three dimensions of PCA with plotReducedDim and the optional argument ncomponents. Colour observations by Enterotype. Which pair of dimensions best explains the variance between Enterotypes? 19.7.3 Principal Coordinate Analysis (PCoA) PCoA turns out to be particularly relevant for microbiome analysis, because unlike PCA it can generate reduced dimensions from distances other than Euclidean. There are several ecological distances to choose from and you can find many of them under methods in the vignettes of vegan::vegdist. Import the mia and scater packages, load enterotype with data and store it into a variable named tse. Transform the counts assay to relative abundances with transformAssay. Perform a Multi-Dimensional Scaling (MDS) based on the relative abundance assay in terms of Bray-Curtis dissimilarity. You can use runMDS with the compulsory argument FUN = vegan::vegdist. Plot the first two dimensions of PCA with plotReducedDim, to which you should give the appropriate reducedDim name as the second argument. Colour the observations by Enterotype with colour_by. Extra: Perform MDS again with runMDS, but this time use Jaccard dissimilarity. The distance metric to use can be defined with the optional argument method, choosing from the methods in ?vegan::vegdist. If you don’t want to overwrite the reducedDim object made in point 3, set name to a name of your choice. Visualize and compare it to the plot from point 4. Good job! You are now able to produce and visualize reduced dimensions of a TreeSE. runMDS is actually one of several algorithms for PCoA and dimensionality reduction, which you can find in section 8.1.2. 19.7.4 PERMANOVA analysis In this exercise we focus on studying the weight of variables on the microbiome composition. Significance of each variable on beta diversity is tested with PERMANOVA (point 4) and the homogeneity assumption is also be controlled with a PERMDISP2 analysis (point 5). Import the mia and vegan packages, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Transform the counts assay into relative abundances with transformAssay. Extract the relative abundance assay, transpose it and save it into a variable named relabund_assay. Perform PERMANOVA with adonis2 to see how much Diet can explain the relative abundance assay (formula = relabund_assay ~ Diet) in terms of Bray-Curtis dissimilarity (method = \"bray\"). Also set data = colData(tse) by = \"margin\" and permutations = 99. What do the results tell you about Diet with respect to beta diversity? Extra: Test homogeneity of distribution across Diet groups with anova(betadisper(my_mat), my_groups, where my_mat is the Bray-Curtis dissimilarity matrix of relabund_assay calculated with vegdist(relabund_assay, \"bray\") and my_groups is the vector of Diet values obtained from the colData of the TreeSE. Well done! You went through testing the effect and significance of Diet on beta diversity. Keep in mind that the formula fed to adonis2 can take more than one independent variable, so that you can also (and very often should) include covariates of your studies. 19.7.5 Redundancy analysis (RDA) Here we apply RDA, an ordination method that provides dimensions with the largest variation between the data based in the light of the specified variables (point 3). The results of RDA are usually assessed with PERMANOVA (point 5) and the homogeneity assumption should be checked as in the previous exercise. This is a relatively complex procedure, but the way this is broken down into steps below will hopefully make more sense. Import the mia and vegan packages, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Transform the counts assay into relative abundances with transformAssay. Perform RDA with calculateRDA to see how much Diet can explain the relative abundance assay (formula = assay ~ Diet and assay.type = relabundance) in terms of Bray-Curtis dissimilarity (method = \"bray\"). Extract the RDA dimensions from the appropriate reducedDim slot with attr(reducedDim(tse, \"RDA\"), \"rda) and store it into rda. Test the effect and significance of Diet on beta diversity by PERMANOVA with anova.cca. Feed this function with rda and set by = \"margin\" and permutations = 99, respectively. What do the results tell you about Diet? Extra: Check what other parameters are stored in the colData of peerj13075, add them to the formula (formula = assay ~ Diet + ...) of calculateRDA and proceed to see how that changes the results of PERMANOVA. Well done! You went through an RDA analysis followed by significance testing with PERMANOVA and BETADISPER2. In the next exercise we’ll go deeper quantify the contributions to beta diversity. 19.7.6 Beta diversity analysis This exercise prompts you to implement a workflow with distance-based RDA (dbRDA). You can refer to chapter 8.3.1 for a step-by-step walkthrough, which may be simplified in the future. Import the mia and vegan packages, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Create dbRDA with Bray-Curtis dissimilarities on relative abundances. Use PERMANOVA. Can differences between samples be explained with variables of sample meta data? Analyze diets’ association on beta diversity. Calculate dbRDA and then PERMANOVA. Visualize coefficients. Which taxa’s abundances differ the most between samples? Interpret your results. Is there association between community composition and location? What are those taxa that differ the most; find information from literature. Useful functions: runMDS, runRDA, anova.cca, transformAssay, mergeFeaturesByRank, ggplot, plotReducedDim, vegan::adonis2 19.8 Differential abundance 19.8.1 Standard analysis with ALDEx2 Import the mia and ALDEx2 packages, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Agglomerate the TreeSE by genus and filter by a prevalence of 10%. You can perform both operations in one go with subsetByPrevalentFeatures by specifying the rank and prevalence arguments. Model the counts assay of the TreeSE with aldex.clr and store it into the variable x. As a second argument, provide the grouping variable Diet, which is contained in a column of the colData. Feed x to the functions aldex.ttest to erform t-test and to aldex.effect to estimate effect sizes. Store the output into x_tt and x_effect, respectively. Create a data.frame named aldex_out which includes both x_tt and x_effect and filter for the features with wi.eBH &lt; 0.05. Are there any significantly differential abundance taxa? Extra: If these results appear boring, repeat steps 1 - 5, but use Gender or Age as the grouping variable. Do we have any better luck with Gender? What is the problem with Age? 19.8.2 Controlling for confounders Import the mia and MicrobiomeStat packages, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Agglomerate the TreeSE by genus and filter by a prevalence of 10%. You can perform both operations in one go with subsetByPrevalentFeatures by specifying the rank and prevalence arguments. Model the counts assay of the TreeSE with linda and store the output into a variable named linda_out. Provide the colData converted into a data.frame (with as.data.frame) as the second argument, and a formula with the Age, Gender and Diet as variables. For example, formula = \"~ A + B\" represents a formula with variables A and B. Extract the output$AgeElderly object from linda_out with $ and store it into a variable named linda_res. Filter linda_res for features with reject == TRUE. How many differentially abundant taxa were found? What are their names and how significant are they in terms of log-fold change and adjusted p-value? 19.8.3 Comparing methods Here, we conduct DAA with identical parameters as in the previous exercise, but with a different method, namely ZicoSeq. We aim to compare the results between these two methods and draw better informed conclusions from such comparative approach. Import the mia and GUniFrac packages, load any of the example data sets mentioned in Chapter 3.3 with data and store it into a variable named tse. Agglomerate the TreeSE by genus and filter by a prevalence of 10%. You can perform both operations in one go with subsetByPrevalentFeatures by specifying the rank and prevalence arguments. Model the counts assay of the TreeSE with ZicoSeq as the feature.dat argument and store the output into a variable named zicoseq_out. Provide also the colData converted to a data.frame (with as.data.frame) as meta.dat. In addition, set grp.name to \"Age\", adj.name to c(\"Diet\", \"Gender\"), feature.dat.type to \"count\", return.feature.dat to TRUE and perm.no to 999. View the top six differentially abundant taxa and their adjusted p-values with head(sort(zicoseq_out$p.adj.fdr)). Is there any significant taxon according to ZicoSeq? Compared to the output of linda, do we see the same taxa at the top in terms of significance? Overall, to what extent do the two methods agree with one another? 19.8.4 Workflow 1 Get the abundances for an individual feature (taxonomic group / row) Visualize the abundances per group with boxplot / jitterplot Is the difference significant (Wilcoxon test)? Is the difference significant (linear model with covariates)? How do transformations affect the outcome (log10, clr..)? Get p-values for all features (taxa), for instance with a for loop Do multiple testing correction Compare the results from different tests with a scatterplot Useful functions: [], ggplot2::geom_boxplot, ggplot2::geom_jitter, wilcox.test, lm.test, transformAssay, p.adjust 19.8.5 Workflow 2 install the latest development version of mia from GitHub. Load experimental dataset from mia. Compare abundances of each taxa between groups. First, use Wilcoxon or Kruskall-Wallis test. Then use some other method dedicated to microbiome data. Summarize findings by plotting a taxa vs samples heatmap. Add column annotation that tells the group of each sample, and row annotation that tells whether the difference of certain taxa was statistically significant. Choose statistically significant taxa and visualize their abundances with boxplot &amp; jitterplot. Useful functions: wilcox.test, kruskal.test, ggplot, pheatmap, ComplexHeatMap::Heatmap, ancombc, aldex2, maaslin2, mergeFeaturesByRank, transformAssay, subsetByPrevalentFeatures 19.9 Visualization 19.9.1 Multivariate ordination Load experimental dataset from mia. Create PCoA with Bray-Curtis dissimilarities Create PCA with Aitchison dissimilarities Visualize and compare both Test other transformations, dissimilarities, and ordination methods Useful functions: runMDS, runNMDS, transformAssay, ggplot, plotReducedDim 19.9.2 Heatmap visualization Load experimental dataset from mia. Visualize abundances with heatmap Visualize abundances with heatmap after CLR + Z transformation See the OMA book for examples. 19.10 Multiomics 19.10.1 Basic exploration Here we learn how to conduct preliminary exploration on a MAE, using HintikkaXOData as an example dataset. Import the mia package, load HintikkaXOData with data and store it into a variable named mae. Which experiments make up the MAE? How many samples and features are contained by each experiment? You can get a summary for all experiments with experiments, and check for each individual experiment with dim, nrow and ncol. What are the names of the features and samples of the different experiments? You can see that with rownames and colnames, respectively. What information is known about the samples? Remember that information about samples is stored in the colData of the MAE. Extra: How do the samples of the individual experiments map to the columns of the MAE? You can find the sample mapping in the sampleMap of the MAE. So far so good. You explored a MAE and its experiments, getting a taste of how information is organized in its internal structure. 19.10.2 Experiment agglomeration Here we learn how to manipulate an experiment contained by a MAE and save the new modified version of the experiment in a suitable place (the altExp slot). Import the mia package, load HintikkaXOData with data and store it into a variable named mae. Agglomerate the microbiota experiment by Genus and store the output into the altExp slot of the microbiota experiment, with the custom name microbiota_genus. How many features remain after agglomerating? What are their names? Extra: create one more alternative experiment named prevalent_microbiota_family, which contains the microbiota experiment agglomerated by Family with a prevalence threshold of 10%. You can agglomerate and in parallel select by prevalence with mergeFeaturesByPrevalence. Good job! You agglomerated one of the experiments in the MAE and stored it as an alternative experiment. 19.10.3 Experiment transformation We proceed with an exercise on a different type of data manipulation, that is, transformation of assays of individual experiments in the MAE. Import the mia package, load HintikkaXOData with data and store it into a variable named mae. What assays are contained by each individual experiment? You can check their names with assays. Apply a log10 transformation to the assay of the metabolite experiment. For that you can use transformAssay and don’t forget to specify the assay to be transformed with the argument assay.type. Apply a CLR transformation to the counts assay of the microbiota experiment. To ensure non-null values in the assay, set pseudocount equal to 1. You made it! You learnt how to apply different transformations to the assays of individual experiments in a MAE with transformAssay, specifying optional arguments based on the used method. 19.10.4 Assay extraction The following exercise walks you through disassembling a MAE object in order to retrieve a specific assay, or to store its components as multiple separate csv files. Import the mia package, load HintikkaXOData with data and store it into a variable named mae. Extract the individual metabolite experiment from the MAE into a distinct TreeSE object named metabolites. Which and how many assays are contained by metabolites? You can check that with assays or assayNames. Write a csv file for the nmr assay with write.csv. You can access an individual assay of a TreeSE with assay by specifying the name of the desired assay. Extra: Repeat step 1 thorugh 4 also for the microbiota and biomarkers experiments, so that a completely disassembled version of the MAE is available. Extra: Besides experiments, MAEs also include a sampleData and a sampleMap, which are accessible with colData(mae) and sampleMap(mae), respectively. Save also each of these two elements into a csv file. Well done! You just splitted a MAE into its components and stored them as csv files. This script shows a possible approach. 19.10.5 MAE reconstruction Next, we will try to reconstruct the same MAE from the files you created. Make sure you know their names and location! Alternatively, you can fetch or download the CSV files in this directory with the readily disassembled components of HintikkaXOData. Read in the csv files containing assays with read.csv and save each of them into a variable named &lt;assay name&gt;_assays. Create one TreeSE from each assays object with the TreeSummarizedExperiment function, as explained in this exercise. Read in the sampleData and the sampleMap and store them into the variables sample_data and sample_map, respectively. Combine the components with MultiAssayExperiment, where the first argument is an ExperimentList (for now include only the microbiota and metabolites TreeSEs), the second is colData and the third is sampleMap. Make sure that the MAE experiments are identical to the original TreeSEs. You can do that qualitatively by checking their head and quantitatively by looking at their dim. Extra: Add the biomarkers TreeSE as a new experiment to the MAE. Note that new experiments can be added to a MAE through simple concatenation with c(mae, experiment). Good job! Now you are aware of how MAEs are built and we can proceed to some analytical exercises. 19.10.6 Cross-correlation analysis Now we will perform a cross-correlation analysis between two of the experiments in the MAE. Import the mia package, load HintikkaXOData with data and store it into a variable named mae. Analyze correlations between the microbiota and the biomarkers experiments with getExperimentCrossAssociation. Don’t forget to specify the experiments you want to compare with the arguments experiment1 and experiment2, and which of their assays with assay.type1 and assay.type2. What does the output look like? By default, correlation is measured in terms of Kendall tau coefficients. Repeat point 2, but this time change method to Spearman coefficients. Are you able to infer significance from the output? In order to also obtain p-values from the cross-correlation analysis, repeat point 2 with the additional argument test_significance = TRUE. Visualize results with a heatmap similarly to the example in section 15.1. Do you see any significant correlations? Interpret your results. Extra: Perform cross-correlation analysis between the remaining experiments (microbiota vs metabolites and metabolites vs biomarkers) and visualize results with heatmaps. Great job! You performed a cross-correlation analysis between two experiments of a MAE and visualized the results with a heatmap. You are also able to customise the correlation method and significance testing used for the analysis. "],["extras.html", "Chapter 20 Extra material 20.1 Slides 20.2 PERMANOVA comparison 20.3 Bayesian Multinomial Logistic-Normal Models 20.4 Interactive 3D Plots", " Chapter 20 Extra material knitr::opts_chunk$set(eval=FALSE) 20.1 Slides Outreach material includes slide sets for training events. 20.2 PERMANOVA comparison Here we present two possible uses of the adonis2 function which performs PERMANOVA. The optional argument by has an effect on the statistical outcome, so its two options are compared here. # import necessary packages library(gtools) library(purrr) library(vegan) library(gtools) library(purrr) Let us load the enterotype TSE object and run PERMANOVA for different orders of three variables with two different approaches: by = \"margin\" or by = \"terms\". # load and prepare data library(mia) data(&quot;enterotype&quot;, package=&quot;mia&quot;) enterotype &lt;- transformAssay(enterotype, method = &quot;relabundance&quot;) # drop samples missing meta data enterotype &lt;- enterotype[ , !rowSums(is.na(colData(enterotype)[, c(&quot;Nationality&quot;, &quot;Gender&quot;, &quot;ClinicalStatus&quot;)]) &gt; 0)] # define variables and list all possible combinations vars &lt;- c(&quot;Nationality&quot;, &quot;Gender&quot;, &quot;ClinicalStatus&quot;) var_perm &lt;- permutations(n = 3, r = 3, vars) formulas &lt;- apply(var_perm, 1, function(row) purrr::reduce(row, function(x, y) paste(x, &quot;+&quot;, y))) # create empty data.frames for further storing p-values terms_df &lt;- data.frame(&quot;Formula&quot; = formulas, &quot;ClinicalStatus&quot; = rep(0, 6), &quot;Gender&quot; = rep(0, 6), &quot;Nationality&quot; = rep(0, 6)) margin_df &lt;- data.frame(&quot;Formula&quot; = formulas, &quot;ClinicalStatus&quot; = rep(0, 6), &quot;Gender&quot; = rep(0, 6), &quot;Nationality&quot; = rep(0, 6)) for (row_idx in 1:nrow(var_perm)) { # generate temporary formula (i.e. &quot;assay ~ ClinicalStatus + Nationality + Gender&quot;) tmp_formula &lt;- purrr::reduce(var_perm[row_idx, ], function(x, y) paste(x, &quot;+&quot;, y)) tmp_formula &lt;- as.formula(paste0(&#39;t(assay(enterotype, &quot;relabundance&quot;)) ~ &#39;, tmp_formula)) # multiple variables, default: by = &quot;terms&quot; set.seed(75) with_terms &lt;- adonis2(tmp_formula, by = &quot;terms&quot;, data = colData(enterotype), permutations = 99) # multiple variables, by = &quot;margin&quot; set.seed(75) with_margin &lt;- adonis2(tmp_formula, by = &quot;margin&quot;, data = colData(enterotype), permutations = 99) # extract p-values terms_p &lt;- with_terms[[&quot;Pr(&gt;F)&quot;]] terms_p &lt;- terms_p[!is.na(terms_p)] margin_p &lt;- with_margin[[&quot;Pr(&gt;F)&quot;]] margin_p &lt;- margin_p[!is.na(margin_p)] # store p-values into data.frames for (col_idx in 1:ncol(var_perm)) { terms_df[var_perm[row_idx, col_idx]][row_idx, ] &lt;- terms_p[col_idx] margin_df[var_perm[row_idx, col_idx]][row_idx, ] &lt;- margin_p[col_idx] } } The following table displays the p-values for the three variables ClinicalStatus, Gender and Nationality obtained by PERMANOVA with adonis2. Note that the p-values remain identical when by = \"margin\", but change with the order of the variables in the formula when by = \"terms\" (default). df &lt;- terms_df %&gt;% dplyr::inner_join(margin_df, by = &quot;Formula&quot;, suffix = c(&quot; (terms)&quot;, &quot; (margin)&quot;)) knitr::kable(df) 20.3 Bayesian Multinomial Logistic-Normal Models Analysis using such model could be performed with the function pibble from the fido package, wihch is in form of a Multinomial Logistic-Normal Linear Regression model; see vignette of package. The following presents such an exemplary analysis based on the data of Sprockett et al. (2020) available through microbiomeDataSets package. library(fido) Loading the libraries and importing data: library(fido) library(microbiomeDataSets) tse &lt;- SprockettTHData() We pick three covariates (“Sex”,“Age_Years”,“Delivery_Mode”) during this analysis as an example, and beforehand we check for missing data: library(mia) cov_names &lt;- c(&quot;Sex&quot;,&quot;Age_Years&quot;,&quot;Delivery_Mode&quot;) na_counts &lt;- apply(is.na(colData(tse)[,cov_names]), 2, sum) na_summary&lt;-as.data.frame(na_counts,row.names=cov_names) We drop missing values of the covariates: tse &lt;- tse[ , !is.na(colData(tse)$Delivery_Mode) ] tse &lt;- tse[ , !is.na(colData(tse)$Age_Years) ] We agglomerate microbiome data to Phylum: tse_phylum &lt;- mergeFeaturesByRank(tse, &quot;Phylum&quot;) We extract the counts assay and covariate data to build the model matrix: Y &lt;- assays(tse_phylum)$counts # design matrix # taking 3 covariates sample_data&lt;-as.data.frame(colData(tse_phylum)[,cov_names]) X &lt;- t(model.matrix(~Sex+Age_Years+Delivery_Mode,data=sample_data)) Building the parameters for the pibble call to build the model; see more at vignette: n_taxa&lt;-nrow(Y) upsilon &lt;- n_taxa+3 Omega &lt;- diag(n_taxa) G &lt;- cbind(diag(n_taxa-1), -1) Xi &lt;- (upsilon-n_taxa)*G%*%Omega%*%t(G) Theta &lt;- matrix(0, n_taxa-1, nrow(X)) Gamma &lt;- diag(nrow(X)) Automatically initializing the priors and visualizing their distributions: priors &lt;- pibble(NULL, X, upsilon, Theta, Gamma, Xi) names_covariates(priors) &lt;- rownames(X) plot(priors, pars=&quot;Lambda&quot;) + ggplot2::xlim(c(-5, 5)) Estimating the posterior by including our response data Y. Note: Some computational failures could occur (see discussion) the arguments multDirichletBoot calcGradHess could be passed in such case. priors$Y &lt;- Y posterior &lt;- refit(priors, optim_method=&quot;adam&quot;, multDirichletBoot=0.5) #calcGradHess=FALSE Printing a summary about the posterior: ppc_summary(posterior) Plotting the summary of the posterior distributions of the regression parameters: names_categories(posterior) &lt;- rownames(Y) plot(posterior,par=&quot;Lambda&quot;,focus.cov=rownames(X)[2:4]) Taking a closer look at “Sex” and “Delivery_Mode”: plot(posterior, par=&quot;Lambda&quot;, focus.cov = rownames(X)[c(2,4)]) 20.4 Interactive 3D Plots # Load libraries library(rgl) library(plotly) library(knitr) knitr::knit_hooks$set(webgl = hook_webgl) In this section we make a 3D version of the earlier Visualizing the most dominant genus on PCoA (see 5), with the help of the plotly (Sievert 2020). # Importing necessary libraries library(curatedMetagenomicData) library(dplyr) library(DT) library(mia) library(scater) # Querying the data tse &lt;- sampleMetadata %&gt;% filter(age &gt;= 18) %&gt;% # taking only data of age 18 or above filter(!is.na(alcohol)) %&gt;% # excluding missing values returnSamples(&quot;relative_abundance&quot;) tse_Genus &lt;- mergeFeaturesByRank(tse, rank=&quot;genus&quot;) tse_Genus &lt;- addPerSampleDominantFeatures(tse_Genus,assay.type=&quot;relative_abundance&quot;, name = &quot;dominant_taxa&quot;) # Performing PCoA with Bray-Curtis dissimilarity. tse_Genus &lt;- runMDS(tse_Genus, FUN = vegan::vegdist, ncomponents = 3, name = &quot;PCoA_BC&quot;, assay.type = &quot;relative_abundance&quot;) # Getting the 6 top taxa top_taxa &lt;- getTopFeatures(tse_Genus,top = 6, assay.type = &quot;relative_abundance&quot;) # Naming all the rest of non top-taxa as &quot;Other&quot; most_abundant &lt;- lapply(colData(tse_Genus)$dominant_taxa, function(x){if (x %in% top_taxa) {x} else {&quot;Other&quot;}}) # Storing the previous results as a new column within colData colData(tse_Genus)$most_abundant &lt;- as.character(most_abundant) # Calculating percentage of the most abundant most_abundant_freq &lt;- table(as.character(most_abundant)) most_abundant_percent &lt;- round(most_abundant_freq/sum(most_abundant_freq)*100, 1) # Retrieving the explained variance e &lt;- attr(reducedDim(tse_Genus, &quot;PCoA_BC&quot;), &quot;eig&quot;); var_explained &lt;- e/sum(e[e&gt;0])*100 "],["developers.html", "Developers", " Developers .rebook-collapse { background-color: #eee; color: #444; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .rebook-content { padding: 0 18px; display: none; overflow: hidden; background-color: #f1f1f1; } Core team Contributions to this Gitbook from the various developers are coordinated by: Leo Lahti, DSc, professor in Data Science at the Department of Computing, University of Turku, Finland, with a focus on computational microbiome analysis. Lahti obtained doctoral degree (DSc) from Aalto University in Finland (2010), developing probabilistic machine learning with applications to high-throughput life science data integration. Since then he has focused on microbiome research and developed, for instance, the phyloseq-based microbiome R package before starting to develop the TreeSummarizedExperiment / MultiAssayExperiment framework and the mia family of Bioconductor packages for microbiome data science introduced in this gitbook. Lahti led the development of national policy on open access to research methods in Finland. He is current member in the Bioconductor Community Advisory Board and runs regular training workshops in microbiome data science. Tuomas Borman, PhD researcher and the lead developer of OMA/mia at the Department of Computing, University of Turku. Contributors This work is a remarkably collaborative effort. The full list of contributors is available via Github. Some key authors/contributors include: Felix Ernst, PhD, among the first developers of R/Bioc methods for microbiome research based on the SummarizedExperiment class and its derivatives. Giulio Benedetti, scientific programmer at the Department of Computing, University of Turku. His research interest is mostly related to Data Science. He has also helped to expand the SummarizedExperiment-based microbiome analysis framework to the Julia language, implementing MicrobiomeAnalysis.jl. Sudarshan Shetty, PhD has supported the establishment of the framework and associated tools. He also maintains a list of microbiome R packages. Henrik Eckermann, in particular to the development of the differential abundance analyses Chouaib Benchraka provided various contributions to the package ecosystem and the OMA book Yağmur Şimşek converted the miaSim R package to support the Bioconductor framework Basil Courbayre provided various contributions to the package ecosystem and the OMA book, in particular on unsupervised machine learning Matti Ruuskanen, PhD, added machine learning techniques for microbiome analysis Stefanie Peschel has contributed chapters on the construction, analysis, and comparison of microbial association networks. Christian L. Müller, group leader at the Computational Health Center, Helmholtz Zentrum München, Germany and a Professor for Biomedical Statistics and Data Science at LMU Munich. He assisted in writing the chapters on network learning and comparison. Shigdel Rajesh, PhD Artur Sannikov Jeba Akewak Himmi Lindgren Lu Yang Acknowledgments This work would not have been possible without the countless contributions and interactions with other researchers, developers, and users. We express our gratitude to the entire Bioconductor community for developing this high-quality open research software repository for life science analytics, continuously pushing the limits in emerging fields (R. C. Gentleman et al. 2004), (Huber et al. 2015). The presented framework for microbiome data science is based on the TreeSummarizedExperiment data container created by Ruizhu Huang and others (Huang 2020), (F. G. M. Ernst et al. 2020), and on the MultiAssayExperiment by Marcel Ramos et al. (Ramos et al. 2017). The idea of using these containers as a basis for microbiome data science was initially advanced by the groundwork of Domenick Braccia, Héctor Corrada Bravo and others and brought together with other microbiome data science developers (Shetty and Lahti 2019). Setting up the base ecosystem of packages and tutorials was then subsequently led by Tuomas Borman, Felix Ernst, and Leo Lahti. We would specifically like to thank everyone who contributed to the work supporting the TreeSummarizedExperiment ecosystem for microbiome research, including but not limited to the R packages mia, miaViz, miaTime, miaSim, philr, ANCOMBC, curatedMetagenomicData, scater, scuttle, and other packages, some of which are listed in Section 2.2. A number of other contributors have advanced the ecosystem further, and will be acknowledged in the individual packages, pull requests, issues, and other work. Ample demonstration data resources supporting this framework have been made available through the curatedMetagenomicData project by Edoardo Pasolli, Lucas Schiffer, Levi Waldron and others (Pasolli et al. 2017). The work has drawn initial inspiration from many sources, most notably from the work on phyloseq by Paul McMurdie and Susan Holmes (P. McMurdie and Holmes 2013) who pioneered the work on rigorous and reproducible microbiome data science ecosystems in R/Bioconductor. The phyloseq framework continues to provide a vast array of complementary packages and methods for microbiome studies. The Orchestrating Single-Cell Analysis with Bioconductor, or OSCA book by Robert Amezquita, Aaron Lun, Stephanie Hicks, and Raphael Gottardo (R. A. Amezquita et al. 2020) has implemented closely related work on the SummarizedExperiment data container and its derivatives in the field of single cell sequencing studies that have inspired this work. In the background, the open source books by Susan Holmes and Wolfgang Huber, Modern Statistics for Modern Biology (Holmes and Huber 2019) and by Garret Grolemund and Hadley Wickham, the R for Data Science (Grolemund and Wickham 2017), and Richard McElreath’s Statistical Rethinking and the associated online resources by Solomon Kurz (McElreath 2020) are key references that have advanced reproducible data science training and dissemination. 20.4.1 How to contribute To contribute reports, follow the Git flow procedure (you can see instructions to getting started with Github): Fork the project Clone your fork Modify the material Check locally that the changes render successfully (see above) Add and commit the changes to your fork Create a pull request (PR) from your fork back to the original repo Fix and discuss issues in the review process More detailed instructions for contributing can be found on OMA README. Support This work has been supported by: Research Council of Finland FindingPheno European Union’s Horizon 2020 research and innovation programme under grant agreement No 952914 COST Action network on Statistical and Machine Learning Techniques for Human Microbiome Studies (ML4microbiome) (Moreno-Indias et al. 2021). Computational Life Science Research Program, Biocity Turku Turku University Foundation "],["sessioninfo.html", "Sessioninfo", " Sessioninfo sessionInfo() ## R version 4.3.1 (2023-06-16) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 22.04.3 LTS ## ## Matrix products: default ## ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## time zone: UTC ## tzcode source: system (glibc) ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] BiocStyle_2.28.1 rebook_1.10.1 ## ## loaded via a namespace (and not attached): ## [1] knitr_1.45 shiny_1.8.0 htmltools_0.5.7 rmarkdown_2.25 ## [5] bookdown_0.36 miniUI_0.1.1.1 tools_4.3.1 "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
