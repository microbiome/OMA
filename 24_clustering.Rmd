# Community typing (clustering) {#clustering}

```{r setup, echo=FALSE, results="asis"}
library(rebook)
chapterPreamble()
```

Clustering is an unsupervised machine learning technique. The idea of 
it is to find clusters from the data. A cluster is a group of 
features/samples that share pattern. For example, with clustering, we 
can find group of samples that share similar community composition.
There are multiple clustering algorithms available. Most can be 
used through the `bluster` package and its `clusterRows` function.

Let's load data from mia package.
```{r load-pkg-data1}
library(mia)
data("GlobalPatterns", package="mia")
tse <- GlobalPatterns
```
Prior to doing any clustering, we can apply transformation to the data. 
Here, we will transform the data using `relabundance`.
```{r load-bluster}
# Load dependencies
if( !require(bluster) ){install.packages("bluster"); library(bluster)}

# Optional: Apply transformation
tse <- transformCounts(tse, method = "relabundance")
```

Also, we can do clustering on samples or on features. 
```{r assay-preparation}
# To cluster the features, simply get the assay
assay <- assay(tse, "relabundance")

# To cluster the samples, transpose the assay so that the samples are now in rows
assay <- t(assay)
```

Once the clustering on the samples is done, we can also plot the clusters.
```{r plot-samples, eval = FALSE}
# Beforehand, the clusters must be computed and put in colData in column "clusters"

# Add the MDS dimensions for plotting
tse <- runMDS(tse, assay.type = "relabundance", FUN = vegan::vegdist, method = "bray")

# Plot the clusters
plotReducedDim(tse, "MDS", colour_by = "clusters")
```
Moving forward, we will cluster on the samples.
## Hierarchical clustering

Hierarchical clustering aims to find hierarchy between samples/features.
There are to approaches: agglomerative (“bottom-up”) and divisive (“top-down”).

In agglomerative approach, each observation is first unique cluster. 
Algorithm continues by agglomerating similar clusters. Divisive 
approach starts with one cluster that contains all the observations. 
Clusters are split recursively to clusters that differ the most. 
Clustering ends when each cluster contains only one observation.

Hierarchical clustering can be visualized with dendrogram tree. 
In each splitting point, the three is divided into two clusters leading 
to hierarchy.

To do the hierarchical clustering, we use the `clusterRows` with 
the `HclustParam()` parameter.

```{r hclust1}
# Simple use of the hierarchical clustering which sets the cut
# height to half the dendrogram height.
hclust.out <- clusterRows(assay, HclustParam())

# Add data to the TSE
colData(tse)$clusters <- hclust.out$clusters

colData(tse)$clusters
```
More parameters can be passed to the algorithm. To see them, 
check the [HclustParam documentation](https://rdrr.io/github/LTLA/bluster/man/HclustParam-class.html).

Hierarchical clustering requires 2 steps. In the fist step, dissimilarities are 
calculated. In prior to that, data transformation is applied if needed. Since
sequencing data is compositional, relative transformation is applied.
In the second step, clustering is performed based on dissimilarities. 

```{r hclust2}
# More complex use of the algorithm with another method and cut height
HclustParam(method="ward.D2", cut.dynamic=TRUE)
```
To plot the dendrogram, we need the full information from the hierarchical clustering.

We can use dendrogram to determine the number of clusters. Usually the
tree is splitted where the branch length is the largest. However, as
we can see from the dendrogram, clusters are not clear. Algorithms are
available to identify the optimal number of clusters.

```{r hclust3}
# Get full hierarchical clustering information
hclust.out <- clusterRows(assay, HclustParam(), full=TRUE)

# Add data to the TSE
colData(tse)$clusters <- hclust.out$clusters

# Get the dendrogram object
dendro <- as.dendrogram(hclust.out$objects$hclust)

# Plot the dendrogram
plot(dendro)

# Plot dendrogram with colors
nbclusters <- length(levels(hclust.out$clusters))
col_val_map <- randomcoloR::distinctColorPalette(nbclusters) %>%
    as.list() %>% setNames(paste0("clust_",seq(nbclusters)))
dend <- color_branches(dendro, k=nbclusters, col=unlist(col_val_map))
labels(dend) <- NULL
plot(dend)
```
## K-means clustering
Let's now try k-means clustering. Here observations are divided into clusters 
so that the distances between observations and cluster centers are minimized; 
an observation belongs to cluster whose center is the nearest.

The algorithm starts by dividing observation to random clusters whose number 
is defined by user. The centroids of clusters are then calculated. After that, 
observations' allocation to clusters are updated so that the means are minimized. 
Again, centroid are calculated, and algorithm continues iteratively until the 
assignments do not change.
### Getting the number of clusters

The number of clusters can be determined based on algorithm. Here we utilize 
silhouette analysis.

```{r kmeans1}
# Load dependencies
# Load dependencies
if( !require(factoextra) ) { 
	install.packages("factoextra"); 
	library(factoextra)
}

# Calculate distances
diss <- vegdist(assay, method = "bray")

# Convert dist object into matrix
diss <- as.matrix(diss)

# Perform silhouette analysis and plot the result
fviz_nbclust(diss, kmeans, method = "silhouette").
```

We can also get the number of clusters by using a function. For example sqrt.

### K-means algorithm

Based on the result of silhouette analysis, we choose **3** to be the number of clusters in k-means clustering.


```{r kmeans2}
# There's a random aspect to K-means (to get the initial centers) so if we want reproducible results, we set a random seed.
set.seed(100)

# Do the K-means clustering
kmeans.out <- clusterRows(assay, KmeansParam(10))

# Add data to the TSE
colData(tse)$clusters <- hclust.out$clusters
hclust.out$clusters
```

As seen in the hierarchical clustering, if the clusters are done on the samples, we can plot them with `plotReducedDim`.

As mentioned before, we can also get the number of clusters using a function:

```{r kmeans2}
kmeans.out <- clusterRows(assay, KmeansParam(sqrt))
```

### K-means variants

#### Mini-batch k-means

This variant uses mini-batching to approximate the full k-means algorithm for greater speed.

```{r mbkmeans}
if( !require("mbkmeans") ){
    BiocManager::install("mbkmeans")
    library("mbkmeans")
}
set.seed(100)
mbkmeans.out <- clusterRows(assay, MbkmeansParam(10))
colData(tse)$clusters <- mbkmeans.out
mbkmeans.out
```

## Affinity propagation

Contrarily to the K-means algorithm, this algorithm doesn't require 
a number of clusters before running. It instead uses a communication 
system between the data points to find which members of the input set 
that are representative of clusters.

This algorithm isn't particularly fast as it involves the calculation 
of a square similarity matrix between all pairs of observations. 
So, we'll speed it up by taking analyzing a subset of the data:

```{r affinity-prop}
set.seed(1000)

# Take a sub dataset
sub <- tse[sample(nrow(tse), 2000),]

# Clustering by sample --> transpose the subset
ap.out <- clusterRows(t(sub), AffinityParam())
colData(tse)$clusters <- ap.out
ap.out
# Plotting the results
plotReducedDim(tse, "MDS", colour_by = "clusters")
```

We can change parameters such as q which determines the resolution 
of the clustering. Find out more on the 
[AffinityParam doc](https://rdrr.io/github/LTLA/bluster/man/AffinityParam-class.html).

## Self-organizing maps (SOM)

Another possibility is to use self-organizing maps from the kohonen 
package. This allocates observations to nodes of a simple neural 
network; each node is then treated as a cluster.

```{r som1}
if( !require("kohonen") ){
    BiocManager::install("kohonen")
    library("kohonen")
}
set.seed(1000)

# Clustering on the samples
som.out <- clusterRows(t(relassay), SomParam(20), full=TRUE)
head(som.out$clusters)
colData(tse)$clusters <- som.out$clusters

# Plotting clusters
plotReducedDim(tse, "MDS", colour_by = "clusters")

```

The key feature of SOMs is that they apply additional topological 
constraints on the relative positions of the nodes. This allows 
us to naturally determine the relationships between clusters 
based on the proximity of the nodes.

```{r som2}
# Plotting relationship between clusters
par(mfrow=c(1,2))
plot(som.out$objects$som, "counts")
grid <- som.out$objects$som$grid$pts
text(grid[,1], grid[,2], seq_len(nrow(grid)))
```

## Graph-based clustering
This algorithm builds a graph were the vertices are the data points 
and there is an edge between two points *p* and *q* if the distance 
between these points is among the *k*-th smallest distances from *p* 
to other objects from *p*. Thus, here, the number of neighbors `k` 
controls the resolution of the clusters.

We can build shared or direct nearest neighbor graphs and perform 
community detection with igraph.

```{r GNN}
set.seed(101) # just in case there are ties.

# Clustering on the samples
graph.out <- clusterRows(t(assay), NNGraphParam(k=3), full=TRUE)
colData(tse)$clusters <- graph.out$clusters
colData(tse)$clusters
plotReducedDim(tse, "MDS", colour_by = "clusters")
```
It is possible to change parameters such as the algorithm. 
Further documentation is available [here](https://rdrr.io/bioc/bluster/man/NNGraphParam-class.html).

## Density-based clustering

The bluster package also offers a density-based clustering algorithm 
with DBSCAN. This algorithm has the particularity to consider certain 
points as noise (if they aren’t close enough to other points). Here 
we determine the neighborhoods using `eps` and `min.pts`.

```{r dbscan}
# DBSCAN clustering on the samples
dbscan.out <- clusterRows(t(relassay), DbscanParam(eps=0.2, min.pts = 3))
colData(tse)$clusters <- dbscan.out
dbscan.out

# Plot the clusters
plotReducedDim(tse, "MDS", colour_by = "clusters")

# Check the number of noise points (with no value)
summary(is.na(dbscan.out))
```

It’s also possible to determine the neighborhoods using core.prop 
and min.pts. Find out more on the DbscanParam with the 
[documentation](https://rdrr.io/github/LTLA/bluster/man/DbscanParam-class.html).

## Dirichlet Multinomial Mixtures (DMM)

This section focus on DMM analysis. 

One technique that allows to search for groups of samples that are
similar to each other is the [Dirichlet-Multinomial Mixture
Model](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030126). In
DMM, we first determine the number of clusters (k) that best fit the
data (model evidence) using Laplace approximation. After fitting the
model with k clusters, we obtain for each sample k probabilities that
reflect the probability that a sample belongs to the given cluster.

Let's cluster the data with DMM clustering. 

```{r dmm1}
# Runs model and calculates the most likely number of clusters from 1 to 7.
# Since this is a large dataset it takes long computational time.
# For this reason we use only a subset of the data; agglomerated by Phylum as a rank.
tse <- GlobalPatterns
tse <- agglomerateByRank(tse, rank = "Phylum", agglomerateTree=TRUE)
```

```{r dmm2}
tse_dmn <- mia::runDMN(tse, name = "DMN", k = 1:7)
```

```{r}
# It is stored in metadata
tse_dmn
```

Return information on metadata that the object contains.

```{r}
names(metadata(tse_dmn))
```

This returns a list of DMN objects for a closer investigation.

```{r}
getDMN(tse_dmn)
```


Show Laplace approximation (model evidence) for each model of the k models.

```{r}
library(miaViz)
plotDMNFit(tse_dmn, type = "laplace")
```

Return the model that has the best fit.

```{r}
getBestDMNFit(tse_dmn, type = "laplace")
```

### PCoA for ASV-level data with Bray-Curtis; with DMM clusters shown with colors

Group samples and return DMNGroup object that contains a summary.
Patient status is used for grouping.

```{r}
dmn_group <- calculateDMNgroup(tse_dmn, variable = "SampleType",  assay.type = "counts",
                               k = 2, seed=.Machine$integer.max)

dmn_group
```

Mixture weights  (rough measure of the cluster size).


```{r}
DirichletMultinomial::mixturewt(getBestDMNFit(tse_dmn))
```


Samples-cluster assignment probabilities / how probable it is that sample belongs
to each cluster

```{r}
head(DirichletMultinomial::mixture(getBestDMNFit(tse_dmn)))
```

Contribution of each taxa to each component

```{r}
head(DirichletMultinomial::fitted(getBestDMNFit(tse_dmn)))
```
Get the assignment probabilities


```{r}
prob <- DirichletMultinomial::mixture(getBestDMNFit(tse_dmn))
# Add column names
colnames(prob) <- c("comp1", "comp2")

# For each row, finds column that has the highest value. Then extract the column 
# names of highest values.
vec <- colnames(prob)[max.col(prob,ties.method = "first")]

```

Computing the euclidean PCoA and storing it as a data frame

```{r}
# Does clr transformation. Pseudocount is added, because data contains zeros.
assay(tse, "pseudo") <- assay(tse, "counts") + 1
tse <- transformCounts(tse, assay.type = "pseudo", method = "relabundance")
tse <- transformCounts(tse, "relabundance", method = "clr")

library(scater)

# Does principal coordinate analysis
df <- calculateMDS(tse, assay.type = "clr", method = "euclidean")

# Creates a data frame from principal coordinates
euclidean_pcoa_df <- data.frame(pcoa1 = df[,1], 
                                pcoa2 = df[,2])

```

```{r}
# Creates a data frame that contains principal coordinates and DMM information
euclidean_dmm_pcoa_df <- cbind(euclidean_pcoa_df,
                               dmm_component = vec)
# Creates a plot
euclidean_dmm_plot <- ggplot(data = euclidean_dmm_pcoa_df, 
                             aes(x=pcoa1, y=pcoa2,
                                 color = dmm_component)) +
  geom_point() +
  labs(x = "Coordinate 1",
       y = "Coordinate 2",
       title = "PCoA with Aitchison distances") +  
  theme(title = element_text(size = 12)) # makes titles smaller

euclidean_dmm_plot
```

## Community Detection

Another approach for discovering communities within the samples of the
data, is to run community detection algorithms after building a
graph. The following demonstration builds a graph based on the k
nearest-neighbors and performs the community detection on the fly.

_`bluster`_ [@R-bluster] package offers several clustering methods,
among which graph-based are present, enabling the community detection
task.

Installing package:

```{r}
if(!require(bluster)){
  BiocManager::install("bluster")
}
```

The algorithm used is "short random walks" [@Pons2006]. Graph is
constructed using different k values (the number of nearest neighbors
to consider during graph construction) using the robust centered log
ratio (rclr) assay data. Then plotting the communities using UMAP
[@McInnes2018] ordination as a visual exploration aid.  In the
following demonstration we use the `enterotype` dataset from the
[@R-mia] package.

```{r, message=FALSE, warning=FALSE}
library(bluster)
library(patchwork) # For arranging several plots as a grid
library(scater)

data("enterotype", package="mia")
tse <- enterotype
tse <- transformCounts(tse, method = "rclr")

# Performing and storing UMAP
tse <- runUMAP(tse, name="UMAP", assay.type="rclr")

k <- c(2,3,5,10)
ClustAndPlot <- function(x) {
  # Creating the graph and running the short random walks algorithm  
  graph_clusters <- clusterRows(t(assays(tse)$rclr), NNGraphParam(k=x))
  
  # Results of the clustering as a color for each sample
  plotUMAP(tse, colour_by = I(graph_clusters)) +
    labs(title = paste0("k = ", x))
}

# Applying the function for different k values
plots <- lapply(k,ClustAndPlot)

# Displaying plots in a grid
(plots[[1]] + plots[[2]]) / (plots[[3]] + plots[[4]])
```

Similarly, the _`bluster`_ [@R-bluster] package offers clustering
diagnostics that can be used for judging the clustering quality (see
[Assorted clustering
diagnostics](http://bioconductor.org/packages/release/bioc/vignettes/bluster/inst/doc/diagnostics.html)).
In the following, Silhouette width as a diagnostic tool is computed
and results are visualized for each case presented earlier. For more
about Silhouettes read [@Rousseeuw1987].

```{r, message=FALSE, warning=FALSE}

ClustDiagPlot <- function(x) {
  # Getting the clustering results
  graph_clusters <- clusterRows(t(assays(tse)$rclr), NNGraphParam(k=x))
  
  # Computing the diagnostic info
  sil <- approxSilhouette(t(assays(tse)$rclr), graph_clusters)
  
  # Plotting as a boxlpot to observe cluster separation
  boxplot(split(sil$width, graph_clusters), main=paste0("k = ", x))
  
}
# Applying the function for different k values
res <- lapply(k,ClustDiagPlot)
```

## Biclustering

Biclustering methods cluster rows and columns simultaneously in order
to find subsets of correlated features/samples.

Here, we use following packages:

-   [biclust](https://cran.r-project.org/web/packages/biclust/index.html)
-   [cobiclust](https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13582)

_cobiclust_ is especially developed for microbiome data whereas _biclust_ is more
general method. In this section, we show three different cases and example 
solutions to apply biclustering to them. 

1.   Taxa vs samples
2.   Taxa vs biomolecule/biomarker
3.   Taxa vs taxa

Biclusters can be visualized using heatmap or boxplot, for
instance. For checking purposes, also scatter plot might be valid
choice.

Check more ideas for heatmaps from chapters \@ref(viz-chapter) and
\@ref(microbiome-community.

### Taxa vs samples

When you have microbial abundance matrices, we suggest to use
_cobiclust_ which is designed for microbial data.

Load example data
```{r load-pkg-data2}
library(mia)
data("HintikkaXOData")
mae <- HintikkaXOData
```

Only the most prevalent taxa are included in analysis. 

```{r cobiclust_1}
# Subset data in the first experiment
mae[[1]] <- subsetByPrevalentTaxa(mae[[1]], rank = "Genus", prevalence = 0.2, detection = 0.001)
# clr-transform in the first experiment
mae[[1]] <- transformCounts(mae[[1]], method = "relabundance")
mae[[1]] <- transformCounts(mae[[1]], "relabundance", method = "rclr")
```

_cobiclust_ takes counts table as an input and gives _cobiclust_ object as an output.
It includes clusters for taxa and samples. 

```{r cobiclust_2}
# Do clustering; use counts table´
clusters <- cobiclust(assay(mae[[1]], "counts"))

# Get clusters
row_clusters <- clusters$classification$rowclass
col_clusters <- clusters$classification$colclass

# Add clusters to rowdata and coldata
rowData(mae[[1]])$clusters <- factor(row_clusters)
colData(mae[[1]])$clusters <- factor(col_clusters)

# Order data based on clusters
mae[[1]] <- mae[[1]][order(rowData(mae[[1]])$clusters), order(colData(mae[[1]])$clusters)]

# Print clusters
clusters$classification
```

Next we can plot clusters. Annotated heatmap is a common choice.

```{r cobiclust_3, fig.width=14, fig.height=12}

if(!require(pheatmap)){
    install.packages("pheatmap")
    library(pheatmap)
}
# z-transform for heatmap
mae[[1]] <- transformCounts(mae[[1]], assay.type = "rclr",
                            MARGIN = "features",
                            method = "z", name = "clr_z")

# Create annotations. When column names are equal, they should share levels. 
# Here samples include 3 clusters, and taxa 2. That is why we have to make 
# column names unique. 
annotation_col <- data.frame(colData(mae[[1]])[, "clusters", drop = F])
colnames(annotation_col) <- "col_clusters"

annotation_row <- data.frame(rowData(mae[[1]])[, "clusters", drop = F])
colnames(annotation_row) <- "row_clusters"
```


Plot the heatmap.

```{r cobiclust_3b, fig.width=14, fig.height=12}
pheatmap(assay(mae[[1]], "clr_z"), cluster_rows = F, cluster_cols = F, 
         annotation_col = annotation_col,
         annotation_row = annotation_row)
```


Boxplot is commonly used to summarize the results:

```{r cobiclust_4}
if(!require(ggplot2)){
    install.packages("ggplot2")
    library(ggplot2)
}
if(!require(patchwork)){
    install.packages("patchwork")
    library(patchwork)
}

# ggplot requires data in melted format
melt_assay <- meltAssay(mae[[1]], assay.type = "rclr", add_col_data = T, add_row_data = T)

# patchwork two plots side-by-side
p1 <- ggplot(melt_assay) +
  geom_boxplot(aes(x = clusters.x, y = rclr)) +
  labs(x = "Taxa clusters")

p2 <- ggplot(melt_assay) +
  geom_boxplot(aes(x = clusters.y, y = rclr)) +
  labs(x = "Sample clusters")

p1 + p2
```

### Taxa vs biomolecules

Here, we analyze cross-correlation between taxa and metabolites. This
is a case, where we use _biclust_ method which is suitable for numeric
matrices in general.

```{r biclust_1}
# Samples must be in equal order 
# (Only 1st experiment  was ordered in cobiclust step leading to unequal order)
mae[[1]] <- mae[[1]][ , colnames(mae[[2]]) ]

# Make rownames unique since it is require by other steps
rownames(mae[[1]]) <- make.unique(rownames(mae[[1]]))
# Calculate correlations
corr <- getExperimentCrossCorrelation(mae, 1, 2, 
                                      assay.type1 = "rclr", 
                                      assay.type2 = "nmr", 
                                      mode = "matrix", 
                                      cor_threshold = 0.2)

```


_biclust_ takes matrix as an input and returns _biclust_ object. 

```{r biclust_2}
# Set seed for reproducibility
set.seed(3973)

# Find biclusters
library(biclust)
bc <- biclust(corr, method=BCPlaid(), fit.model = y ~ m,
              background = TRUE, shuffle = 100, back.fit = 0, max.layers = 10,
              iter.startup = 10, iter.layer = 100, verbose = FALSE)

bc
```

The object includes cluster information. However compared to
_cobiclust_, _biclust_ object includes only information about clusters
that were found, not general cluster.

Meaning that if one cluster size of 5 features was found out of 20 features, 
those 15 features do not belong to any cluster. That is why we have to create an
additional cluster for features/samples that are not assigned into any cluster.

```{r biclust_3}
# Functions for obtaining biclust information

# Get clusters for rows and columns
.get_biclusters_from_biclust <- function(bc, assay){
  # Get cluster information for columns and rows
  bc_columns <- t(bc@NumberxCol)
  bc_columns <- data.frame(bc_columns)
  bc_rows <- bc@RowxNumber
  bc_rows <- data.frame(bc_rows)
  
  # Get data into right format
  bc_columns <- .manipulate_bc_data(bc_columns, assay, "col")
  bc_rows <- .manipulate_bc_data(bc_rows, assay, "row")
  
  return(list(bc_columns = bc_columns, bc_rows = bc_rows))
}

# Input clusters, and how many observations there should be, i.e.,
# the number of samples or features
.manipulate_bc_data <- function(bc_clusters, assay, row_col){
  # Get right dimension
  dim <- ifelse(row_col == "col", ncol(assay), nrow(assay))
  # Get column/row names
  if( row_col == "col" ){
    names <- colnames(assay)
  } else{
    names <- rownames(assay)
  }
  
  # If no clusters were found, create one. Otherwise create additional
  # cluster which
  # contain those samples that are not included in clusters that were found.
  if( nrow(bc_clusters) != dim ){
      bc_clusters <- data.frame(cluster = rep(TRUE, dim))
  } else {
      # Create additional cluster that includes those samples/features that
      # are not included in other clusters.
      vec <- ifelse(rowSums(bc_clusters) > 0, FALSE, TRUE)
      # If additional cluster contains samples, then add it
      if ( any(vec) ){
          bc_clusters <- cbind(bc_clusters, vec)
      }
  }
  # Adjust row and column names
  rownames(bc_clusters) <- names
  colnames(bc_clusters) <- paste0("cluster_", 1:ncol(bc_clusters))
  return(bc_clusters)
}
```


```{r biclust_4}
# Get biclusters
bcs <- .get_biclusters_from_biclust(bc, corr)

bicluster_rows <- bcs$bc_rows
bicluster_columns <- bcs$bc_columns

# Print biclusters for rows
head(bicluster_rows)
```

Let's collect information for the scatter plot. 

```{r biclust_5}
# Function for obtaining sample-wise sum, mean, median, and mean variance
# for each cluster

.sum_mean_median_var <- function(tse1, tse2, assay.type1, assay.type2, clusters1, clusters2){
  
  list <- list()
  # Create a data frame that includes all the information
  for(i in 1:ncol(clusters1) ){
    # Subset data based on cluster
    tse_subset1 <- tse1[clusters1[,i], ]
    tse_subset2 <- tse2[clusters2[,i], ]
    # Get assay
    assay1 <- assay(tse_subset1, assay.type1)
    assay2 <- assay(tse_subset2, assay.type2)
    # Calculate sum, mean, median, and mean variance
    sum1 <- colSums2(assay1, na.rm = T)
    mean1 <- colMeans2(assay1, na.rm = T)
    median1 <- colMedians(assay1, na.rm = T)
    var1 <- colVars(assay1, na.rm = T)
    
    sum2 <- colSums2(assay2, na.rm = T)
    mean2 <- colMeans2(assay2, na.rm = T)
    median2 <- colMedians(assay2, na.rm = T)
    var2 <- colVars(assay2, na.rm = T)
    
    list[[i]] <- data.frame(sample = colnames(tse1), sum1, sum2, mean1, mean2, 
                     median1, median2, var1, var2)
  }

  return(list)
}

# Calculate info
df <- .sum_mean_median_var(mae[[1]], mae[[2]], "rclr", "nmr", bicluster_rows, bicluster_columns)
```

Now we can create a scatter plot. X-axis includes median clr abundance
of microbiome and y-axis median absolute concentration of each
metabolite. Each data point represents a single sample.

From the plots, we can see that there is low negative correlation in
both cluster 1 and 3.  This means that when abundance of bacteria
belonging to cluster 1 or 3 is higher, the concentration of
metabolites of cluster 1 or 3 is lower, and vice versa.

```{r biclust_6, fig.width=14, fig.height=6, fig.show="keep", out.width="33%"}
pics <- list()
for(i in seq_along(df)){
  pics[[i]] <- ggplot(df[[i]])  +
      geom_point(aes(x = median1, y = median2)) + 
      labs(title = paste0("Cluster ", i),
           x = "Taxa (rclr median)",
           y = "Metabolites (abs. median)")
  print(pics[[i]])
}
# pics[[1]] + pics[[2]] + pics[[3]]
```

_pheatmap_ does not allow boolean values, so they must be converted into factors.

```{r biclust_7}
bicluster_columns <- data.frame(apply(bicluster_columns, 2, as.factor))
bicluster_rows <- data.frame(apply(bicluster_rows, 2, as.factor))
```

Again, we can plot clusters with heatmap.

```{r biclust_8, fig.width=10, fig.height=10}
# Adjust colors for all clusters
if( ncol(bicluster_rows) > ncol(bicluster_columns) ){
  cluster_names <- colnames(bicluster_rows)
} else {
  cluster_names <- colnames(bicluster_columns)
}
annotation_colors <- list()
for(name in cluster_names){
  annotation_colors[[name]] <- c("TRUE" = "red", "FALSE" = "white")
}

# Create a heatmap
pheatmap(corr, cluster_cols = F, cluster_rows = F,
         annotation_col = bicluster_columns, 
         annotation_row = bicluster_rows,
         annotation_colors = annotation_colors)
```

### Taxa vs taxa

Third and final example deals with situation where we want to analyze
correlation between taxa. _biclust_ is suitable for this.

```{r biclust_9}
# Calculate cross-correlation
corr <- getExperimentCrossCorrelation(mae, 1, 1, 
                                      assay.type1 = "rclr", assay.type2 = "rclr", 
                                      mode = "matrix",
                                      cor_threshold = 0.2, verbose = F, show_warning = F)

# Find biclusters
library(biclust)
bc <- biclust(corr, method=BCPlaid(), fit.model = y ~ m,
              background = TRUE, shuffle = 100, back.fit = 0, max.layers = 10,
              iter.startup = 10, iter.layer = 100, verbose = FALSE)
```

```{r biclust_10}
# Get biclusters
bcs <- .get_biclusters_from_biclust(bc, corr)

bicluster_rows <- bcs$bc_rows
bicluster_columns <- bcs$bc_columns
```


```{r biclust_11}
# Create a column that combines information
# If row/column includes in multiple clusters, cluster numbers are separated with "_&_"
bicluster_columns$clusters <- apply(bicluster_columns, 1, 
                                    function(x){paste(paste(which(x)), collapse = "_&_") })
bicluster_columns <- bicluster_columns[, "clusters", drop = FALSE]

bicluster_rows$clusters <- apply(bicluster_rows, 1, 
                                 function(x){paste(paste(which(x)), collapse = "_&_") })
bicluster_rows <- bicluster_rows[, "clusters", drop = FALSE]
```

```{r biclust_12, fig.width=14, fig.height=12}
# Convert boolean values into factor
bicluster_columns <- data.frame(apply(bicluster_columns, 2, as.factor))
bicluster_rows <- data.frame(apply(bicluster_rows, 2, as.factor))

pheatmap(corr, cluster_cols = F, cluster_rows = F,
         annotation_col = bicluster_columns, 
         annotation_row = bicluster_rows)
```

## Additional Community Typing

For more community typing techniques applied to the 'SprockettTHData'
data set, see the attached .Rmd file.

Link:

   * [Rmd](add-comm-typing.Rmd)


## Going further with bluster
There are additional algorithms and handy functions that are 
in bluster, but weren't covered here:
- `clusterSweep` which allows to do a sweep across combinations 
of parameters to obtain different clusterings from the same algorithm.
- `compareClusterings` to compare pairs of clusterings. This can 
be combined with the previous method to find out which parameter 
settings have the greatest effect in a sweep.
- `clusterRMSD` to compute the root mean-squared deviation (RMSD) 
for each cluster.

The full documentation is available [here](https://bioconductor.org/packages/release/bioc/manuals/bluster/man/bluster.pdf).